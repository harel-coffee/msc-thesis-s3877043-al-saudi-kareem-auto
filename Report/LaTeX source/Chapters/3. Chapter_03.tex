\chapter{Background Information}
\label{ch:Background-Information}
This chapter will serve predominantly to explain concepts and methods relevant to the research conducted throughout the duration of this project. Most, if not all, of the information in this chapter might be considered prior knowledge to most readers but regardless may still suffice as a brisk refresher. Feel free to click \hyperref[ch:Data-Description]{here} if you would prefer to skip this chapter.

\section{Dimension Reduction}
\label{sec:Background-Information:Dimension-Reduction}
Depending on how we choose to transform our data set(s) each individual candidate day could be represented by feature vectors of up to 96 temporal dimensions, if not more when taking into consideration supplementary external variables. As a precursory step to our clustering algorithms we can make use of the \gls{som} to project our data onto a feature space that is much lower in terms of the overall dimensionality thus allowing us to achieve visibly clearer clusters in terms of days that express high levels of similarity in their overall energy consumption patterns.

\subsection{Self-Organizing Maps}
\label{subsec:Background-Information:Dimension-Reduction:Self-Organizing-Maps}
The neurobiologically inspired \glsentryfull{som}, also known as the Kohonen network, is a topology preserving network proposed by Teuvo Kohonen in 1982 \cite{Kohonen}. The \gls{som} belongs to the class of unsupervised learning neural networks and is predominantly used within the realm of feature detection. In contrast to traditional \glspl{ann}, that make use of error backpropagation and gradient descent, the \gls{som} applies a form of competitive learning in which the neurons of the network compete for the right to activate when presented with input data and as a result are forced to organize themselves. Within the realm of this paper, the principal objective of the \gls{som} is to transform a complex, high-dimensional input into a much simpler low-dimensional discrete output space while preserving the overall relationship of the data. Generally the projected output lies in a two-dimensional space where the spatial locations (\ie coordinates) of the neurons are indicative of inherent statistical features contained within the data \cite{Asan}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Images/Chapter 3/SOM/SOM-Illustration.png}
    \caption{A schematic representation of a \gls{som}. A set of n feature vectors is mapped onto a two-dimensional lattice that consists of K neurons. The color of the map encodes the organization of groups of said feature vectors with similar properties. Image source: \cite{Carrasco} (with permission from the author).}
    \label{fig:SOM-Illustration}
\end{figure}

\noindent \newline To best understand how the \gls{som} works we can break it down into its core components.

\begin{enumerate}
    \item \textit{Initialization:} All weights are initialized to small, standardized, random values.
    \item \textit{Competition:} A random feature vector from the data set is chosen and presented to the lattice. Neurons compete within themselves by comparing their respective weights to the current input vector which is traditionally done by calculating the Euclidean distance from the weight vector to the input vector. The node with a weight vector closest to the input vector is activated in a winner-takes-all fashion and is marked as the \gls{bmu}.
    \item \textit{Co-operation:} The winning node determines a neighborhood of excited neurons that are adjusted to match the input vector more closely with neurons closer to the \gls{bmu} being adjusted to a greater extent.
    \item \textit{Adaptation:} The response of a winning neuron to subsequent presentations of similar input patterns is enhanced and we can present the next feature vector to the network for evaluation.
\end{enumerate}

\clearpage

\section{Clustering Algorithms}
\label{sec:Background-Information:Clustering-Algorithms}
Throughout the duration of this project we will be making use of the \gls{hdbscan} clustering algorithm. This section serves to both introduce readers to the \gls{dbscan} algorithm that precedes \gls{hdbscan} as well as provide a high-level, intuitive explanation of both algorithms so that we may better understand the differences between them.

\subsection{DBSCAN}
\label{subsec:Background-Information:Clustering-Algorithms:DBSCAN}
\glsentryfull{dbscan}, proposed by \citet{Ester}, is a non-parametric data clustering algorithm that works on the principle of grouping together points that are closely packed together (\ie located in high-density regions) while marking points that lie alone in low-density regions as outliers. This allows the \gls{dbscan} algorithm to find arbitrarily shaped clusters while also rendering it robust to noise present in the data. Furthermore, the \gls{dbscan} algorithm does not require us to specify, or otherwise know ahead of time, the number of clusters that our data contains and instead automatically determines this number based on the input data as well as the hyperparameters passed to the algorithm on its initialization. This leads us to the very first downside associated with the \gls{dbscan} algorithm and that is that it is \textit{exceptionally} sensitive to hyperparameter selection and thus it is imperative to have a solid grasp on the understanding of said hyperparameters so as to obtain ideal and meaningful results.

\noindent \newline The \gls{dbscan} algorithm classifies points in a feature space as either core points, density-reachable points, and outliers. To best understand how this is done we must first define the two hyperparameters that are essential to the initialization of the algorithm. The first, and arguably, most important hyperparameter is labelled $\epsilon$ and defines the maximum distance between two points or, in other words, the radius of a neighborhood with respect to a point. The second hyperparameter is aptly titled \textit{minPts} ($m_{pts}$) and represents the minimum number of points that must be within distance $\epsilon$ of a point to define it as a \textit{core} point. If a point does not contain the minimum number of points within its neighborhood to define it as a core point but \textit{is} within distance $\epsilon$ from a core point then we consider it a \textit{density-reachable point} and it belongs to the cluster. Any points that cannot be reached from any other point are considered outliers or noise points. In essence, any core point forms a cluster together with all points (core or not) that are within distance $\epsilon$ from said core point. Non-core points cannot be used to reach more points and belong to the \textit{"edge"} of the cluster. The pseudocode in Listing \ref{lst:DBSCAN} below depicts an explanation of how this process works.

\noindent \newline \begin{lstlisting}[float=hbt!, frame=tb, caption={The \gls{dbscan} algorithm.},label=lst:DBSCAN]
DBSCAN(D, eps, minPts)
    C = 0
    foreach unvisited point P in dataset D
        P = visited
        neighborPts_P = queryNeighborhood(P, eps)
        if(neighborPts < minPts)
            P = noise
        else
            C = C + 1
            expandCluster(P, neighborPts, C, eps, minPts)

expandCluster(P, neighborPts, C, eps, minPts)
    add P to C
    foreach point Q in neighborPts_P
        if Q = unvisited
            Q = visited
            neighborPts_Q = queryNeighborhood(Q, eps)
            if(neighborPts_Q >=  minPts)
                join(neighborPts_Q, neighborPts_P)
        else
            add Q to C
            
queryNeighborhood(P, eps)
    return all points within distance eps to P
\end{lstlisting}

\noindent To round things off, we present the main advantages and disadvantages of the \gls{dbscan} algorithm:

\begin{itemize}
    \pro No prior knowledge of the number of clusters is required.
    \pro Can find arbitrarily shaped clusters.
    \pro Contains the notion of noise and outliers.
    \pro Only two hyperparameters need to be set.
    \con Reliant on the distance metric being used.
    \con Choosing a meaningful distance threshold can be quite difficult if the data and scale are not well understood.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Images/Chapter 3/DBSCAN/DBSCAN-Illustration.png}
    \caption{Depiction of the \gls{dbscan} algorithm at work with minPts set to 4. Here, point A, as well as the other red points, are core points as the area surrounding these points in a radius $\epsilon$ contains at least 4 points (including the point itself). Points B and C are reachable from A through other core points and as a result can be considered density-reachable points. The cluster is made up of the core points as well as points B and C. Point N is neither a core point and cannot be reached through any of the core points and so is considered an outlier. Image source: \cite{Wikipedia_DBSCAN}, licensed under CC BY-SA 3.0.}
    \label{fig:DBSCAN-Illustration}
\end{figure}

\subsection{HDBSCAN}
\label{subsec:Background-Information:Clustering-Algorithms:HDBSCAN}
\glsentryfull{hdbscan}, proposed by \citet{Campello}, is a hierarchical non-parametric clustering algorithm that was designed to overcome the main limitations of \gls{dbscan}. The most substantial changes come in the form of no longer explicitly needing to predefine a value for the distance threshold $\epsilon$. Instead, \gls{hdbscan} generates a complete density-based clustering hierarchy over variable densities from which we can extract a simplified hierarchy composed only of the most significant clusters in our data. Without delving deep into the concepts of cluster stability, minimum spanning trees and hierarchy construction we instead refer the reader to the detailed explanation in the literature \cite{Campello} and leave off with Figure \ref{fig:HDBSCAN-Illustration} that does well to illustrate how \gls{hdbscan} works as a hierarchical clustering algorithm.

\begin{figure}[H]
        \begin{adjustwidth*}{-3.0cm}{-3.0cm}%
                \myfloatalign
                \subfloat[\gls{hdbscan} clustering applied in the case of 3 clusters.]
                {
                        \includegraphics[width=0.7\textwidth]{Images/Chapter 3/HDBSCAN/HDBSCAN-Illustration-1.1.png}
                        \label{fig:HDBSCAN-Illustration-1.1}
                } \quad
                \subfloat[\gls{hdbscan} clustering applied in the case of 2 clusters.]
                {
                        \includegraphics[width=0.7\textwidth]{Images/Chapter 3/HDBSCAN/HDBSCAN-Illustration-2.1.png}
                        \label{fig:HDBSCAN-Illustration-2.1}
                } \quad
                \subfloat[Cluster hierarchy of \ref{fig:HDBSCAN-Illustration-1.1}.]
                {
                        \includegraphics[width=0.7\textwidth]{Images/Chapter 3/HDBSCAN/HDBSCAN-Illustration-1.2.png}
                        \label{fig:HDBSCAN-Illustration-1.2}
                } \quad
                \subfloat[Cluster hierarchy of \ref{fig:HDBSCAN-Illustration-2.1}.]
                {
                        \includegraphics[width=0.7\textwidth]{Images/Chapter 3/HDBSCAN/HDBSCAN-Illustration-2.2.png}
                        \label{fig:HDBSCAN-Illustration-2.2}
                } \quad
                \subfloat[Density landscape of \ref{fig:HDBSCAN-Illustration-1.1}.]
                {
                        \includegraphics[width=0.7\textwidth]{Images/Chapter 3/HDBSCAN/HDBSCAN-Illustration-1.4.png}
                        \label{fig:HDBSCAN-Illustration-1.4}
                } \quad
                \subfloat[Density landscape of \ref{fig:HDBSCAN-Illustration-2.1}.]
                {
                        \includegraphics[width=0.7\textwidth]{Images/Chapter 3/HDBSCAN/HDBSCAN-Illustration-2.4.png}
                        \label{fig:HDBSCAN-Illustration-2.4}
                } \quad
                \caption{An illustration of the hierarchical aspects of the \gls{hdbscan} algorithm. In layman's terms, when presented with the density landscape the \gls{hdbscan} algorithm decides whether peaks of a mountain are part of the same mountain or whether they belong to different mountains where each of these mountains represent a cluster. When multiple peaks represent multiple mountains the sum of their respective volumes tends to be larger than the volume of their base. The opposite is true for when multiple peaks are just features of a singular mountain.}
                \label{fig:HDBSCAN-Illustration}
        \end{adjustwidth*}
\end{figure}

\clearpage

\section{Distance Metrics}
\label{sec:Background-Information:Distance-Metrics}
As mentioned in section \ref{subsec:Background-Information:Clustering-Algorithms:DBSCAN}, the \gls{dbscan} algorithm as well as the \gls{hdbscan} algorithm are heavily reliant on the distance metric being used. For that reason, we explored the possibility of 2 commonly used distance metrics when working with time series data sets: Euclidean distance and Dynamic Time Warping.

\subsection{Euclidean Distance}
\label{subsec:Background-Information:Distance-Metrics:Euclidean-Distance}

\begin{equation}
    d_{euc}(p, q) = \sqrt{\sum\limits_{i=1}^{n} (p_i - q_i)^2}
\label{eq:Euclidean-Distance}
\end{equation}

\noindent where:

\begin{conditions*}
        p, q        &   two points in the Euclidean n-space. \\
        p_i, q_i    &   Euclidean vectors starting from the origin of the space. \\
        n           &   n-space.
\end{conditions*}

\subsection{Dynamic Time Warping}
\label{subsec:Background-Information:Distance-Metrics:Dynamic-Time-Warping}
When working with time series data sets, \gls{dtw} provides us with the means of determining whether two temporal sequences exhibit any measure of similarity. To best understand how \gls{dtw} works let us assume that we are working with two sequences S and T of lengths n and m respectively. We can arrange the sequences in an $n \times m$ grid where each point (x, y) is the alignment between S[x] and T[y]. Thus, we can calculate the path with minimal distance between elements of both S and T, or what is known as a warping path as follows:

\begin{equation}
    D_{min}(S_k, T_k) = \argmin_{S_{k-1}, T_{k-1}} D_{min} (S_{k-1}, T_{k-1}) + d_{euc}(S_k, T_k | S_{k-1}, T_{k-1})
\label{eq:DTW}
\end{equation}

\noindent \newline Furthermore, the warping path can only make the following moves:

\begin{enumerate}
    \item Horizontal moves $(x, y) \rightarrow (x, y + 1)$ -- insertion.
    \item Vertical moves: $(x, y) \rightarrow (x + 1, y)$ -- deletion.
    \item Diagonal moves: $(x, y) \rightarrow (x + 1, y + 1)$ -- match.
\end{enumerate}

\noindent \newline Some final notes on the implementation of \gls{dtw}:

\begin{itemize}
    \item Each index from the first sequence must be matched with one (or more) indices from the second sequence and vice versa.
    \item The first and last index from the first sequence must be matched with the first and last index of the second sequence respectively.
    \item The mapping of indices from the first sequence to indices of the second sequence must be monotonically increasing and vice versa such that $S_{t-1} \leq S_{t}$ and $T_{t-1} \leq T_{t}$.
\end{itemize}

\noindent Figure \ref{fig:DTW-Illustration} denotes an example of how we can calculate the \gls{dtw} path of 2 sequences of length 6. The first sequence contains the values [1, 2, 6, 3, 1, 3] and the second sequence contains the values [2, 6, 3, 1, 2, 7].

\begin{figure}[hbt!]
        \begin{adjustwidth*}{-0.5cm}{-0.5cm}%
                \myfloatalign
                \subfloat[Accumulated cost matrix and warping path.]
                {
                        \includegraphics[width=0.5\textwidth]{Images/Chapter 3/DTW/DTW-Illustration-1.png}
                        \label{fig:DTW-Illustration-1}
                } \quad
                \subfloat[DTW distance between our 2 sequences.]
                {
                        \includegraphics[width=0.5\textwidth]{Images/Chapter 3/DTW/DTW-Illustration-2.png}
                        \label{fig:DTW-Illustration-2}
                } \quad
                \caption{An illustration depicting how we can use \gls{dtw} to calculate the distance between 2 sequences.}
                \label{fig:DTW-Illustration}
        \end{adjustwidth*}
\end{figure}

\section{Forecasting Models}
\label{sec:Background-Information:Forecasting-Models}
The primary forecasting model that we will be utilizing in our forecasting pipeline is a hybrid \gls{cnn-lstm} network. This section serves to introduce readers to both the \gls{cnn} component as well as the \gls{lstm} component of this network.

\subsection{Convolutional Neural Networks}
\label{subsec:Background-Information:Forecasting-Models:Convolutional-Neural-Networks}
\glsentryfullpl{cnn} first truly started gaining traction in the 1990s when \citet{LeCun} demonstrated that a \gls{cnn} which aggregates simple features into progressively more complex features can be successfully used for the task of recognizing handwritten characters. Since then, their relevance has become more and more widespread with applications in image and video classification, natural language processing \cite{Collobert}, and when working with time series data sets \cite{Tsantekidis}. The following sections will briefly outline key points of the inner machinations of key \gls{cnn} components.

\subsubsection{Convolutional Layers}
\label{subsubsec:Background-Information:Forecasting-Models:Convolutional-Neural-Networks:Convolutional-Layers}
Firstly, and most importantly, \glspl{cnn} derive their name from the so-called "convolution" operator whose primary function is to extract features from the input vector while maintaining the spatial relationship between the features in said input vector. Put simply, the convolutional layer works by sliding a pre-determined number of filters, otherwise known as 'kernels', a pre-determined 'distance' or stride over an input vector and returning a feature map per filter. The value of said filters are, in practice, learned by the network during the training process while other hyperparameters, such as the number of filters as well as their respective sizes are pre-determined by the network architect. Other things to keep note of are that the resulting feature maps are reduced in dimensionality when compared to the input; however this can be offset by utilizing a variation of techniques such as the application of a form of \textit{padding}.

\begin{figure}[hbt!]
    \centering
    \includegraphics[width=\textwidth]{Images/Chapter 3/CNN/Convolution-Illustration.png}
    \caption{An example of a convolutional kernel at work. A 3x3 kernel traverses over a 5x5 "image" with a stride of 1 to produce the convolved feature map.}
    \label{fig:Convolution-Illustration}
\end{figure}

\subsubsection{Rectified Linear Unit Operation}
\label{subsubsec:Background-Information:Forecasting-Models:Convolutional-Neural-Networks:ReLU}
Following every convolution operation is a \gls{relu} operation where \gls{relu} is a non-linear operation whose output is given by:

\begin{equation}
    R(z) = \text{max}(0, z)
\label{eq:ReLU}
\end{equation}

\noindent \newline The purpose of the \gls{relu} operation is to replace all negative values in the feature map by zero. This nonlinear function allows for the use of stochastic gradient descent with backpropagation of errors that enables us to learn complex relationships within the data.  Other operations, or activation functions, such as \textit{tanh} or \textit{sigmoid} can also be used here but generally \gls{relu} performs much better in most situations and is much quicker to perform due to its sheer simplicity.

\begin{figure}[hbt!]
    \centering
    \includegraphics[width=\textwidth]{Images/Chapter 3/CNN/ReLU-Illustration.png}
    \caption{A simplified demonstration of a \gls{relu} operation.}
    \label{fig:ReLU-Illustration}
\end{figure}

\subsubsection{Max Pooling Layers}
\label{subsubsec:Background-Information:Forecasting-Models:Convolutional-Neural-Networks:Max-Pooling-Layers}
Inter-mingled between convolutional layers are a set of pooling layers that serve to reduce the dimensionality of each feature map while retaining the most important information. In the case of \textit{max} pooling layers, the network defines a spatial neighborhood and takes the largest element from the rectified feature map within that window. The goal of pooling layers then is to reduce the feature dimensions of our input vectors thus making them smaller and more manageable to work with while also reducing the number of parameters and computations needed to fit our network, thus minimizing the risk of overfitting. Furthermore, this renders the network invariant to small transformations, distortions and translations in the input vector by providing us with what is essentially a scale invariant representation of our vector.

\begin{figure}[hbt!]
    \centering
    \includegraphics[width=\textwidth]{Images/Chapter 3/CNN/Max-Pooling-Illustration.png}
    \caption{An example of max pooling using a 2x2 window.}
    \label{fig:Max-Pooling-Illustration}
\end{figure}

\clearpage

\subsection{Long Short-Term Memory Networks}
\label{subsec:Background-Information:Forecasting-Models:Long-Short-Term-Memory-Networks}
\glsentryfull{lstm} networks, first proposed by Hochreiter \& Schmidhuber in 1997 \cite{Hochreiter}, are a special kind of \gls{rnn} network that are capable of learning long-term dependencies while overcoming the main limitations that plagued traditional \gls{rnn} networks (such as the exploding/vanishing gradient problem). The cell state of an \gls{lstm} can be seen as a highway that transfers relative information all the way down the sequence chain allowing information throughout the processing of the sequence to be retained giving the network a form of \textit{"memory"}. The key to the functionality of the \gls{lstm} is through the use of a number of gates that give it the ability to remove or add information to this cell state by learning what information is relevant to keep, or otherwise forget, during training. The following sections will serve to outline the functionality of the cell state and gates so that we may gain a better understanding of them.

\begin{figure}[hbt!]
    \centering
    \includegraphics[width=\textwidth]{Images/Chapter 3/LSTM/LSTM-Chain-Illustration.png}
    \caption{The repeating module in an \gls{lstm} that contains four interacting layers. Image source: \cite{Colah}  (with permission from the author).}
    \label{fig:LSTM-Chain-Illustration}
\end{figure}

\subsubsection{Forget Gate}
\label{subsubsec:Background-Information:Forecasting-Models:Long-Short-Term-Memory-Networks:Forget-Gate}
The first gate that we will be taking a look at is the forget gate $(f_t)$. This gate decides what information should be thrown away and what information should be kept from prior steps. Information from the previous hidden state $(h_{t-1})$ and information from the current input $(x_t)$ are passed through a \textit{sigmoid} $(\sigma)$ function where values come out between 0 and 1 for each number in the cell state $(C_{t-1})$. Values closer to and including 0 indicate that we should completely forget this information while values closer to and including 1 indicate that we should completely retain all of this information. The formulation of the forget gate can be seen in equation \ref{eq:LSTM-Forget-Gate}.

\begin{equation}
    f_t = \sigma (W_f \cdot \left[h_{t-1}, x_t\right] + b_f)
\label{eq:LSTM-Forget-Gate}
\end{equation}

\begin{figure}[hbt!]
    \centering
    \includegraphics[width=0.75\textwidth]{Images/Chapter 3/LSTM/LSTM-Forget-Gate-Illustration.png}
    \caption{An illustration of the forget gate in an \gls{lstm} network. Image source: \cite{Colah}  (with permission from the author).}
    \label{fig:LSTM-Forget-Gate-Illustration}
\end{figure}

\subsubsection{Input Gate}
\label{subsubsec:Background-Information:Forecasting-Models:Long-Short-Term-Memory-Networks:Input-Gate}
The input gate $(i_t)$ mainly serves to decide what new information will be stored in the cell state from the current step. The input gate is a \textit{sigmoid} $(\sigma)$ function that is passed the previous hidden state $(h_{t-1})$ and the current input $(x_t)$ and outputs values between 0 and 1 where values closer to and including 0 indicate that the information is not important while values closer to and including 1 indicate that the information is important. This value is multiplied by a \textit{tanh} layer that serves the purpose of creating a vector of new candidate values $(\tilde{C}_t)$ that could potentially be added to the cell state. The formulation of the input gate and its respective layers can be seen in equations \ref{eq:Input-Gate}.

\begin{align}
    \begin{split}
        & i_t = \sigma (W_i \cdot \left[h_{t-1}, x_t \right] + b_i) \\
        & \tilde{C}_t = tanh (W_C \cdot \left[h_{t-1}, x_t \right] + b_C)
    \end{split}
\label{eq:Input-Gate}
\end{align}

\begin{figure}[hbt!]
    \centering
    \includegraphics[width=0.75\textwidth]{Images/Chapter 3/LSTM/LSTM-Input-Gate-Illustration.png}
    \caption{An illustration of the input gate in an \gls{lstm} network. Image source: \cite{Colah} (with permission from the author).}
    \label{fig:LSTM-Input-Gate-Illustration}
\end{figure}

\subsubsection{Output Gate}
\label{subsubsec:Background-Information:Forecasting-Models:Long-Short-Term-Memory-Networks:Output-Gate}
The final gate is the output gate $(o_t)$ which decides what the next hidden state $(h_t)$ should be. As like in the previous gates, we pass the previous hidden state $(h_{t-1})$ and the current input $(x_t)$ into a \textit{sigmoid} $(\sigma)$ function which is multiplied by the output of the \textit{tanh} function applied to the modified cell state $(C_t)$ which finally gives us our new hidden state. The new hidden state as well as the new cell state are carried over to the next time step. The formulation of the output gate and its respective layers can be seen in equations \ref{eq:LSTM-Output-Gate}.

\begin{align}
    \begin{split}
        & C_t = f_t * C_{t-1} + i_t * \tilde{C}_t \\
        & o_t = \sigma (W_o \left[h_{t-1}, x_t \right] + b_o) \\
        & h_t = o_t * tanh(C_t)
    \end{split}
\label{eq:LSTM-Output-Gate}
\end{align}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{Images/Chapter 3/LSTM/LSTM-Output-Gate-Illustration.png}
    \caption{An illustration of the output gate in an \gls{lstm} network. Image source: \cite{Colah}  (with permission from the author).}
    \label{fig:LSTM-Output-Gate-Illustration}
\end{figure}

\clearpage

\section{Performance Metrics}
\label{sec:Background-Information:Performance-Metrics}
Throughout the duration of this project we will be making use of a variety of performance metrics. These performance metrics, as well as the reasoning behind choosing them, will be explained in the following sections.

\subsection{Mean Absolute Error}
\label{subsec:Background-Information:Performance-Metrics:Mean-Absolute-Error}
The first performance metric that we will be taking a look at is the \gls{mae}. It provides us with a direct interpretation of how far off the predictions made by our forecasting models were from the actual, ground truth. However, the \gls{mae} does not provide us with the capability of drawing comparisons between results obtained from disparate data sets as it is a scale-dependent metric. That said it provides a satisfactory level of insight nonetheless. Its equation is:

\begin{equation}
    \text{MAE} = \frac{\sum\limits_{i = 1}^{n} \abs{\hat{y_i} - y_i}}{n}
\label{eq:MAE}
\end{equation}

\noindent where:

\begin{conditions*}
        \hat{y}_i   &   predicted value. \\
        y_i         &   actual value. \\
        n           &   total number of data points.
\end{conditions*}

\subsection{Root Mean Squared Error}
\label{subsec:Background-Information:Performance-Metrics:Root-Mean-Squared-Error}
The second performance metric that we will be working with is the \gls{rmse}. Its primary purpose is to serve as a cost function that our forecasting models will seek to minimize. The primary reason behind choosing \gls{rmse} as our cost function is that it penalizes larger errors. Its equation is:

\begin{equation}
\centering
    \text{RMSE} = \sqrt{\frac{\sum\limits_{i = 1}^{n} (\hat{y_i} - y_i)^2}{n}}
\label{eq:RMSE}
\end{equation}

\noindent where:

\begin{conditions*}
        \hat{y}_i   &   predicted value. \\
        y_i         &   actual value. \\
        n           &   total number of data points.
\end{conditions*}

\subsection{Mean Absolute Percentage Error}
\label{subsec:Background-Information:Performance-Metrics:Mean-Absolute-Percentage-Error}
The final performance metric we will be taking a look at is the \glsentryfull{mape}. As it is a scale-invariant metric, its primary purpose is to allow us to assess the performance of our forecasting models across the multiple, disparate data sets we have on hand and draw comparisons between them. Its equation is:

\begin{equation}
    \text{MAPE} = \frac{1}{n} \sum\limits_{i = 1}^{n} \abs{\frac{\hat{y_i} - y_i}{y_i}}
\label{eq:MAPE}
\end{equation}

\noindent where:

\begin{conditions*}
        \hat{y}_i   &   predicted value. \\
        y_i         &   actual value. \\
        n           &   total number of data points.
\end{conditions*}