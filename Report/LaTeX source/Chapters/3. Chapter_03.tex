\chapter{Background Information}
\label{ch:Background-Information}
This chapter serves predominantly to explain concepts and methods relevant to the research. This includes dimensionality reduction techniques, such as \glsentryfull{t-sne} and \glsentryfull{umap}, as well as our chosen clustering and forecasting techniques (\glsentryfull{hdbscan} and \glsentryfull{cnn-lstm} respectively). Most, if not all, of the information in this chapter might be considered prior knowledge to most readers but regardless may still suffice as a brisk refresher. Feel free to click \hyperref[ch:Exploratory-Data-Analysis]{here} if you would prefer to skip this chapter.

\section{Dimension Reduction}
\label{sec:Background-Information:Dimension-Reduction}
Depending on how we choose to transform our data set(s) each individual candidate day could be represented by feature vectors of up to 96 temporal dimensions, if not more when taking into consideration supplementary external variables. As a precursory step to our clustering algorithms we can make use of various manifold approximation techniques (such as \gls{t-sne} and \gls{umap} to project our data onto a feature space that is much lower in terms of the overall dimensionality thus allowing us to achieve visibly clearer clusters in terms of days that express high levels of similarity in their overall energy consumption patterns.

\subsection{t-Distributed Stochastic Neighbor Embedding}
\label{subsec:Background-Information:Dimension-Reduction:t-Distributed-Stochastic-Neighbor-Embedding}
\glsentryfull{t-sne}, proposed by \citet{Laurens}, is a statistical, or otherwise stochastic, method, that is utilized primarily for visualizing high-dimensional data. In principle, \gls{t-sne} works by giving each high-dimensional point in the data set a location in an easy-to-digest 2 or 3-dimensional map. In contrast to \gls{pca}, a well-known linear dimensionality reduction technique, \gls{t-sne} is a \textit{non-linear} technique for dimensionality reduction that allows for the separation of our data set in a manner that cannot be separated by any straight line. In order to keep things relatively simple, we can best understand how \gls{t-sne} works by breaking it down and providing an overview of the steps the algorithm undertakes.

\noindent \newline Let us take a data point, from a subset of \textit{N} total data points, $x_i$ in the original, high-dimensional space $R^D$ where D represents the dimensionality of said original space. A map point, $y_i$, that lies in the map space $R^2$ or, less commonly, $R^3$ represents one of our original points in a lower-dimensional mapping that serves as the final representation of our data. To preserve the global structure of the data set in this lower-dimensional space we first define a conditional \textit{similarity}, or conditional probability, that any point $x_i$ would pick another point $x_j$ as its neighbor if neighbors were picked in proportion to their probability density under a Gaussian distribution centered around $x_i$ with a given variance $\sigma^2_i$. This is defined by Equation \ref{eq:tSNE-1}.

\begin{equation}
    p_{j|i} = \frac{\exp\left(-\abs{x_i - x_j}^2 / 2\sigma^2_i\right)}{\sum_{k \neq i} \exp\left(-\abs{x_i - x_k}^2 / 2\sigma^2_i\right)}
\label{eq:tSNE-1}
\end{equation}

\noindent \newline The variance ($\sigma^2_i$) differs between points and is chosen in such a way that points in dense areas are given a smaller variance than points in sparse areas. This is introduced as the concept of \textit{Perplexity} within the realm of the \gls{t-sne} algorithm (determining optimal $\sigma$ for each point) and is defined as:

\begin{equation}
    \text{Perplexity} = 2^{-\sum_j p_{j|i} \log_2 p_{j|i}}
\label{eq:tSNE-2}
\end{equation}

\noindent \newline \textit{Similarity} is the defined as a symmetrized version of the previously defined conditional probability in Equation \ref{eq:tSNE-1}:

\begin{equation}
    p_{ij} = \frac{p_{i|j} + p_{j|i}}{2N}
\label{eq:tSNE-3}
\end{equation}

\noindent \newline Equation \ref{eq:tSNE-3} provides us with a similarity matrix for our original data set that represents the similarity between all of our data points as they lie in their original, high-dimensional space. We then define a similarity matrix for the points that lie on the mapping space as such:

\begin{equation}
    q_{ij} = \frac{\left(1 + \abs{y_i - y_j}^2\right)^{-1}}{\sum_{k \neq i}\left(1 + \abs{y_k - y_i}^2\right)^{-1}}
\label{eq:tSNE-4}
\end{equation}

\noindent \newline The goal then is to minimize the differences between the 2 similarity matrices in order to achieve a good overall representation of our data points in the lower-dimensional, mapping space. To do this the \gls{t-sne} algorithm minimizes the Kullback-Leiber divergence between the 2 distributions $p_{ij}$ and $q_{ij}$:

\begin{equation}
    \text{KL}(P\|Q) = \sum_{ij} p_{ij} \log \frac{p_{ij}}{q_{ij}}
\label{eq:tSNE-5}
\end{equation}

\noindent \newline This \textit{score} is minimized by performing a gradient descent that can be computed analytically and, in essence, represents the magnitude of the pull between data points in our lower-dimensional, mapping space as well as the direction of said pull:

\begin{equation}
    \frac{\partial \text{KL}}{\partial y_i} = 4 \sum_j (p_{ij} - q_{ij})(y_i - y_j)\left(1 + \abs{y_i - y_j}^2\right)^{-1}
\label{eq:tSNE-6}
\end{equation}

\noindent \newline The choice of using the so-called t-Student (or Cauchy) distribution, as seen in Equation \ref{eq:tSNE-4}, for the map points as opposed to the Gaussian distribution used for the original data points is to alleviate the \textit{crowding problem}. This problem is defined in the original paper as follows -- "the area of the two-dimensional map that is available to accommodate moderately distant data points will not be nearly large enough compared with the area available to accommodate nearby data points".

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Images/Chapter 3/t-SNE/t-SNE-Illustration-1.pdf}
    \caption{Overlapping both a Gaussian distribution and the t-Student, or Cauchy, distribution. Note the heavy tails on the t-Student distribution.}
    \label{fig:t-SNE-Illustration}
\end{figure}

\noindent \newline In essence, using the same Gaussian distribution for the original data points and the map points would lead to an imbalance in the distribution of the distances of a point's neighbors due to the fact that the distribution of the distances varies vastly between high-dimensional spaces and low-dimensional spaces. By using the t-Student distribution, points close in the high-dimensional, original space get even closer in the lower-dimensional, mapping space while points that are further away from each other in the high-dimensional, original space get even further in the lower-dimensional, mapping space. The differences between both distributions can be seen in Figure \ref{fig:t-SNE-Illustration}.

\noindent \newline To round things off, we note that the \gls{t-sne} algorithm is heavily reliant on the hyperparameters chosen on initialization (predominantly with respect to the chosen value for \textit{perplexity}) and, as it is a stochastic algorithm, different runs performed on the same data set will produce different, albeit similar, results. Furthermore, standard deviations between clusters (or cluster size in terms of bounding box measurements) are not representative of the relative sizes of the actual clusters and generally mean nothing. Finally, distances between clusters in the mapping space does not always give us a good sense of the global geometry -- that is, in the sense that distance between clusters may (or may not) mean anything significant.

\subsubsection{Uniform Manifold Approximation and Projection}
\label{subsec:Background-Information:Dimension-Reduction:t-Distributed-Stochastic-Neighbor-Embedding:Uniform-Manifold-Approximation-and-Projection}
Although \gls{t-sne} is fine to use as a dimensionality reduction technique it is predominantly cited as a visualization heuristic. Interpreting the resulting map obtained from performing or otherwise executing the algorithm must generally be done with some measure of caution. A novel algorithm proposed by \citet{UMAP}, and aptly named \glsentryfull{umap} claims to be competitive with \gls{t-sne} in terms of visualization quality and argues that it preserves more of the global structure present in the data while being superior in terms of run time performance. Given the novelty of the algorithm, it lacks the rigorous testing and mathematical analysis that its counterpart, the \gls{t-sne} algorithm, is subject to. Nonetheless, we will be making use of the \gls{umap} algorithm for the purpose of this project and refer the reader to the original paper located \href{https://arxiv.org/abs/1802.03426}{here} \cite{UMAP} to better understand the core concepts of the algorithm as well as what differentiates it from the \gls{t-sne} algorithm.

\clearpage

\section{Clustering Algorithms}
\label{sec:Background-Information:Clustering-Algorithms}
Throughout the duration of this project we will be making use of the \gls{hdbscan} clustering algorithm. This section serves to both introduce readers to the \gls{dbscan} algorithm that precedes \gls{hdbscan} as well as provide a high-level, intuitive explanation of both algorithms so that we may better understand the differences between them.

\subsection{DBSCAN}
\label{subsec:Background-Information:Clustering-Algorithms:DBSCAN}
\glsentryfull{dbscan}, proposed by \citet{Ester}, is a non-parametric data clustering algorithm that works on the principle of grouping together points that are closely packed together (\ie located in high-density regions) while marking points that lie alone in low-density regions as outliers. This allows the \gls{dbscan} algorithm to find arbitrarily shaped clusters while also rendering it robust to noise present in the data. Furthermore, the \gls{dbscan} algorithm does not require us to specify, or otherwise know ahead of time, the number of clusters that our data contains and instead automatically determines this number based on the input data as well as the hyperparameters passed to the algorithm on its initialization. This leads us to the very first downside associated with the \gls{dbscan} algorithm and that is that it is \textit{exceptionally} sensitive to hyperparameter selection and thus it is imperative to have a solid grasp on the understanding of said hyperparameters so as to obtain ideal and meaningful results.

\noindent \newline The \gls{dbscan} algorithm classifies points in a feature space as either core points, density-reachable points, and outliers. To best understand how this is done we must first define the two hyperparameters that are essential to the initialization of the algorithm. The first, and arguably, most important hyperparameter is labeled $\epsilon$ and defines the maximum distance between two points or, in other words, the radius of a neighborhood with respect to a point. The second hyperparameter is aptly titled \textit{minPts} ($m_{pts}$) and represents the minimum number of points that must be within distance $\epsilon$ of a point to define it as a \textit{core} point. If a point does not contain the minimum number of points within its neighborhood to define it as a core point but \textit{is} within distance $\epsilon$ from a core point then we consider it a \textit{density-reachable point} and it belongs to the cluster. Any points that cannot be reached from any other point are considered outliers or noise points. In essence, any core point forms a cluster together with all points (core or not) that are within distance $\epsilon$ from said core point. Non-core points cannot be used to reach more points and belong to the \textit{"edge"} of the cluster. The pseudocode in Listing \ref{lst:DBSCAN} below depicts an explanation of how this process works.

\noindent \newline \begin{lstlisting}[float=hbt!, frame=tb, caption={The \gls{dbscan} algorithm.},label=lst:DBSCAN]
DBSCAN(D, eps, minPts)
    C = 0
    foreach unvisited point P in dataset D
        P = visited
        neighborPts_P = queryNeighborhood(P, eps)
        if(neighborPts < minPts)
            P = noise
        else
            C = C + 1
            expandCluster(P, neighborPts, C, eps, minPts)

expandCluster(P, neighborPts, C, eps, minPts)
    add P to C
    foreach point Q in neighborPts_P
        if Q = unvisited
            Q = visited
            neighborPts_Q = queryNeighborhood(Q, eps)
            if(neighborPts_Q >=  minPts)
                join(neighborPts_Q, neighborPts_P)
        else
            add Q to C
            
queryNeighborhood(P, eps)
    return all points within distance eps to P
\end{lstlisting}

\noindent To round things off, we present the main advantages and disadvantages of the \gls{dbscan} algorithm:

\begin{itemize}
    \pro No prior knowledge of the number of clusters is required.
    \pro Can find arbitrarily shaped clusters.
    \pro Contains the notion of noise and outliers.
    \pro Only two hyperparameters need to be set.
    \con Reliant on the distance metric being used.
    \con Choosing a meaningful distance threshold can be quite difficult if the data and scale are not well understood.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Images/Chapter 3/DBSCAN/DBSCAN-Illustration.pdf}
    \caption{Depiction of the \gls{dbscan} algorithm at work with minPts set to 4. Here, point A, as well as the other red points, are core points as the area surrounding these points in a radius $\epsilon$ contains at least 4 points (including the point itself). Points B and C are reachable from A through other core points and as a result can be considered density-reachable points. The cluster is made up of the core points as well as points B and C. Point N is neither a core point and cannot be reached through any of the core points and so is considered an outlier. Image source: \cite{Wikipedia_DBSCAN}, licensed under CC BY-SA 3.0.}
    \label{fig:DBSCAN-Illustration}
\end{figure}

\subsection{HDBSCAN}
\label{subsec:Background-Information:Clustering-Algorithms:HDBSCAN}
\glsentryfull{hdbscan}, proposed by \citet{Campello}, is a hierarchical non-parametric clustering algorithm that was designed to overcome the main limitations of \gls{dbscan}. The most substantial changes come in the form of no longer explicitly needing to predefine a value for the distance threshold $\epsilon$. Instead, \gls{hdbscan} generates a complete density-based clustering hierarchy over variable densities from which we can extract a simplified hierarchy composed only of the most significant clusters in our data. Without delving deep into the concepts of cluster stability, minimum spanning trees and hierarchy construction we instead refer the reader to the detailed explanation in the literature \cite{Campello} and leave off with Figure \ref{fig:HDBSCAN-Illustration} that does well to illustrate how \gls{hdbscan} works as a hierarchical clustering algorithm.

\begin{figure}[H]
        \begin{adjustwidth}{-3.0cm}{-3.0cm}%
                \myfloatalign
                \subfloat[\gls{hdbscan} clustering applied in the case of 3 clusters.]
                {
                        \includegraphics[width=0.7\textwidth]{Images/Chapter 3/HDBSCAN/HDBSCAN-Illustration-1.1.pdf}
                        \label{fig:HDBSCAN-Illustration-1.1}
                } \quad
                \subfloat[\gls{hdbscan} clustering applied in the case of 2 clusters.]
                {
                        \includegraphics[width=0.7\textwidth]{Images/Chapter 3/HDBSCAN/HDBSCAN-Illustration-2.1.pdf}
                        \label{fig:HDBSCAN-Illustration-2.1}
                } \quad
                \subfloat[Cluster hierarchy of \ref{fig:HDBSCAN-Illustration-1.1}.]
                {
                        \includegraphics[width=0.7\textwidth]{Images/Chapter 3/HDBSCAN/HDBSCAN-Illustration-1.2.pdf}
                        \label{fig:HDBSCAN-Illustration-1.2}
                } \quad
                \subfloat[Cluster hierarchy of \ref{fig:HDBSCAN-Illustration-2.1}.]
                {
                        \includegraphics[width=0.7\textwidth]{Images/Chapter 3/HDBSCAN/HDBSCAN-Illustration-2.2.pdf}
                        \label{fig:HDBSCAN-Illustration-2.2}
                } \quad
                \subfloat[Density landscape of \ref{fig:HDBSCAN-Illustration-1.1}.]
                {
                        \includegraphics[width=0.7\textwidth]{Images/Chapter 3/HDBSCAN/HDBSCAN-Illustration-1.4.pdf}
                        \label{fig:HDBSCAN-Illustration-1.4}
                } \quad
                \subfloat[Density landscape of \ref{fig:HDBSCAN-Illustration-2.1}.]
                {
                        \includegraphics[width=0.7\textwidth]{Images/Chapter 3/HDBSCAN/HDBSCAN-Illustration-2.4.pdf}
                        \label{fig:HDBSCAN-Illustration-2.4}
                } \quad
                \caption{An illustration of the hierarchical aspects of the \gls{hdbscan} algorithm. In layman's terms, when presented with the density landscape the \gls{hdbscan} algorithm decides whether peaks of a mountain are part of the same mountain or whether they belong to different mountains where each of these mountains represent a cluster. When multiple peaks represent multiple mountains the sum of their respective volumes tends to be larger than the volume of their base. The opposite is true for when multiple peaks are just features of a singular mountain.}
                \label{fig:HDBSCAN-Illustration}
        \end{adjustwidth}
\end{figure}

\clearpage

\section{Forecasting Models}
\label{sec:Background-Information:Forecasting-Models}
The primary forecasting model that we will be utilizing in our forecasting pipeline is a hybrid \gls{cnn-lstm} network. This section serves to introduce readers to both the \gls{cnn} component as well as the \gls{lstm} component of this network.

\subsection{Convolutional Neural Networks}
\label{subsec:Background-Information:Forecasting-Models:Convolutional-Neural-Networks}
\glsentryfullpl{cnn} first truly started gaining traction in the 1990s when \citet{LeCun} demonstrated that a \gls{cnn} which aggregates simple features into progressively more complex features can be successfully used for the task of recognizing handwritten characters. Since then, their relevance has become more and more widespread with applications in image and video classification, natural language processing \cite{Collobert}, and when working with time series data sets \cite{Tsantekidis}. The following sections will briefly outline key points of the inner machinations of key \gls{cnn} components.

\subsubsection{Convolutional Layers}
\label{subsubsec:Background-Information:Forecasting-Models:Convolutional-Neural-Networks:Convolutional-Layers}
Firstly, and most importantly, \glspl{cnn} derive their name from the so-called "convolution" operator whose primary function is to extract features from the input vector while maintaining the spatial relationship between the features in said input vector. Put simply, the convolutional layer works by sliding a pre-determined number of filters, otherwise known as 'kernels', a pre-determined 'distance' or stride over an input vector and returning a feature map per filter. The value of said filters are, in practice, learned by the network during the training process while other hyperparameters, such as the number of filters as well as their respective sizes are pre-determined by the network architect. Other things to keep note of are that the resulting feature maps are reduced in dimensionality when compared to the input; however this can be offset by utilizing a variation of techniques such as the application of a form of \textit{padding}.

\begin{figure}[hbt!]
    \centering
    \includegraphics[width=\textwidth]{Images/Chapter 3/CNN/Convolution-Illustration.pdf}
    \caption{An example of a convolutional kernel at work. A 3x3 kernel traverses over a 5x5 "image" with a stride of 1 to produce the convolved feature map.}
    \label{fig:Convolution-Illustration}
\end{figure}

\subsubsection{Rectified Linear Unit Operation}
\label{subsubsec:Background-Information:Forecasting-Models:Convolutional-Neural-Networks:ReLU}
Following every convolution operation is a \gls{relu} operation where \gls{relu} is a non-linear operation whose output is given by:

\begin{equation}
    R(z) = \text{max}(0, z)
\label{eq:ReLU}
\end{equation}

\noindent \newline The purpose of the \gls{relu} operation is to replace all negative values in the feature map by zero. This nonlinear function allows for the use of stochastic gradient descent with backpropagation of errors that enables us to learn complex relationships within the data.  Other operations, or activation functions, such as \textit{tanh} or \textit{sigmoid} can also be used here but generally \gls{relu} performs much better in most situations and is much quicker to perform due to its sheer simplicity.

\begin{figure}[hbt!]
    \centering
    \includegraphics[width=\textwidth]{Images/Chapter 3/CNN/ReLU-Illustration.pdf}
    \caption{A simplified demonstration of a \gls{relu} operation.}
    \label{fig:ReLU-Illustration}
\end{figure}

\noindent \newline It is worth mentioning that in contrast to the \gls{relu} activation function we will be utilizing the leaky \gls{relu} activation function (illustrated in Figure \ref{fig:Leaky-ReLU}) so as to avoid the "dying" \gls{relu} problem in which the \gls{relu} neurons of a network always output a value of 0 thus effectively not contributing anything to the learning of the network.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Images/Chapter 5/Stage 4/Other/Leaky-ReLU.pdf}
    \caption{Illustration of leaky \gls{relu}.}
    \label{fig:Leaky-ReLU}
\end{figure}

\subsubsection{Max Pooling Layers}
\label{subsubsec:Background-Information:Forecasting-Models:Convolutional-Neural-Networks:Max-Pooling-Layers}
Inter-mingled between convolutional layers are a set of pooling layers that serve to reduce the dimensionality of each feature map while retaining the most important information. In the case of \textit{max} pooling layers, the network defines a spatial neighborhood and takes the largest element from the rectified feature map within that window. The goal of pooling layers then is to reduce the feature dimensions of our input vectors thus making them smaller and more manageable to work with while also reducing the number of parameters and computations needed to fit our network, thus minimizing the risk of overfitting. Furthermore, this renders the network invariant to small transformations, distortions and translations in the input vector by providing us with what is essentially a scale invariant representation of our vector.

\begin{figure}[hbt!]
    \centering
    \includegraphics[width=\textwidth]{Images/Chapter 3/CNN/Max-Pooling-Illustration.pdf}
    \caption{An example of max pooling using a 2x2 window.}
    \label{fig:Max-Pooling-Illustration}
\end{figure}

\subsection{Long Short-Term Memory Networks}
\label{subsec:Background-Information:Forecasting-Models:Long-Short-Term-Memory-Networks}
\glsentryfull{lstm} networks, first proposed by Hochreiter \& Schmidhuber in 1997 \cite{Hochreiter}, are a special kind of \gls{rnn} network that are capable of learning long-term dependencies while overcoming the main limitations that plagued traditional \gls{rnn} networks (such as the exploding/vanishing gradient problem). The cell state of an \gls{lstm} can be seen as a highway that transfers relative information all the way down the sequence chain allowing information throughout the processing of the sequence to be retained giving the network a form of \textit{"memory"}. The key to the functionality of the \gls{lstm} is through the use of a number of gates that give it the ability to remove or add information to this cell state by learning what information is relevant to keep, or otherwise forget, during training. The following sections will serve to outline the functionality of the cell state and gates so that we may gain a better understanding of them.

\begin{figure}[hbt!]
    \centering
    \includegraphics[width=\textwidth]{Images/Chapter 3/LSTM/LSTM-Chain-Illustration.pdf}
    \caption{The repeating module in an \gls{lstm} that contains four interacting layers. Image source: \cite{Colah}  (with permission from the author).}
    \label{fig:LSTM-Chain-Illustration}
\end{figure}

\subsubsection{Forget Gate}
\label{subsubsec:Background-Information:Forecasting-Models:Long-Short-Term-Memory-Networks:Forget-Gate}
The first gate that we will be taking a look at is the forget gate $(f_t)$. This gate decides what information should be thrown away and what information should be kept from prior steps. Information from the previous hidden state $(h_{t-1})$ and information from the current input $(x_t)$ are passed through a \textit{sigmoid} $(\sigma)$ function where values come out between 0 and 1 for each number in the cell state $(C_{t-1})$. Values closer to and including 0 indicate that we should completely forget this information while values closer to and including 1 indicate that we should completely retain all of this information. The formulation of the forget gate can be seen in equation \ref{eq:LSTM-Forget-Gate}.

\begin{equation}
    f_t = \sigma (W_f \cdot \left[h_{t-1}, x_t\right] + b_f)
\label{eq:LSTM-Forget-Gate}
\end{equation}

\begin{figure}[hbt!]
    \centering
    \includegraphics[width=0.5\textwidth]{Images/Chapter 3/LSTM/LSTM-Forget-Gate-Illustration.pdf}
    \caption{An illustration of the forget gate in an \gls{lstm} network. Image source: \cite{Colah}  (with permission from the author).}
    \label{fig:LSTM-Forget-Gate-Illustration}
\end{figure}

\subsubsection{Input Gate}
\label{subsubsec:Background-Information:Forecasting-Models:Long-Short-Term-Memory-Networks:Input-Gate}
The input gate $(i_t)$ mainly serves to decide what new information will be stored in the cell state from the current step. The input gate is a \textit{sigmoid} $(\sigma)$ function that is passed the previous hidden state $(h_{t-1})$ and the current input $(x_t)$ and outputs values between 0 and 1 where values closer to and including 0 indicate that the information is not important while values closer to and including 1 indicate that the information is important. This value is multiplied by a \textit{tanh} layer that serves the purpose of creating a vector of new candidate values $(\tilde{C}_t)$ that could potentially be added to the cell state. The formulation of the input gate and its respective layers can be seen in equations \ref{eq:Input-Gate}.

\begin{align}
    \begin{split}
        & i_t = \sigma (W_i \cdot \left[h_{t-1}, x_t \right] + b_i) \\
        & \tilde{C}_t = tanh (W_C \cdot \left[h_{t-1}, x_t \right] + b_C)
    \end{split}
\label{eq:Input-Gate}
\end{align}

\begin{figure}[hbt!]
    \centering
    \includegraphics[width=0.5\textwidth]{Images/Chapter 3/LSTM/LSTM-Input-Gate-Illustration.pdf}
    \caption{An illustration of the input gate in an \gls{lstm} network. Image source: \cite{Colah} (with permission from the author).}
    \label{fig:LSTM-Input-Gate-Illustration}
\end{figure}

\subsubsection{Output Gate}
\label{subsubsec:Background-Information:Forecasting-Models:Long-Short-Term-Memory-Networks:Output-Gate}
The final gate is the output gate $(o_t)$ which decides what the next hidden state $(h_t)$ should be. As like in the previous gates, we pass the previous hidden state $(h_{t-1})$ and the current input $(x_t)$ into a \textit{sigmoid} $(\sigma)$ function which is multiplied by the output of the \textit{tanh} function applied to the modified cell state $(C_t)$ which finally gives us our new hidden state. The new hidden state as well as the new cell state are carried over to the next time step. The formulation of the output gate and its respective layers can be seen in equations \ref{eq:LSTM-Output-Gate}.

\begin{align}
    \begin{split}
        & C_t = f_t * C_{t-1} + i_t * \tilde{C}_t \\
        & o_t = \sigma (W_o \left[h_{t-1}, x_t \right] + b_o) \\
        & h_t = o_t * tanh(C_t)
    \end{split}
\label{eq:LSTM-Output-Gate}
\end{align}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{Images/Chapter 3/LSTM/LSTM-Output-Gate-Illustration.pdf}
    \caption{An illustration of the output gate in an \gls{lstm} network. Image source: \cite{Colah}  (with permission from the author).}
    \label{fig:LSTM-Output-Gate-Illustration}
\end{figure}

\clearpage

\section{Performance Metrics}
\label{sec:Background-Information:Performance-Metrics}
Throughout the duration of this project we will be making use of a variety of performance metrics. These performance metrics, as well as the reasoning behind choosing them, will be explained in the following sections.

\subsection{Mean Absolute Error}
\label{subsec:Background-Information:Performance-Metrics:Mean-Absolute-Error}
The first performance metric that we will be taking a look at is the \gls{mae}. It provides us with a direct interpretation of how far off the predictions made by our forecasting models were from the actual, ground truth. However, the \gls{mae} does not provide us with the capability of drawing comparisons between results obtained from disparate data sets as it is a scale-dependent metric. That said it provides a satisfactory level of insight nonetheless. Its equation is:

\begin{equation}
    \text{MAE} = \frac{\sum\limits_{i = 1}^{n} \abs{\hat{y_i} - y_i}}{n}
\label{eq:MAE}
\end{equation}

\noindent where:

\begin{conditions*}
        \hat{y}_i   &   predicted value. \\
        y_i         &   actual value. \\
        n           &   total number of data points.
\end{conditions*}

\subsection{Mean Absolute Percentage Error}
\label{subsec:Background-Information:Performance-Metrics:Mean-Absolute-Percentage-Error}
The second performance metric we will be taking a look at is the \glsentryfull{mape}. As it is a scale-invariant metric, its primary purpose is to allow us to assess the performance of our forecasting models across the multiple, disparate data sets we have on hand and draw comparisons between them. Its equation is:

\begin{equation}
    \text{MAPE} = \frac{1}{n} \sum\limits_{i = 1}^{n} \abs{\frac{\hat{y_i} - y_i}{y_i}}
\label{eq:MAPE}
\end{equation}

\noindent where:

\begin{conditions*}
        \hat{y}_i   &   predicted value. \\
        y_i         &   actual value. \\
        n           &   total number of data points.
\end{conditions*}

\subsection{Log-Cosh Loss}
\label{subsec:Background-Information:Performance-Metrics:Log-Cosh-Loss}
The final metric that we will be working with is the log-cosh function. Its primary purpose is to serve as a cost function that our forecasting models will seek to minimize. The primary reason behind choosing log-cosh to act as our cost function is that it is robust against the occasional wildly incorrect prediction that our networks are bound to make. Its equation is:

\begin{equation}
\centering
    \text{Log-Cosh L} = \sum\limits_{i = 1}^{n} \log(\cosh{(\hat{y}_i - y_i)})
\label{eq:Log-Cosh}
\end{equation}

\noindent where:

\begin{conditions*}
        \hat{y}_i   &   predicted value. \\
        y_i         &   actual value. \\
        n           &   total number of data points.
\end{conditions*}