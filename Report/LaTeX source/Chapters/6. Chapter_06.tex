\chapter{Methodology}
\label{ch:Methodology}
This paper proposes a forecasting method that utilizes dimensionality reduction and clustering techniques to group days that exhibit similarity in terms of electric consumption behaviour. Days that are grouped into the same cluster are thought to contain shared features, whether those features be temporal or meteorological or otherwise, that cause this similarity in behaviour. The formed clusters (per household) are used for 2 purposes: firstly, they will be used to train a classification model that utilizes available context information to assign a new day to the correct cluster. Secondly, and finally, a novel deep learning method will be applied on a per-cluster basis to forecast future energy consumption. An outline of the proposed method can be seen in Figure \ref{fig:Proposed-Model}.

\begin{figure}[H]
    \begin{adjustwidth}{-3.0cm}{-3.0cm}%
        \centering
        \includegraphics[width=\linewidth]{Images/Chapter 6/Other/Proposed-Model.pdf}
        \caption{Proposed daily profile extraction and load forecasting model.}
        \label{fig:Proposed-Model}
    \end{adjustwidth}
\end{figure}

\section{Stage 1 - Data Collection and Cleaning}
\label{sec:Methodology:Stage-1}
\begin{adjustwidth}{-1.65cm}{0.0cm}%
    \begin{enumerate}[label=Step 1.\arabic*:, leftmargin=*]
    \setlength\itemsep{1em}
        \item As mentioned prior, this paper utilizes available historical data with regards to energy consumption on an individual household basis. In reality, as part of stage 1 of our forecasting pipeline, time series data of daily electricity consumption would need to be collected from an individual household meter for an adequate amount of time at an ideal resolution so as to obtain acceptable results.
        \item After collecting, or in our case loading in, the data we perform common preprocessing techniques to account for noisy or otherwise missing data that occurred during the transmission of the data from the meters. In our case, the available data was resampled into a resolution of 15 minutes and any days that contained less than 96 values (given that there are 96 15 minute chunks in a day) were dropped from our data set. All other days that contained \gls{nan} values were also not considered and subsequently dropped from our data set.
    \end{enumerate}
\end{adjustwidth}

\section{Stage 2 - Dimensionality Reduction and Clustering}
\label{sec:Methodology:Stage-2}
\begin{adjustwidth}{-1.65cm}{-0.0cm}%
    \begin{enumerate}[label=Step 2.\arabic*:, leftmargin=*]
    \setlength\itemsep{1em}
        \item Given that each day in our data set is represented by 96 dimensions, each dimension comprising mean active power consumption over a time period of 15 minutes, the first logical step to undertake would be to transform the data in a manner that enables our clustering techniques to more efficiently determine which days exhibit similarity in terms of electric consumption behaviour. This \textit{dimensionality reduction} step comprises 2 parts that are outlined in the substeps below.
        
        \noindent \newline To start things off we divide each day into 5 different periods as follows:
            \begin{enumerate}[label=\arabic*:, leftmargin=.75cm]
                \item Morning: 06:00 - 11:00
                \item Late morning/afternoon: 11:00 - 15:00
                \item Late afternoon/early evening: 15:00 - 20:30
                \item Evening: 20:30 - 23:30
                \item Late evening/early morning: 23:30 - 06:00
            \end{enumerate}
        
        \noindent \newline Following that, we represent each period by its respective mean, minimum, maximum value as well as its standard deviation. The outcome of performing this is that each day is now represented by a total of 20 dimensions rather than the initial 96 which is a reduction of $\sim 80\%$.
        
        \noindent \newline We can reduce this even further, and even visualize our data in 2 or 3 dimensions, by making use of either of the \gls{t-sne} or \gls{umap} algorithms outlined in Section \ref{subsec:Background-Information:Dimension-Reduction:t-Distributed-Stochastic-Neighbor-Embedding}. The most important hyperparameter to tune for either algorithm is the \textit{perplexity} hyperparameter for the \gls{t-sne} algorithm and the equivalent $n_{\text{neighbors}}$ hyperparameter for the \gls{umap} algorithm. During our research, we found that an optimal value for either of these hyperparameters is $N^{\frac{1}{2}}$ where N is the number of samples present in the data set. To better understand each of the steps of our proposed model, we will begin a series of visualizations showcasing each step as performed on the \gls{ucid} data set starting off with the output of performing the \gls{t-sne} algorithm which can be seen in Figure \ref{fig:UCID-t-SNE}.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=\textwidth]{Images/Chapter 6/Stage 2/UCID/UCID-t-SNE.pdf}
            \caption{The output of performing the \gls{t-sne} algorithm on the 20-dimensional \gls{ucid} data set. Each point in this figure represents a single sample (or day) within our data set mapped onto a 2-dimensional surface.}
            \label{fig:UCID-t-SNE}
        \end{figure}
        
        \item After performing the dimensionality reduction step on our data, we proceed to cluster the resulting output by applying the \gls{hdbscan} algorithm, as outlined in Section \ref{subsec:Background-Information:Clustering-Algorithms:HDBSCAN}. As previously mentioned, the only important parameters that need to be passed to the \gls{hdbscan} algorithm are the minimum size we expect each cluster to be. In this case we set that value to $\frac{1}{10}\left(N\right)$ where N is the number of samples present in the data set. Our reasoning for selecting this value is predominantly based on the adequate results observed by \citet{Kong} in their implementation of the \gls{dbscan} algorithm in a similar setting whilst utilizing a similar selection in terms of hyperparameter settings. The other hyperparameter we choose to tune is the \textit{min\_samples} hyperparameter which, in layman's terms, denotes how conservative we would like to be with our clustering in terms of restricting clusters to progressively more dense areas and classifying samples from our data set as noise. In our case, an arbitrary value of 15 was selected, in contrast to the default value that sets $min\_samples = min\_cluster\_size$. The results of performing the \gls{hdbscan} algorithm on our 2-dimensional representation of the \gls{ucid} data set (represented by Figure \ref{fig:UCID-t-SNE}) can be seen in Figure \ref{fig:UCID-HDBSCAN-1}.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=\textwidth]{Images/Chapter 6/Stage 2/UCID/UCID-HDBSCAN-1.pdf}
            \caption{The output of performing the \gls{hdbscan} algorithm on the 2-dimensional \gls{ucid} data set previously seen in Figure \ref{fig:UCID-t-SNE}.}
            \label{fig:UCID-HDBSCAN-1}
        \end{figure}
        
        \noindent \newline For the sake of comparison, we present the output of applying the k-means clustering algorithm (assuming $k = 3$) on the same 2-dimensional representation of the \gls{ucid} data set. This can be seen in Figure \ref{fig:UCID-K-Means}. We note immediately the capability of the \gls{hdbscan} algorithm in capturing a better representation of the clusters present in our 2-dimensional representation of the \gls{ucid} data set. The representation of outliers as noise points and not having to have a priori knowledge on the number of clusters present in the data we are working with is a definite pro as well further compounding our choice of clustering algorithm in our proposed model.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=\textwidth]{Images/Chapter 6/Stage 2/UCID/UCID-K-Means.pdf}
            \caption{The output of performing the k-means algorithm on the 2-dimensional \gls{ucid} data set previously seen in Figure \ref{fig:UCID-t-SNE}.}
            \label{fig:UCID-K-Means}
        \end{figure}
        
        \item Visualizing, or otherwise manually inspecting, the clusters we obtain as a result of our application of the \gls{hdbscan} algorithm is necessary so that we maybe be better able to understand whether our clustering algorithm truly captures the habits of the individuals residing in the households we are working with. The first step in our analysis of the resulting clusters would be to plot the averaged power consumption on a per cluster basis so that we may be able to clearly visualize the patterns in power consumption per cluster. An example of this, in line with the previous examples showcasing our proposed model on the \gls{ucid} data set, can be seen in Figure \ref{fig:UCID-HDBSCAN-2}. We note that, in this example, a subset of our data (24 samples in total), were recorded as noise by the \gls{hdbscan} algorithm. Inspecting these samples manually lead to the confirmation that, of the 4 year's worth of data, these 24 days were the only days that exhibited no tangible shift in terms of power consumption throughout the entirety of the day; however, this is not explicitly outlined in the documentation of the \gls{ucid} data set. This can be seen as a more or less flat line in Figure \ref{fig:UCID-HDBSCAN-2}.
    
        \begin{figure}[hbt!]
            \centering
            \includegraphics[width=\textwidth]{Images/Chapter 6/Stage 2/UCID/UCID-HDBSCAN-2.pdf}
            \caption{Average power consumption per hour of the day for each of the resulting clusters obtained after utilizing the \gls{hdbscan} algorithm on our 2-dimensional representation of the \gls{ucid} data set.}
            \label{fig:UCID-HDBSCAN-2}
        \end{figure}
        
        \noindent \newline Following this, Figures \ref{fig:UCID-HDBSCAN-3} and \ref{fig:UCID-HDBSCAN-4} help us visualize the distribution of the clusters over the months of the year as well as the days of the week to ascertain whether any of the clusters present any correlation with these temporal variables. Given that the initial spread of the data throughout the months of the year and days of the week of the \gls{ucid} were relatively uniform, we should not see any bias towards any particular month or day in either Figure \ref{fig:UCID-HDBSCAN-3} or Figure \ref{fig:UCID-HDBSCAN-4} respectively.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=\textwidth]{Images/Chapter 6/Stage 2/UCID/UCID-HDBSCAN-3.pdf}
            \caption{Distribution of the clusters over the different months of the year.}
            \label{fig:UCID-HDBSCAN-3}
        \end{figure}
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=\textwidth]{Images/Chapter 6/Stage 2/UCID/UCID-HDBSCAN-4.pdf}
            \caption{Distribution of the clusters over the different days of the week.}
            \label{fig:UCID-HDBSCAN-4}
        \end{figure}
        
        \noindent \newline At a glance, we notice that clusters 1 and 2 are more likely to occur on the weekdays with cluster 3 taking over the majority share of the weekend which tends to explain the more consistent draw in power throughout the entirety of the day for samples that belong to cluster 3. Furthermore, samples in cluster 1 tend to gravitate towards the warmer, summery months peaking in terms of number occurrences in the month of July while samples in clusters 2 and 3 exhibit a more uniform spread over the remainder of the colder months which could explain the lower average draw in power present in samples that belong to cluster 1 being a result of the owners of the home not being in as often or potentially not needing to make use of appliances to heat up their home (we note that this data was collected in Sceaux, France that experiences a warm season of $\sim \! 3$  months with otherwise, generally, cooler temperatures). 
        
        \begin{figure}[hbt!]
            \centering
            \includegraphics[width=\textwidth]{Images/Chapter 6/Stage 2/UCID/UCID-HDBSCAN-5.pdf}
            \caption{Spread in number of samples per cluster label.}
            \label{fig:UCID-HDBSCAN-5}
        \end{figure}
        
        \noindent \newline \textit{N.B.: It is worth noting that performing these same steps on households from within the \gls{refit} data set exhibit similar results.}
    \end{enumerate}
\end{adjustwidth}

\section{Stage 3 - Further Data Preprocessing}
\label{sec:Methodology:Stage-3}
\begin{adjustwidth}{-1.65cm}{-0.0cm}%
    \begin{enumerate}[label=Step 3.\arabic*:, leftmargin=*]
    \item The details pertaining to the majority of the steps undertaken throughout the entirety of stage 3 of the proposed model have, to an extent, been explained in-depth during the \gls{eda} performed in Sections \ref{sec:Exploratory-Data-Analysis:REFIT} and \ref{sec:Exploratory-Data-Analysis:UCID}. Nonetheless, a brief summary will be provided as part of Chapter \ref{ch:Methodology}. The first step undertaken, again on a per-cluster basis, is to append both temporal data as well as meteorological data to our data sets. Table \ref{tab:Temporal-variables} pertains to the temporal variables that we will be taken into consideration as part of this feature engineering step while Table \ref{tab:Solcast-parameters} pertains to the obtained, historical meteorological data that concern the regions associated with our data sets. Incidentally, as outlined in Table \ref{tab:Temporal-variables}, the temporal variables we have chosen to append do not hold much value given their current format. This is due mostly in part to their cyclical nature (think of how the 23rd hour of the day is rather close to hours 0 and 1). To handle this we can encode our temporal variables (for example, through the use of both the sine and cosine function) in an attempt to transpose our linear interpretation of time into a cyclical state that can be better interpreted by our deep learning model further down the line. The result of performing this so-called encoding can be seen in both Figures \ref{fig:Temporal-Illustration-1} and \ref{fig:Temporal-Illustration-2}.
    
    \begin{figure}[H]
        \begin{adjustwidth}{-3.0cm}{-3.0cm}%
                \myfloatalign
                \subfloat[Representing the time of the day as a combination of both sine and cosine waves. ]
                {
                        \includegraphics[width=0.7\textwidth]{Images/Chapter 6/Stage 3/Temporal_Illustration/Temporal_Illustration_1.pdf}
                        \label{fig:Temporal-Illustration-1}
                } \quad
                \subfloat[Visualizing our cyclical encoding of the time of day.]
                {
                        \includegraphics[width=0.7\textwidth]{Images/Chapter 6/Stage 3/Temporal_Illustration/Temporal_Illustration_2.pdf}
                        \label{fig:Temporal-Illustration-2}
                } \quad
                \caption{By utilizing a combination of the sine function and the cosine function, we eliminate the possibility that two different times would receive the same value had we used either function independently. The combination of both functions can be thought of as an artifical 2-axis coordinate system that represents the time of day.}
        \end{adjustwidth}
    \end{figure}
    
    \item Following the feature engineering process, the feature selection process (or feature minimization process), heavily revolves around minimizing the overall number of features that do not serve as good predictors of our target variable. This has been explored predominantly in Sections \ref{subsec:Exploratory-Data-Analysis:REFIT:Causality-and-Correlation} and \ref{subsec:Exploratory-Data-Analysis:UCID:Causality-and-Correlation}. To summarize on the steps undertaken in the sections mentioned prior -- we performed a series of tests to determine whether our variables (temporal or meteorological) present a significant level of, independent (or combinatorial), correlation or causation against our target variable for each of the \gls{refit} and \gls{ucid} data sets. The primary tests conducted revolved around the concepts of Granger Causality and mutual information gain although other factors (such as a per-variable variance threshold) were also looked into. In lieu of repeating content and prolonging this section, we refer the reader to the results we procured in the previously mentioned sections.
    
    \item When taking our target variable into consideration, the notion of outliers (and how to deal with them), is inevitable. Scaling the values in such a fashion that accounts for outliers is one possibility whilst defining a threshold and trimming outlier values is another possibility. Alternatively, leaving them in is another possibility as some level of noise is unavoidable in the data collection process and training our models on unrealistically curated data does not serve to produce an accurate representation of a real-life scenario in which a model of this calibre could be applied. Nonetheless, we explore a few possibilities with respect to dealing with outlier values. One possibility, assuming a Gaussian distribution of the values of our target variable, would be to remove all values that fall a pre-defined number of standard deviations, generally 2 or 3, away from the mean. Unfortunately, the distribution of our target variable does not fall under this pre-condition (as seen in Figure \ref{fig:UCID-GAP-Distribution}) and so this is not a feasible option. Another method, explored during prior, related research \cite{Kareem}, worked on the basis of defining an upper and lower bound based on the \gls{iqr}. The \gls{iqr} is calculated as the difference between the 75th (Q3) and 25th (Q1) percentiles of the data and comprises the box in a traditional box and whiskers plot. Using the \gls{iqr} we can define outliers as any values that are a pre-defined factor below the 25th percentile or above the 75th percentile as follows:
    
    \begin{equation}
        Q1 - (1.5 * IQR) < x < Q3 + (1.5 * IQR)
    \label{eq:IQR-Outliers}
    \end{equation}
    
    \noindent \newline Figures \ref{fig:UCID-Box-and-Whiskers-GAP-Out} and \ref{fig:UCID-Box-and-Whiskers-GAP-NoOut} represent the distribution of values for our target variable over the different months of the year both before and after removing outliers respectively.

    
    \begin{figure}[hbt!]
        \begin{adjustwidth}{-3.0cm}{-3.0cm}%
                \myfloatalign
                \subfloat[Before removing outliers.]
                {
                        \includegraphics[width=0.7\textwidth]{Images/Chapter 6/Stage 3/UCID/UCID-Box-and-Whiskers-GAP-Out.pdf}
                        \label{fig:UCID-Box-and-Whiskers-GAP-Out}
                } \quad
                \subfloat[After removing outliers.]
                {
                        \includegraphics[width=0.7\textwidth]{Images/Chapter 6/Stage 3/UCID/UCID-Box-and-Whiskers-GAP-NoOut.pdf}
                        \label{fig:UCID-Box-and-Whiskers-GAP-NoOut}
                } \quad
                \caption{Illustrating the distribution of values with respect to the global active power of the \gls{ucid} data set both before and after removing outlier values as defined by Equation \ref{eq:IQR-Outliers}.}
        \end{adjustwidth}
    \end{figure}
    
    \noindent \newline \textit{Smoothing}, or otherwise filtering, the data can also be done through the use of a variety of techniques and can help alleviate some of the issues inherent to the noise present in our data as a byproduct of the data collection process. An example of performing a preliminary smoothing step on energy consumption data can be seen in the work of \citet{Hsiao} in which a moving (or rolling) average method was utilized. With regards to our proposed forecasting pipeline, we will be utilizing Savitzky-Golay filters to smooth our raw, electrical energy consumption data as, when compared to the moving average method, Savitzky-Golay filters tend to do a better job at preserving the integrity of the raw data.  Figures \ref{fig:UCID-Smoothing-Rolling-Average} and \ref{fig:UCID-Smoothing-Savitzy-Golay-Filter} serve to illustrate the application of both the moving average method as well as the Savitzky-Golay filter method on a subset of our (raw) data set in order to better visualize the differences between both methods.
    
    \begin{figure}[hbt!]
        \begin{adjustwidth}{-3.0cm}{-3.0cm}%
                \myfloatalign
                \subfloat[Application of the moving average method with a window size of 3.]
                {
                        \includegraphics[width=0.7\textwidth]{Images/Chapter 6/Stage 3/UCID/UCID-Smoothing-Rolling-Average.pdf}
                        \label{fig:UCID-Smoothing-Rolling-Average}
                } \quad
                \subfloat[Application of the Savitzky-Golay filter method with a polynomial order of 3 and a window size of 5.]
                {
                        \includegraphics[width=0.7\textwidth]{Images/Chapter 6/Stage 3/UCID/UCID-Smoothing-Savitzky-Golay-Filter.pdf}
                        \label{fig:UCID-Smoothing-Savitzy-Golay-Filter}
                } \quad
                \caption{Illustrating the application of both the moving average method as well the Savitzky-Golay filter method in smoothing on a subset of our raw data.}
        \end{adjustwidth}
    \end{figure}
    
    \noindent \newline Similarily, when considering the trend component of our data (previously defined in Section \ref{subsec:Exploratory-Data-Analysis:REFIT:Data-Visualization} and visualized in Figure \ref{fig:UCID-Time-Series-Decomposition}); a preliminary smoothing step can be undertaken through the use of \gls{loess} -- this is illustrated in Figure \ref{fig:UCID-Trend-Smoothing-LOESS}.
    
    \begin{figure}[hbt!]
        \centering
        \includegraphics[width=\textwidth]{Images/Chapter 6/Stage 3/UCID/UCID-Trend-Smoothing-LOESS.pdf}
        \caption{An illustration of the previously obtained trend component both with and without the application of \gls{loess}.}
        \label{fig:UCID-Trend-Smoothing-LOESS}
    \end{figure}
    
    \item The final step taken as part of Stage 3 of the forecasting pipeline is to split the data into 3 subsets that serve to act as training, validation and testing sets that will be fed to both our classification tree as well as the CNN-LSTM network that we will be using for the purpose of forecasting. A split employing an arbitrarily selected ratio of 60:20:20 is taken. Given the nature of our study, we choose not to shuffle the data in either of the generated sets as we are primarily interested in our model's capability of forecasting future trends in energy consumption given a measure of historically available data.
    \end{enumerate}
\end{adjustwidth}

\section{Stage 4 - Training and Testing}
\label{sec:Methodology:Stage-4}

\subsection{Stage 4.1 - Classification Tree}
\label{subsec:Methodology:Stage-4:Classification-Tree}

\subsection{Stage 4.2 - CNN-LSTM Network}
\label{subsec:Methodology:Stage-4:CNN-LSTM-Network}
\begin{adjustwidth}{-1.65cm}{-0.0cm}%
    \begin{enumerate}[label=Step 4.2.\arabic*:, leftmargin=*]
    \item The focal point of our research lies in the implementation of  a \gls{cnn-lstm} model in which the \gls{cnn} component serves to learn the relative importance of each of the features we pass to the network as input in what we can call a \textit{feature extraction} step. The extracted features are then passed to the \gls{lstm} portion of the network that learns the temporal relationship between past, or otherwise historical, values of said features with the present, or future, value(s) of the target variable where finally, an output prediction is made. An example network, built upon this foundational architecture, is visualized in Figure \ref{fig:UCID-CNN-LSTM-Model}.
    
    \begin{figure}[hbt!]
        \begin{adjustwidth}{-4.5cm}{-4.5cm}%
            \centering
            \includegraphics[width=\linewidth]{Images/Chapter 6/Stage 4/UCID/UCID-CNN-LSTM-Model.pdf}
            \caption{A simple, example \gls{cnn-lstm} network that makes one-step-ahead predictions.}
            \label{fig:UCID-CNN-LSTM-Model}
        \end{adjustwidth}
    \end{figure}
    
    The combination of both \gls{cnn} and \gls{lstm} components allows the network to learn spatio-temporal relationships between the features being passed as input and the target variable that we are attempting to forecast. In contrast to other architectures and forecasting models, this architecture is demonstrably more efficient \cite{Kim} when tackling time series problems such as that of residential energy consumption forecasting. The sample network illustrated in Figure \ref{fig:UCID-CNN-LSTM-Model} can be expanded to forecast multiple time steps ahead with minor adjustments and is capable of understanding patterns at variable time resolutions. For the purposes of this example, we will be moving forward with the previously defined resolution of 15 minutes using a window of 24 historical values $(t - 24, t - 23, ..., t)$ to make a prediction one step into the future $(t + 1)$ for both the previously established trend component as well as the raw, unadulterated data.
    
    \begin{figure}[H]
        \centering
        \includegraphics[width=\textwidth]{Images/Chapter 6/Stage 4/UCID/UCID-CNN-LSTM-Training-Validation-Loss.pdf}
        \caption{Training and validation loss when applying the previously defined network (illustrated in Figure \ref{fig:UCID-CNN-LSTM-Model}) on the raw \gls{ucid} data set in an attempt to make predictions one time step into the future.}
        \label{fig:UCID-CNN-LSTM-Training-Validation-Loss}
    \end{figure}
    
    \noindent \newline To train our network, we will be utilizing Adam \cite{Kingma}: an adaptive learning rate optimization algorithm that was designed specifically for training deep neural networks. In contrast to the ever-familiar \gls{sgd}, Adam leverages the power of adaptive learning rate methods and momentum to allocate individual learning rates for each parameter of the network being trained. For further explanations as to the workings of this algorithm we refer the reader to the initial paper by \citet{Kingma}.
    
    \begin{figure}[H]
        \begin{adjustwidth}{-3.0cm}{-3.0cm}%
                \myfloatalign
                \subfloat[An example of where our networks performs optimally, achieving a \gls{mape} of $\sim 11\%$.]
                {
                        \includegraphics[width=0.7\textwidth]{Images/Chapter 6/Stage 4/UCID/UCID-Forecast-1-Step-Ahead-Good-Example.pdf}
                        \label{fig:UCID-Forecast-1-Step-Ahead-Good-Example}
                } \quad
                \subfloat[An example of where our networks performs sub-optimally, achieving a \gls{mape} of $\sim 26\%$.]
                {
                        \includegraphics[width=0.7\textwidth]{Images/Chapter 6/Stage 4/UCID/UCID-Forecast-1-Step-Ahead-Poor-Example.pdf}
                        \label{fig:UCID-Forecast-1-Step-Ahead-Poor-Example}
                } \quad
                \caption{An illustration of one-step-ahead forecasts on 2 separate days in an attempt to showcase cases in which our network performs both optimally and sub-optimally.}
        \end{adjustwidth}
    \end{figure}
    
    \noindent \newline When training our network(s), we will be making use of a variety of techniques to improve generalization and prevent overfitting to the training data set(s). The first of these techniques is the notion of \textit{early stopping} (illustrated in Figure \ref{fig:Early-Stopping}). Early stopping is a form of regularization that monitors the validation loss (or generalization error) and aborts the training when the monitored values either begin to degrade or do not shift for an arbitrarily set number of epochs. The second technique we will be using works on the notion of employing a variable learning rate that, in theory, facilitates convergence of our weight update rule and prevents learning from stagnating thus allowing us to break through plateaus and avoid settling at local minima.
    
    \begin{figure}[H]
        \centering
        \includegraphics[width=\textwidth]{Images/Chapter 6/Stage 4/Other/Early-Stopping.pdf}
        \caption{Illustration of early stopping.}
        \label{fig:Early-Stopping}
    \end{figure}

    \noindent \newline For the purposes of our experiments and procuring the results showcased in Chapter \ref{ch:Results-and-Discussion}, we will be implementing a network on a per-cluster basis for both the raw data as well as the trend component of each of our data sets. The networks implemented will serve to provide one-step-ahead forecasts as well as one-shot 12-step-ahead (3 hour) forecasts as proof of concept.
    
    \noindent \newline \textit{N.B.: It is worth mentioning that in contrast to the \gls{relu} activation function defined in Section \ref{subsubsec:Background-Information:Forecasting-Models:Convolutional-Neural-Networks:ReLU} we will be utilizing the leaky \gls{relu} activation function (illustrated in Figure \ref{fig:Leaky-ReLU}) so as to avoid the "dying" \gls{relu} problem in which the \gls{relu} neurons of a network always output a value of 0 thus effectively not contributing anything to the learning of the network.}
    
    \begin{figure}[H]
        \centering
        \includegraphics[width=\textwidth]{Images/Chapter 6/Stage 4/Other/Leaky-ReLU.pdf}
        \caption{Illustration of leaky \gls{relu}.}
        \label{fig:Leaky-ReLU}
    \end{figure}

    \end{enumerate}
\end{adjustwidth}