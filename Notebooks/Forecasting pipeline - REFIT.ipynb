{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "boolean-posting",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "# Part 1: Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "returning-genius",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## 1.1: Initial setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-hayes",
   "metadata": {},
   "source": [
    "**Step 1:** Import the relevant packages and set Seaborn/Matplotlib hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handled-finnish",
   "metadata": {},
   "outputs": [],
   "source": [
    "import holidays\n",
    "import os\n",
    "\n",
    "import matplotlib.dates as md\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from statsmodels.tsa.stattools import adfuller, grangercausalitytests\n",
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "from tqdm import notebook\n",
    "\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "sns.set(style=\"whitegrid\", palette=\"muted\")\n",
    "\n",
    "plt.rcParams[\"axes.labelsize\"] = 26\n",
    "plt.rcParams[\"axes.titlesize\"] = 26\n",
    "plt.rcParams[\"figure.figsize\"] = 16, 10\n",
    "plt.rcParams[\"xtick.labelsize\"] = 22\n",
    "plt.rcParams[\"ytick.labelsize\"] = 22\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tired-utility",
   "metadata": {},
   "source": [
    "**Step 2:** Define the location of our data as well as the relevant columns that we would like to include."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contained-respondent",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "house_number = 12\n",
    "\n",
    "cols_REFIT = [\n",
    "    \"Time\",\n",
    "    \"Aggregate\",\n",
    "    \"Appliance1\",\n",
    "    \"Appliance2\",\n",
    "    \"Appliance3\",\n",
    "    \"Appliance4\",\n",
    "    \"Appliance5\",\n",
    "    \"Appliance6\",\n",
    "    \"Appliance7\",\n",
    "    \"Appliance8\",\n",
    "    \"Appliance9\",\n",
    "]\n",
    "\n",
    "data_directory_REFIT = os.path.join(\"Data\", \"REFIT\")\n",
    "data_directory_Solcast = os.path.join(\"Data\", \"Solcast_REFIT\")\n",
    "\n",
    "house = f\"CLEAN_House{house_number}.csv\"\n",
    "solcast_15 = \"Solcast_REFIT_15.csv\"\n",
    "\n",
    "file_destination_REFIT = os.path.join(data_directory_REFIT, house)\n",
    "file_destination_Solcast = os.path.join(data_directory_Solcast, solcast_15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "level-rotation",
   "metadata": {},
   "source": [
    "**Step 3:** Read in the data and save it to a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animated-prague",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_REFIT = pd.read_csv(file_destination_REFIT, index_col=0, parse_dates=True, usecols=cols_REFIT)\n",
    "df_Solcast = pd.read_csv(file_destination_Solcast, index_col=0, parse_dates=True)\n",
    "\n",
    "df_Solcast.index = df_Solcast.index.rename(\"Time\")\n",
    "df_Solcast.index = pd.to_datetime(df_Solcast.index).tz_localize(None)\n",
    "\n",
    "cols_REFIT.remove(\"Time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranking-david",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## 1.2: Scale the data in the dataframe(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "packed-possession",
   "metadata": {},
   "source": [
    "**Step 1.1:** Scale the data in a range between 0 and 1 (optional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adapted-gnome",
   "metadata": {},
   "outputs": [],
   "source": [
    "# minmax_REFIT = MinMaxScaler()\n",
    "# minmax_Solcast = MinMaxScaler()\n",
    "\n",
    "# df_REFIT[cols_REFIT] = minmax_REFIT.fit_transform(df_REFIT[cols_REFIT])\n",
    "# df_Solcast[cols_Solcast] = minmax_Solcast.fit_transform(df_Solcast[cols_Solcast])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stunning-paper",
   "metadata": {},
   "source": [
    "**Step 1.2:** Standardize the data by removing the mean and scaling to unit variance (optional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "terminal-joseph",
   "metadata": {},
   "outputs": [],
   "source": [
    "standardscale_REFIT = StandardScaler()\n",
    "# standardscale_Solcast = StandardScaler()\n",
    "\n",
    "df_REFIT[cols_REFIT] = standardscale_REFIT.fit_transform(df_REFIT[cols_REFIT])\n",
    "# df_Solcast[cols_Solcast] = standardscale_Solcast.fit_transform(df_Solcast[cols_Solcast])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "close-bankruptcy",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## 1.3: Merge the dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-coast",
   "metadata": {},
   "source": [
    "**Step 1:** Create a copy of our REFIT dataframe that is resampled into a resolution of 15 minutes and drop any days that contain an incomplete number of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "central-explanation",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_REFIT_resampled = df_REFIT.resample(\"15min\").mean()\n",
    "df_REFIT_resampled = df_REFIT_resampled.dropna()\n",
    "\n",
    "mask = df_REFIT_resampled.groupby(df_REFIT_resampled.index.date).size()\n",
    "mask = mask[mask < 96].index.to_list()\n",
    "\n",
    "df_REFIT_resampled = df_REFIT_resampled[~df_REFIT_resampled.index.floor(\"D\").isin(mask)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interracial-sympathy",
   "metadata": {},
   "source": [
    "**Step 2:** Create a third dataframe that is the result of merging the Solcast dataframe with the REFIT dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "leading-wholesale",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Merged = pd.merge(left=df_Solcast, left_on=df_Solcast.index, right=df_REFIT_resampled, right_on=df_REFIT_resampled.index)\n",
    "\n",
    "cols_Merged = [\n",
    "    \"PeriodStart\",\n",
    "    \"Period\",\n",
    "    \"Appliance1\",\n",
    "    \"Appliance2\",\n",
    "    \"Appliance3\",\n",
    "    \"Appliance4\",\n",
    "    \"Appliance5\",\n",
    "    \"Appliance6\",\n",
    "    \"Appliance7\",\n",
    "    \"Appliance8\",\n",
    "    \"Appliance9\",\n",
    "]\n",
    "\n",
    "df_Merged.drop(cols_Merged, axis=1, inplace=True)\n",
    "df_Merged.rename(columns={\"key_0\": \"Time\"}, inplace=True)\n",
    "df_Merged = df_Merged.set_index(\"Time\")\n",
    "df_Merged.index = pd.to_datetime(df_Merged.index)\n",
    "df_Merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opponent-chapter",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## 1.4: Append temporal features to our merged dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cloudy-point",
   "metadata": {},
   "source": [
    "**Step 1:** Append public holidays to our merged dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expanded-privilege",
   "metadata": {},
   "outputs": [],
   "source": [
    "UK_holidays = holidays.UnitedKingdom()\n",
    "df_Merged.insert(0, \"Holiday\", [1 if str(val).split()[0] in UK_holidays else 0 for val in df_Merged.index.date])\n",
    "df_Merged[\"Holiday\"] = df_Merged[\"Holiday\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noted-sampling",
   "metadata": {},
   "source": [
    "**Step 2:** Define day of the year ranges for each of the seasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imposed-surge",
   "metadata": {},
   "outputs": [],
   "source": [
    "spring = range(79, 172)\n",
    "summer = range(172, 266)\n",
    "fall = range(266, 355)\n",
    "\n",
    "def season(doy):\n",
    "    if doy in spring:\n",
    "        return \"Spring\"\n",
    "    if doy in summer:\n",
    "        return \"Summer\"\n",
    "    if doy in fall:\n",
    "        return \"Fall\"\n",
    "    else:\n",
    "        return \"Winter\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brave-saver",
   "metadata": {},
   "source": [
    "**Step 3:** Append temporal data to our merged dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strategic-cuisine",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Merged.insert(0, \"Year\", df_Merged.index.year)\n",
    "df_Merged.insert(1, \"Month\", df_Merged.index.month)\n",
    "df_Merged.insert(3, \"Day\", df_Merged.index.day)\n",
    "df_Merged.insert(4, \"Hour\", df_Merged.index.hour)\n",
    "df_Merged.insert(5, \"Minute\", df_Merged.index.minute)\n",
    "df_Merged.insert(6, \"Weekday\", df_Merged.index.weekday)\n",
    "# df_Merged.insert(8, \"Season\", df_Merged.index.dayofyear.map(season))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tropical-circle",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "# Part 2: Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automotive-sarah",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## 2.1: Enumerate values of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developed-entity",
   "metadata": {},
   "source": [
    "**Step 1:** We iterate over each of the houses available in our data set and calculate the total number of missing days, total number of days that contain *incomplete* data, total number of values recorded as well as the number of values that contain *issues* and save the results to a `.csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convinced-muslim",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output = {\n",
    "    \"House number\": [],\n",
    "    \"Date start\": [],\n",
    "    \"Date end\": [],\n",
    "    \"Total no. of days\": [],\n",
    "    \"No. of days missing\": [],\n",
    "    \"% of days missing\": [],\n",
    "    \"No. of days incomplete\": [],\n",
    "    \"% of days incomplete\": [],\n",
    "    \"Total no. of values recorded\": [],\n",
    "    \"No. of values missing data\": [],\n",
    "    \"% of values missing data\": [],\n",
    "    \"Number of values with issues\": [],\n",
    "    \"% of values with issues\": [],\n",
    "    \"Longest period of missing days\": [],\n",
    "}\n",
    "\n",
    "for i in notebook.trange(1, 22):\n",
    "    if i == 14:\n",
    "        continue\n",
    "\n",
    "    house_number = i\n",
    "    house = f\"CLEAN_House{house_number}.csv\"\n",
    "    file_destination = os.path.join(data_directory_REFIT, house)\n",
    "    df = pd.read_csv(file_destination, index_col=0, parse_dates=True)\n",
    "\n",
    "    # Define the starting date and ending date of our dataframe.\n",
    "    date_start = df.index.min()\n",
    "    date_end = df.index.max()\n",
    "\n",
    "    df_grouped = df.groupby(df.index.date).mean()\n",
    "    total_values = len(df)\n",
    "    total_days = len(pd.date_range(df_grouped.index.min(), df_grouped.index.max()))\n",
    "    \n",
    "    # Set up a dataframe with which to calculate incomplete days.\n",
    "    df_r = df.resample(\"15min\").mean()\n",
    "    df_r = df_r.dropna()\n",
    "\n",
    "    # Count the number of missing days.\n",
    "    days_missing = len(pd.date_range(df_grouped.index.min(), df_grouped.index.max()).difference(df_grouped.index))\n",
    "    percentage_days_missing = np.round((days_missing / total_days) * 100, 2)\n",
    "\n",
    "    # Count the number of days with incomplete data.\n",
    "    mask = df_r.groupby(df_r.index.date).size()\n",
    "    mask = mask[mask < 96].index.to_list()\n",
    "    days_incomplete = len(mask)\n",
    "    percentage_days_incomplete = np.round((days_incomplete / total_days) * 100, 2)\n",
    "\n",
    "    # Count the number of missing values.\n",
    "    values_missing = (df[\"Aggregate\"] == 0).astype(int).sum()\n",
    "    percentage_values_missing = np.round((values_missing / total_values) * 100, 2)\n",
    "\n",
    "    # Count the number of values with 'issues'.\n",
    "    values_issues = (df[\"Issues\"] == 1).astype(int).sum()\n",
    "    percentage_values_issues = np.round((values_issues / total_values) * 100, 2)\n",
    "\n",
    "    # Calculate the longest period of missing days.\n",
    "    df_grouped[\"Diff_Date\"] = df_grouped.index.to_series().diff()\n",
    "    df_grouped[\"Diff_Date\"] = df_grouped[\"Diff_Date\"] / np.timedelta64(1, \"D\")\n",
    "    stretch = df_grouped[\"Diff_Date\"].max().astype(int) - 1\n",
    "\n",
    "    output[\"House number\"].append(house_number)\n",
    "    output[\"Date start\"].append(date_start)\n",
    "    output[\"Date end\"].append(date_end)\n",
    "    output[\"Total no. of days\"].append(total_days)\n",
    "    output[\"No. of days missing\"].append(days_missing)\n",
    "    output[\"% of days missing\"].append(percentage_days_missing)\n",
    "    output[\"Total no. of values recorded\"].append(total_values)\n",
    "    output[\"No. of days incomplete\"].append(days_incomplete)\n",
    "    output[\"% of days incomplete\"].append(percentage_days_incomplete)\n",
    "    output[\"No. of values missing data\"].append(values_missing)\n",
    "    output[\"% of values missing data\"].append(percentage_values_missing)\n",
    "    output[\"Number of values with issues\"].append(values_issues)\n",
    "    output[\"% of values with issues\"].append(percentage_values_issues)\n",
    "    output[\"Longest period of missing days\"].append(stretch)\n",
    "    \n",
    "output = pd.DataFrame.from_dict(output).set_index(\"House number\")\n",
    "output.to_csv(\"Data Analysis - REFIT.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mechanical-proportion",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## 2.2: Visualize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intellectual-metro",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### 2.2.1: Line plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "private-cookbook",
   "metadata": {},
   "source": [
    "**Step 1:** Create a copy of our REFIT dataframe and reformat the index into a string so as to make sure that Matplotlib does not automatically interpolate missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enclosed-turning",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_REFIT_resampled_c = df_REFIT_resampled.copy()\n",
    "df_REFIT_resampled_c.index = df_REFIT_resampled_c.index.strftime(\"%d-%m-%y %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-sweden",
   "metadata": {},
   "source": [
    "**Step 2:** Drop all columns barre the `Aggregate` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dated-upper",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_REFIT_c = cols_REFIT.copy()\n",
    "cols_REFIT_c.remove(\"Aggregate\")\n",
    "df_REFIT_resampled_c.drop(cols_REFIT_c, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "black-marketing",
   "metadata": {},
   "source": [
    "**Step 3:** Plot the aggregate power consumption over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elect-parameter",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "df_REFIT_resampled_c[\"Aggregate\"].plot(ax=ax)\n",
    "\n",
    "ax.set_xlabel(\"\")\n",
    "ax.set_ylabel(\"Aggregate Power Consumption (Watts)\")\n",
    "ax.set_xlim(left=-5, right=len(df_REFIT_resampled_c) + 5)\n",
    "ax.set_ylim(bottom=df_REFIT_resampled_c[\"Aggregate\"].min() - 100)\n",
    "\n",
    "plt.setp(ax.get_xticklabels(), ha=\"right\", rotation=60)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disciplinary-horse",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### 2.2.2: Stack plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "renewable-bristol",
   "metadata": {},
   "source": [
    "**Step 1:** Create a copy of our REFIT dataframe that contains only the IAM readings (with their labels) and group by the averaged reading per hour of the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boxed-theme",
   "metadata": {},
   "outputs": [],
   "source": [
    "hours = []\n",
    "for i in range(0, 24):\n",
    "    if i < 10:\n",
    "        hours.append(f\"0{i}:00:00\")\n",
    "    if i >= 10:\n",
    "        hours.append(f\"{i}:00:00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shaped-posting",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Stacked = df_REFIT.iloc[:, 1:10].groupby(df_REFIT.index.hour).mean()\n",
    "\n",
    "df_Stacked = df_Stacked.rename(\n",
    "    index=str,\n",
    "    columns={\n",
    "        \"Appliance1\": \"Fridge-Freezer\",\n",
    "        \"Appliance2\": \"Unknown\",\n",
    "        \"Appliance3\": \"Unknown\",\n",
    "        \"Appliance4\": \"Computer Site\",\n",
    "        \"Appliance5\": \"Microwave\",\n",
    "        \"Appliance6\": \"Kettle\",\n",
    "        \"Appliance7\": \"Toaster\",\n",
    "        \"Appliance8\": \"Television\",\n",
    "        \"Appliance9\": \"Unknown\",\n",
    "    },\n",
    ")\n",
    "df_Stacked.index = hours\n",
    "df_Stacked.index = pd.to_datetime(df_Stacked.index, format=\"%H:%M:%S\")\n",
    "df_Stacked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increased-paper",
   "metadata": {},
   "source": [
    "**Step 2:** Create the stack plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transparent-techno",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "colors = [\"b\", \"orange\", \"g\", \"r\", \"purple\", \"brown\", \"pink\", \"gray\", \"yellow\"]\n",
    "\n",
    "ax.stackplot(df_Stacked.index.values, np.transpose(df_Stacked.values), labels=df_Stacked.columns.values, colors=colors)\n",
    "ax.legend(loc=\"upper left\", fontsize=18)\n",
    "ax.set_xlim(df_Stacked.index.min(), df_Stacked.index.max())\n",
    "ax.xaxis.set_major_locator(md.HourLocator())\n",
    "ax.xaxis.set_major_formatter(md.DateFormatter(\"%H:%M\"))\n",
    "\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Average Power Consumption (Watts)\")\n",
    "plt.xticks(df_Stacked.index.values)\n",
    "\n",
    "fig.autofmt_xdate()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-dance",
   "metadata": {},
   "source": [
    "**Step 3:** Create individual plots for each of the IAMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educated-carnival",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 3)\n",
    "i = 0\n",
    "colors = [\"b\", \"orange\", \"g\", \"r\", \"purple\", \"brown\", \"pink\", \"gray\", \"yellow\"]\n",
    "\n",
    "list = []\n",
    "for x in range(0, 24, 6):\n",
    "    list.append(df_Stacked.index[x])\n",
    "list.append(df_Stacked.index[-1])\n",
    "\n",
    "for nrow in range(0, 3):\n",
    "    for ncol in range(0, 3):\n",
    "        axs[nrow, ncol].plot(df_Stacked.index, df_Stacked.iloc[:, i], color=colors[i])\n",
    "        axs[nrow, ncol].set_xlim(df_Stacked.index.min(), df_Stacked.index.max())\n",
    "        axs[nrow, ncol].set_title(df_Stacked.columns[i], fontsize=18)\n",
    "        axs[nrow, ncol].xaxis.set_major_locator(md.HourLocator())\n",
    "        axs[nrow, ncol].xaxis.set_major_formatter(md.DateFormatter(\"%H:%M\"))\n",
    "        axs[nrow, ncol].set_xticks(list)\n",
    "        i = i + 1\n",
    "\n",
    "fig.autofmt_xdate()\n",
    "fig.add_subplot(111, frameon=False)\n",
    "\n",
    "plt.grid(False)\n",
    "plt.tick_params(labelcolor=\"none\", top=False, bottom=False, left=False, right=False)\n",
    "plt.xlabel(\"Time\", labelpad=25)\n",
    "plt.ylabel(\"Average Power Consumption (Watts)\", labelpad=15)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "provincial-tracy",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### 2.2.3: Count plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pretty-valuable",
   "metadata": {},
   "source": [
    "**Step 1:** Visualize the number of samples per day of the week over the entire data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "obvious-piano",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Merged_c = df_Merged.copy()\n",
    "df_Merged_c.insert(0, \"Weekday_name\", df_Merged.index.day_name())\n",
    "\n",
    "ax = sns.countplot(\n",
    "    x=df_Merged_c[\"Weekday_name\"],\n",
    "    data=df_Merged_c,\n",
    "    palette=\"icefire\",\n",
    "    order=[\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"],\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Weekday\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternative-woman",
   "metadata": {},
   "source": [
    "**Step 2:** Visualize the number of samples per month over the entire data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endless-browser",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Merged_c = df_Merged.copy()\n",
    "df_Merged_c.insert(0, \"Month_name\", df_Merged.index.month_name())\n",
    "\n",
    "ax = sns.countplot(\n",
    "    x=df_Merged_c[\"Month_name\"],\n",
    "    data=df_Merged_c,\n",
    "    palette=\"icefire\",\n",
    "    order=[\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"],\n",
    ")\n",
    "\n",
    "plt.setp(ax.get_xticklabels(), ha=\"right\", rotation=60)\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "younger-hampshire",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### 2.2.4: Stationarity plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "official-squad",
   "metadata": {},
   "source": [
    "**Step 1:** Test for stationarity and plot the result using the pre-defined `test_stationarity` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saving-interval",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stationarity(df_REFIT_resampled_c, 0.05, \"Aggregate\", \"Aggregate Power Consumption (Watts)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "piano-updating",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### 2.2.5: Box and whisker plots/outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acceptable-meter",
   "metadata": {},
   "source": [
    "**Step 1:** Box and whiskers plot for the aggregate as well as each of the IAM readings grouped by the months of the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dying-verse",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(len(cols_REFIT), 1, figsize=(16, 10 * (len(cols_REFIT))), sharex=True)\n",
    "\n",
    "for col, ax in zip([*cols_REFIT], axs):\n",
    "    sns.boxplot(\n",
    "        data=df_REFIT,\n",
    "        x=df_REFIT.index.month_name(),\n",
    "        y=col,\n",
    "        ax=ax,\n",
    "        order=[\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"],\n",
    "    )\n",
    "    ax.set_title(col)\n",
    "    ax.xaxis.set_label_text(\"\")\n",
    "\n",
    "plt.setp(ax.get_xticklabels(), ha=\"right\", rotation=60)\n",
    "plt.xlabel(\"Month\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "direct-collins",
   "metadata": {},
   "source": [
    "**Step 2:** We remove outliers that are 3 standard deviations away from the mean and repeat **step 1**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affecting-litigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = df_REFIT[(np.abs(stats.zscore(df_REFIT[\"Aggregate\"])) < 3)]\n",
    "print(f\"Number of outliers: {(len(df_REFIT) - len(df_out))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corresponding-tucson",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(len(cols_REFIT), 1, figsize=(16, 10 * (len(cols_REFIT))), sharex=True)\n",
    "\n",
    "for col, ax in zip([*cols_REFIT], axs):\n",
    "    sns.boxplot(\n",
    "        data=df_out,\n",
    "        x=df_out.index.month_name(),\n",
    "        y=col,\n",
    "        ax=ax,\n",
    "        order=[\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"],\n",
    "    )\n",
    "    ax.xaxis.set_label_text(\"\")\n",
    "\n",
    "plt.setp(ax.get_xticklabels(), ha=\"right\", rotation=60)\n",
    "plt.xlabel(\"Month\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roman-consequence",
   "metadata": {},
   "source": [
    "**Step 2.1:** Purely for the aggregate power consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tropical-plant",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(\n",
    "    data=df_out,\n",
    "    x=df_out.index.month_name(),\n",
    "    y=df_out[\"Aggregate\"],\n",
    "    order=[\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"],\n",
    ")\n",
    "\n",
    "plt.setp(ax.get_xticklabels(), ha=\"right\", rotation=60)\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Aggregate Power Consumption (Watts)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "negative-armor",
   "metadata": {},
   "source": [
    "**Step 3:** Box and whiskers plot for the aggregate as well as each of the IAM readings over the entirety of the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separated-aircraft",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.boxplot(data=df_out, ax=ax)\n",
    "\n",
    "plt.setp(ax.get_xticklabels(), ha=\"right\", rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "above-wrong",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## 2.3: Time series decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creative-concern",
   "metadata": {},
   "source": [
    "**Step 1:** Create a copy of our REFIT dataframe and reformat the index into a string so as to make sure that Matplotlib does not automatically interpolate missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documentary-kingston",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_REFIT_resampled_c = df_REFIT_resampled.copy()\n",
    "df_REFIT_resampled_c.index = df_REFIT_resampled_c.index.strftime(\"%d-%m-%y %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recovered-japan",
   "metadata": {},
   "source": [
    "**Step 2:** Drop all columns barre the `Aggregate` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painted-politics",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_REFIT_c = cols_REFIT.copy()\n",
    "cols_REFIT_c.remove(\"Aggregate\")\n",
    "df_REFIT_resampled_c.drop(cols_REFIT_c, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-conviction",
   "metadata": {},
   "source": [
    "**Step 3:** Define a period of 2-3 months (as an example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "circular-record",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_REFIT_resampled_c = df_REFIT_resampled_c.loc[\"2015-01-01\":\"2015-04-01\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integral-piece",
   "metadata": {},
   "source": [
    "**Step 4:** Perform time series decomposition using LOESS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "front-management",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = (24 * 60) // 15\n",
    "stl_decompose_result = STL(df_REFIT_resampled_c, period=freq).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suited-preview",
   "metadata": {},
   "source": [
    "**Step 5:** Separate/plot the obtained results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metallic-steering",
   "metadata": {},
   "outputs": [],
   "source": [
    "observed = stl_decompose_result.observed\n",
    "trend = stl_decompose_result.trend.to_frame()\n",
    "seasonal = stl_decompose_result.seasonal.to_frame()\n",
    "noise = stl_decompose_result.resid.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seven-isaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4, sharex=True)\n",
    "\n",
    "observed.plot(ax=axs[0], title=\"Observed\")\n",
    "trend.plot(ax=axs[1], title=\"Trend\")\n",
    "seasonal.plot(ax=axs[2], title=\"Seasonal\")\n",
    "noise.plot(ax=axs[3], title=\"Noise\")\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_xlim(0, right=len(observed))\n",
    "    ax.get_legend().remove()\n",
    "    ax.title.set_size(22)\n",
    "\n",
    "fig.text(0.03, 0.6, \"Aggregate Power Consumption (Watts)\", fontsize=\"22\", va=\"center\", rotation=\"vertical\")\n",
    "plt.setp(ax.get_xticklabels(), ha=\"right\", rotation=60)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recovered-guidance",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## 2.4: Causality and correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verbal-comedy",
   "metadata": {},
   "source": [
    "**Step 1:** Perform the Augmented Dicky-Fuller test to determine whether our time series is stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pediatric-serum",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, column in df_REFIT_resampled.iteritems():\n",
    "    adfuller_test(column, name=column.name)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compatible-crystal",
   "metadata": {},
   "source": [
    "**Step 2:** Check for autocorrelation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fallen-tracker",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acf(df_REFIT_resampled[\"Aggregate\"], lags=50, zero=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federal-salad",
   "metadata": {},
   "source": [
    "**Step 3:** Check for partial autocorrelation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustainable-gospel",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pacf(df_REFIT_resampled[\"Aggregate\"], lags=50, zero=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uniform-workstation",
   "metadata": {},
   "source": [
    "**Step 4:** Check for highly correlated features in the Solcast dataframe and drop them from our merged dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-tourism",
   "metadata": {},
   "outputs": [],
   "source": [
    "highly_correlated = correlation(df_Solcast, 0.7)\n",
    "df_Merged.drop(highly_correlated, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-polyester",
   "metadata": {},
   "source": [
    "**Step 5:** Estimate mutual information of our independent variables on our target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "induced-pixel",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = (df_Merged.loc[:, df_Merged.columns != \"Aggregate\"], df_Merged[\"Aggregate\"])\n",
    "mutual_info = mutual_info_regression(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caring-disabled",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutual_info = pd.Series(mutual_info)\n",
    "mutual_info.index = X.columns\n",
    "mutual_info.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atmospheric-windsor",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutual_info.sort_values(ascending=False).plot.bar()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aerial-asthma",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equal-tournament",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## Miscellaneous functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-drama",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### 1) Augmented Dickey–Fuller test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empirical-world",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adfuller_test(series, signif=0.05, name=\"\"):\n",
    "    r = adfuller(series, autolag=\"AIC\")\n",
    "    output = {\"test_statistic\": round(r[0], 4), \"pvalue\": round(r[1], 4), \"n_lags\": round(r[2], 4), \"n_obs\": r[3]}\n",
    "    p_value = output[\"pvalue\"]\n",
    "\n",
    "    def adjust(val, length=6):\n",
    "        return str(val).ljust(length)\n",
    "\n",
    "    print(f'      Augmented Dickey-Fuller Test on \"{name}\"', \"\\n   \", \"-\" * 47)\n",
    "    print(f\" Null Hypothesis: Data has unit root. Non-Stationary.\")\n",
    "    print(f\" Significance Level    = {signif}\")\n",
    "    print(f' Test Statistic        = {output[\"test_statistic\"]}')\n",
    "    print(f' No. Lags Chosen       = {output[\"n_lags\"]}')\n",
    "\n",
    "    for key, val in r[4].items():\n",
    "        print(f\" Critical value {adjust(key)} = {round(val, 3)}\")\n",
    "    if p_value <= signif:\n",
    "        print(f\" => P-Value = {p_value}. Rejecting Null Hypothesis.\")\n",
    "        print(f\" => Series is Stationary.\")\n",
    "    else:\n",
    "        print(f\" => P-Value = {p_value}. Weak evidence to reject the Null Hypothesis.\")\n",
    "        print(f\" => Series is Non-Stationary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intense-convention",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### 2) Augmented Dickey–Fuller test w/plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olympic-calgary",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_stationarity(series, signif=0.05, name=\"\", ylabel=\"\"):\n",
    "    def adjust(val, length=6):\n",
    "        return str(val).ljust(length)\n",
    "\n",
    "    rolmean = series.rolling(12).mean()\n",
    "    rolstd = series.rolling(12).std()\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    series.plot(ax=ax, alpha=0.5)\n",
    "    rolmean.plot(ax=ax, alpha=0.7)\n",
    "    rolstd.plot(ax=ax, alpha=0.7)\n",
    "\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_xlim(left=0, right=len(series))\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.title(\"Rolling Mean & Standard Deviation\")\n",
    "    plt.setp(ax.get_xticklabels(), ha=\"right\", rotation=60)\n",
    "\n",
    "    leg = plt.legend()\n",
    "    leg.get_texts()[0].set_text(name)\n",
    "    leg.get_texts()[1].set_text(\"Rolling Mean\")\n",
    "    leg.get_texts()[2].set_text(\"Rolling STD\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show(block=False)\n",
    "\n",
    "    adfuller_test(series, 0.05, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cathedral-sullivan",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### 3) Granger Causality test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anticipated-objective",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grangers_causation_matrix(data, variables, test=\"ssr_chi2test\", maxlag=12, verbose=False):\n",
    "    df = pd.DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)\n",
    "    for c in df.columns:\n",
    "        for r in df.index:\n",
    "            test_result = grangercausalitytests(data[[r, c]], maxlag=maxlag, verbose=False)\n",
    "            p_values = (1 - [round(test_result[i + 1][0][test][1], 4) for i in range(maxlag)])\n",
    "            if verbose:\n",
    "                print(f\"Y = {r}, X = {c}, P Values = {p_values}\")\n",
    "            min_p_value = np.min(p_values)\n",
    "            df.loc[r, c] = min_p_value\n",
    "    df.columns = [var + \"_x\" for var in variables]\n",
    "    df.index = [var + \"_y\" for var in variables]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "better-waterproof",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### 4) Cointegration test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specialized-louisiana",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cointegration_test(df, alpha=0.05):\n",
    "    out = coint_johansen(df, -1, 5)\n",
    "    d = {\"0.90\": 0, \"0.95\": 1, \"0.99\": 2}\n",
    "    traces = out.lr1\n",
    "    cvts = out.cvt[:, d[str(1 - alpha)]]\n",
    "\n",
    "    def adjust(val, length=6):\n",
    "        return str(val).ljust(length)\n",
    "\n",
    "    print(\"Name   ::  Test Stat > C(95%)    =>   Signif  \\n\", \"--\" * 20)\n",
    "    for col, trace, cvt in zip(df.columns, traces, cvts):\n",
    "        print(adjust(col), \":: \", adjust(round(trace, 2), 9), \">\", adjust(cvt, 8), \" =>  \", trace > cvt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bulgarian-frederick",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### 5) Determine which highly correlated independent variables have a stronger correlation with our target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wanted-convergence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(df, threshold, target_variable):\n",
    "    col_corr = set()\n",
    "    corr_matrix = df.corr()\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold:\n",
    "                colname = corr_matrix.columns[i]\n",
    "                rowname = corr_matrix.index[j]\n",
    "                cor1 = abs(df[colname].corr(target_variable))\n",
    "                cor2 = abs(df[rowname].corr(target_variable))\n",
    "                if  cor1 > cor2:\n",
    "                    col_corr.add(corr_matrix.index[j])\n",
    "                else:\n",
    "                    col_corr.add(corr_matrix.columns[i])\n",
    "    return col_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "related-queensland",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### 6) Reshape correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metric-dominant",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_corr(df):\n",
    "    df_corr = df.corr().stack().reset_index()\n",
    "    df_corr.columns = [\"Feature 1\", \"Feature 2\", \"Correlation\"]\n",
    "    mask_dups = (df_corr[[\"Feature 1\", \"Feature 2\"]].apply(frozenset, axis=1).duplicated()) | (df_corr[\"Feature 1\"] == df_corr[\"Feature 2\"])\n",
    "    df_corr = df_corr[~mask_dups]\n",
    "\n",
    "    return df_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infrared-selection",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## Testing grounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becoming-diversity",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### 1) Using Bayesian Optimization and Ordinary Least Squares for feature selection/pruning. (Doesn't really work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polish-calcium",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "from itertools import combinations\n",
    "from statsmodels.formula.api import ols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worthy-fourth",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = (df_Merged.loc[:, df_Merged.columns != \"Aggregate\"], df_Merged[\"Aggregate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuffed-pension",
   "metadata": {},
   "outputs": [],
   "source": [
    "b2 = []\n",
    "for i in range(1, len(X.columns) - 15):\n",
    "    b2.append(list(combinations(X.columns, i)))\n",
    "b2 = [item for sublist in b2 for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "useful-things",
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_function(i):\n",
    "    max_r2 = 0\n",
    "    max_list = None\n",
    "    item = format(\"+\".join(b2[int(i)]))\n",
    "    model = ols(f\"Aggregate ~ {item}\", data=df_Merged).fit()\n",
    "    r2 = model.rsquared_adj**.5\n",
    "    if r2 > max_r2:\n",
    "        max_r2 = r2\n",
    "        max_list = b2[int(i)]\n",
    "    return max_r2\n",
    "\n",
    "def function_to_be_optimized(i):\n",
    "    d = int(i)\n",
    "    return example_function(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immune-provider",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbounds = {'i': (1, len(b2) - 1)}\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    pbounds=pbounds,\n",
    "    f=function_to_be_optimized,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "optimizer.maximize(\n",
    "    init_points=2,\n",
    "    n_iter=200,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naughty-handle",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### 2) Dimensionality reduction and HDBSCAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "material-packet",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaptive-pharmaceutical",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_REFIT_resampled_c = df_REFIT_resampled.copy()\n",
    "cols_REFIT_c = cols_REFIT.copy()\n",
    "cols_REFIT_c.remove(\"Aggregate\")\n",
    "df_REFIT_resampled_c.drop(cols_REFIT_c, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identified-configuration",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_REFIT_resampled_c.index = pd.MultiIndex.from_arrays([df_REFIT_resampled_c.index.date, df_REFIT_resampled_c.index.time], names=['Date','Time'])\n",
    "df_REFIT_resampled_c = df_REFIT_resampled_c.unstack()\n",
    "df_REFIT_resampled_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heated-audience",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = TSNE(n_components=2).fit_transform(df_REFIT_resampled_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greek-effort",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nervous-abraham",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA().fit(df_REFIT_resampled_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random-reflection",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Cumulative explained variance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaningful-wholesale",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X = pca.fit_transform(df_REFIT_resampled_c)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mounted-durham",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=10, min_samples=4)\n",
    "clusterer = clusterer.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satellite-costa",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = clusterer.labels_\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "print(n_clusters_, n_noise_, len(df_REFIT_resampled_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "headed-oriental",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_REFIT_resampled_c['Labels'] = labels\n",
    "for i in range(0, n_clusters_):\n",
    "    globals()['df_c' + str(i)] = (df_REFIT_resampled_c.loc[df_REFIT_resampled_c['Labels'] == i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identified-mechanics",
   "metadata": {},
   "outputs": [],
   "source": [
    "for q in range(len(df_c3)):\n",
    "    row = df_c3.iloc[q,0:48].plot(alpha=0.7)\n",
    "    row.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loving-giving",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0],X[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bronze-international",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0],X[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capital-scottish",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "X = LocallyLinearEmbedding(n_components=2).fit_transform(df_REFIT_resampled_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anonymous-woman",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0],X[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tracked-spanish",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import FactorAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saved-colleague",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = FactorAnalysis(n_components=2, random_state=0).fit_transform(df_REFIT_resampled_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competent-remainder",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0],X[:, 1])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
