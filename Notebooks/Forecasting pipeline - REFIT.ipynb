{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "enormous-wayne",
   "metadata": {},
   "source": [
    "The following notebook will be split into 4 parts:\n",
    " - [Part 1](#Part1_REFIT): Setup\n",
    " - [Part 2](#Part2_REFIT): Exploratory Data Analysis\n",
    " - [Part 3](#Part3_REFIT): Clustering\n",
    " - [Part 4](#Part4_REFIT): Forecasting\n",
    " \n",
    "**Part 1:** Initial setup (importing relevant packages, setting up global hyperparameters, importing/cleaning our data set).\n",
    "\n",
    "**Part 2:** Exploratory data analysis that serves to act as the feature selection step by determining which features are ir/relevant.\n",
    "\n",
    "**Part 3:** Clustering days based on a similarity metric.\n",
    "\n",
    "**Part 4:** Forecasting on a per-cluster basis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ancient-powell",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "# Part 1: Setup <a id=\"Part1_REFIT\"> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finite-solution",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## 1.1: Initial setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bigger-narrative",
   "metadata": {},
   "source": [
    "**Step 1:** Import the relevant packages and set Seaborn/Matplotlib hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecological-checklist",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "import holidays\n",
    "import os\n",
    "\n",
    "import matplotlib.dates as md\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from statsmodels.tsa.stattools import adfuller, grangercausalitytests\n",
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "from tqdm import notebook\n",
    "\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "sns.set(style=\"whitegrid\", palette=\"muted\")\n",
    "\n",
    "plt.rcParams[\"axes.labelsize\"] = 26\n",
    "plt.rcParams[\"axes.titlesize\"] = 26\n",
    "plt.rcParams[\"figure.figsize\"] = 16, 10\n",
    "plt.rcParams[\"figure.dpi\"] = 300\n",
    "plt.rcParams[\"xtick.labelsize\"] = 22\n",
    "plt.rcParams[\"ytick.labelsize\"] = 22\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respiratory-walnut",
   "metadata": {},
   "source": [
    "**Step 2:** Define the location of our data as well as the relevant columns that we would like to include."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electoral-advantage",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "house_number = 12\n",
    "\n",
    "cols_REFIT = [\n",
    "    \"Time\",\n",
    "    \"Aggregate\",\n",
    "    \"Appliance1\",\n",
    "    \"Appliance2\",\n",
    "    \"Appliance3\",\n",
    "    \"Appliance4\",\n",
    "    \"Appliance5\",\n",
    "    \"Appliance6\",\n",
    "    \"Appliance7\",\n",
    "    \"Appliance8\",\n",
    "    \"Appliance9\",\n",
    "]\n",
    "\n",
    "data_directory_REFIT = os.path.join(\"Data\", \"REFIT\")\n",
    "data_directory_Solcast = os.path.join(\"Data\", \"Solcast_REFIT\")\n",
    "\n",
    "house = f\"CLEAN_House{house_number}.csv\"\n",
    "solcast_15 = \"Solcast_REFIT_15.csv\"\n",
    "\n",
    "file_destination_REFIT = os.path.join(data_directory_REFIT, house)\n",
    "file_destination_Solcast = os.path.join(data_directory_Solcast, solcast_15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abandoned-attribute",
   "metadata": {},
   "source": [
    "**Step 3:** Read in the data and save it to a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neural-catch",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_REFIT = pd.read_csv(file_destination_REFIT, index_col=0, parse_dates=True, usecols=cols_REFIT)\n",
    "df_Solcast = pd.read_csv(file_destination_Solcast, index_col=0, parse_dates=True)\n",
    "\n",
    "df_Solcast.index = df_Solcast.index.rename(\"Time\")\n",
    "df_Solcast.index = pd.to_datetime(df_Solcast.index).tz_localize(None)\n",
    "\n",
    "cols_REFIT.remove(\"Time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlikely-google",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## 1.2: Scale the data in the dataframe(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lyric-athens",
   "metadata": {},
   "source": [
    "**Step 1.1:** Scale the data in a range between 0 and 1 (optional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supreme-machinery",
   "metadata": {},
   "outputs": [],
   "source": [
    "# minmax_REFIT = MinMaxScaler()\n",
    "# minmax_Solcast = MinMaxScaler()\n",
    "\n",
    "# df_REFIT[cols_REFIT] = minmax_REFIT.fit_transform(df_REFIT[cols_REFIT])\n",
    "# df_Solcast[cols_Solcast] = minmax_Solcast.fit_transform(df_Solcast[cols_Solcast])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rolled-pastor",
   "metadata": {},
   "source": [
    "**Step 1.2:** Standardize the data by removing the mean and scaling to unit variance (optional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consistent-hearts",
   "metadata": {},
   "outputs": [],
   "source": [
    "standardscale_REFIT = StandardScaler()\n",
    "# standardscale_Solcast = StandardScaler()\n",
    "\n",
    "df_REFIT[cols_REFIT] = standardscale_REFIT.fit_transform(df_REFIT[cols_REFIT])\n",
    "# df_Solcast[cols_Solcast] = standardscale_Solcast.fit_transform(df_Solcast[cols_Solcast])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "julian-intensity",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## 1.3: Merge the dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certified-wedding",
   "metadata": {},
   "source": [
    "**Step 1:** Create a copy of our REFIT dataframe that is resampled into a resolution of 15 minutes and drop any days that contain an incomplete number of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spoken-combat",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_REFIT_resampled = df_REFIT.resample(\"15min\").mean()\n",
    "df_REFIT_resampled = df_REFIT_resampled.dropna()\n",
    "\n",
    "mask = df_REFIT_resampled.groupby(df_REFIT_resampled.index.date).size()\n",
    "mask = mask[mask < 96].index.to_list()\n",
    "\n",
    "df_REFIT_resampled = df_REFIT_resampled[~df_REFIT_resampled.index.floor(\"D\").isin(mask)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spare-thriller",
   "metadata": {},
   "source": [
    "**Step 2:** Create a third dataframe that is the result of merging the Solcast dataframe with the REFIT dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "united-medicare",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Merged = pd.merge(left=df_Solcast, left_on=df_Solcast.index, right=df_REFIT_resampled, right_on=df_REFIT_resampled.index)\n",
    "\n",
    "cols_Merged = [\n",
    "    \"PeriodStart\",\n",
    "    \"Period\",\n",
    "    \"Appliance1\",\n",
    "    \"Appliance2\",\n",
    "    \"Appliance3\",\n",
    "    \"Appliance4\",\n",
    "    \"Appliance5\",\n",
    "    \"Appliance6\",\n",
    "    \"Appliance7\",\n",
    "    \"Appliance8\",\n",
    "    \"Appliance9\",\n",
    "]\n",
    "\n",
    "df_Merged.drop(cols_Merged, axis=1, inplace=True)\n",
    "df_Merged.rename(columns={\"key_0\": \"Time\"}, inplace=True)\n",
    "df_Merged = df_Merged.set_index(\"Time\")\n",
    "df_Merged.index = pd.to_datetime(df_Merged.index)\n",
    "df_Merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-great",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## 1.4: Append temporal features to our merged dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "progressive-weekend",
   "metadata": {},
   "source": [
    "**Step 1:** Append public holidays to our merged dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compliant-royal",
   "metadata": {},
   "outputs": [],
   "source": [
    "UK_holidays = holidays.UnitedKingdom()\n",
    "df_Merged.insert(0, \"Holiday\", [1 if str(val).split()[0] in UK_holidays else 0 for val in df_Merged.index.date])\n",
    "df_Merged[\"Holiday\"] = df_Merged[\"Holiday\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infrared-efficiency",
   "metadata": {},
   "source": [
    "**Step 2:** Define day of the year ranges for each of the seasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twenty-crack",
   "metadata": {},
   "outputs": [],
   "source": [
    "spring = range(79, 172)\n",
    "summer = range(172, 266)\n",
    "fall = range(266, 355)\n",
    "\n",
    "def season(doy):\n",
    "    if doy in spring:\n",
    "        return \"0\"\n",
    "    if doy in summer:\n",
    "        return \"1\"\n",
    "    if doy in fall:\n",
    "        return \"2\"\n",
    "    else:\n",
    "        return \"3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sweet-situation",
   "metadata": {},
   "source": [
    "**Step 3:** Append temporal data to our merged dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "encouraging-thread",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Merged.insert(0, \"Year\", df_Merged.index.year)\n",
    "df_Merged.insert(1, \"Month\", df_Merged.index.month)\n",
    "df_Merged.insert(3, \"Day\", df_Merged.index.day)\n",
    "df_Merged.insert(4, \"Hour\", df_Merged.index.hour)\n",
    "df_Merged.insert(5, \"Minute\", df_Merged.index.minute)\n",
    "df_Merged.insert(6, \"Weekday\", df_Merged.index.weekday)\n",
    "df_Merged.insert(7, \"Season\", df_Merged.index.dayofyear.map(season))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heavy-pattern",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "# Part 2: Exploratory Data Analysis <a id=\"Part2_REFIT\"> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moved-montgomery",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## 2.1: Enumerate values of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secondary-medline",
   "metadata": {},
   "source": [
    "**Step 1:** We iterate over each of the houses available in our data set and calculate the total number of missing days, total number of days that contain *incomplete* data, total number of values recorded as well as the number of values that contain *issues* and save the results to a `.csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geographic-effectiveness",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output = {\n",
    "    \"House number\": [],\n",
    "    \"Date start\": [],\n",
    "    \"Date end\": [],\n",
    "    \"Total no. of days\": [],\n",
    "    \"No. of days missing\": [],\n",
    "    \"% of days missing\": [],\n",
    "    \"No. of days incomplete\": [],\n",
    "    \"% of days incomplete\": [],\n",
    "    \"Total no. of values recorded\": [],\n",
    "    \"No. of values missing data\": [],\n",
    "    \"% of values missing data\": [],\n",
    "    \"Number of values with issues\": [],\n",
    "    \"% of values with issues\": [],\n",
    "    \"Longest period of missing days\": [],\n",
    "}\n",
    "\n",
    "for i in notebook.trange(1, 22):\n",
    "    if i == 14:\n",
    "        continue\n",
    "\n",
    "    house_number = i\n",
    "    house = f\"CLEAN_House{house_number}.csv\"\n",
    "    file_destination = os.path.join(data_directory_REFIT, house)\n",
    "    df = pd.read_csv(file_destination, index_col=0, parse_dates=True)\n",
    "\n",
    "    # Define the starting date and ending date of our dataframe.\n",
    "    date_start = df.index.min()\n",
    "    date_end = df.index.max()\n",
    "\n",
    "    df_grouped = df.groupby(df.index.date).mean()\n",
    "    total_values = len(df)\n",
    "    total_days = len(pd.date_range(df_grouped.index.min(), df_grouped.index.max()))\n",
    "    \n",
    "    # Set up a dataframe with which to calculate incomplete days.\n",
    "    df_r = df.resample(\"15min\").mean()\n",
    "    df_r = df_r.dropna()\n",
    "\n",
    "    # Count the number of missing days.\n",
    "    days_missing = len(pd.date_range(df_grouped.index.min(), df_grouped.index.max()).difference(df_grouped.index))\n",
    "    percentage_days_missing = np.round((days_missing / total_days) * 100, 2)\n",
    "\n",
    "    # Count the number of days with incomplete data.\n",
    "    mask = df_r.groupby(df_r.index.date).size()\n",
    "    mask = mask[mask < 96].index.to_list()\n",
    "    days_incomplete = len(mask)\n",
    "    percentage_days_incomplete = np.round((days_incomplete / total_days) * 100, 2)\n",
    "\n",
    "    # Count the number of missing values.\n",
    "    values_missing = (df[\"Aggregate\"] == 0).astype(int).sum()\n",
    "    percentage_values_missing = np.round((values_missing / total_values) * 100, 2)\n",
    "\n",
    "    # Count the number of values with 'issues'.\n",
    "    values_issues = (df[\"Issues\"] == 1).astype(int).sum()\n",
    "    percentage_values_issues = np.round((values_issues / total_values) * 100, 2)\n",
    "\n",
    "    # Calculate the longest period of missing days.\n",
    "    df_grouped[\"Diff_Date\"] = df_grouped.index.to_series().diff()\n",
    "    df_grouped[\"Diff_Date\"] = df_grouped[\"Diff_Date\"] / np.timedelta64(1, \"D\")\n",
    "    stretch = df_grouped[\"Diff_Date\"].max().astype(int) - 1\n",
    "\n",
    "    output[\"House number\"].append(house_number)\n",
    "    output[\"Date start\"].append(date_start)\n",
    "    output[\"Date end\"].append(date_end)\n",
    "    output[\"Total no. of days\"].append(total_days)\n",
    "    output[\"No. of days missing\"].append(days_missing)\n",
    "    output[\"% of days missing\"].append(percentage_days_missing)\n",
    "    output[\"Total no. of values recorded\"].append(total_values)\n",
    "    output[\"No. of days incomplete\"].append(days_incomplete)\n",
    "    output[\"% of days incomplete\"].append(percentage_days_incomplete)\n",
    "    output[\"No. of values missing data\"].append(values_missing)\n",
    "    output[\"% of values missing data\"].append(percentage_values_missing)\n",
    "    output[\"Number of values with issues\"].append(values_issues)\n",
    "    output[\"% of values with issues\"].append(percentage_values_issues)\n",
    "    output[\"Longest period of missing days\"].append(stretch)\n",
    "    \n",
    "output = pd.DataFrame.from_dict(output).set_index(\"House number\")\n",
    "output.to_csv(\"Data Analysis - REFIT.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brave-biography",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.2: Visualize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "headed-writer",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### 2.2.1: Line plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reduced-tackle",
   "metadata": {},
   "source": [
    "**Step 1:** Create a copy of our REFIT dataframe and reformat the index into a string so as to make sure that Matplotlib does not automatically interpolate missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternative-status",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_REFIT_resampled_c = df_REFIT_resampled.copy()\n",
    "df_REFIT_resampled_c.index = df_REFIT_resampled_c.index.strftime(\"%d-%m-%y %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "referenced-channels",
   "metadata": {},
   "source": [
    "**Step 2:** Drop all columns barre the `Aggregate` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tropical-graph",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_REFIT_c = cols_REFIT.copy()\n",
    "cols_REFIT_c.remove(\"Aggregate\")\n",
    "df_REFIT_resampled_c.drop(cols_REFIT_c, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imposed-consultancy",
   "metadata": {},
   "source": [
    "**Step 3:** Plot the aggregate power consumption over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reported-edgar",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "df_REFIT_resampled_c[\"Aggregate\"].plot(ax=ax)\n",
    "\n",
    "ax.set_xlabel(\"\")\n",
    "ax.set_ylabel(\"Aggregate Power Consumption (Watts)\")\n",
    "ax.set_xlim(left=-5, right=len(df_REFIT_resampled_c) + 5)\n",
    "ax.set_ylim(bottom=df_REFIT_resampled_c[\"Aggregate\"].min() - 100)\n",
    "\n",
    "plt.setp(ax.get_xticklabels(), ha=\"right\", rotation=60)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulation-picking",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### 2.2.2: Stack plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preliminary-receipt",
   "metadata": {},
   "source": [
    "**Step 1:** Create a copy of our REFIT dataframe that contains only the IAM readings (with their labels) and group by the averaged reading per hour of the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepted-kentucky",
   "metadata": {},
   "outputs": [],
   "source": [
    "hours = []\n",
    "for i in range(0, 24):\n",
    "    if i < 10:\n",
    "        hours.append(f\"0{i}:00:00\")\n",
    "    if i >= 10:\n",
    "        hours.append(f\"{i}:00:00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phantom-restaurant",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Stacked = df_REFIT.iloc[:, 1:10].groupby(df_REFIT.index.hour).mean()\n",
    "\n",
    "df_Stacked = df_Stacked.rename(\n",
    "    index=str,\n",
    "    columns={\n",
    "        \"Appliance1\": \"Fridge-Freezer\",\n",
    "        \"Appliance2\": \"Unknown\",\n",
    "        \"Appliance3\": \"Unknown\",\n",
    "        \"Appliance4\": \"Computer Site\",\n",
    "        \"Appliance5\": \"Microwave\",\n",
    "        \"Appliance6\": \"Kettle\",\n",
    "        \"Appliance7\": \"Toaster\",\n",
    "        \"Appliance8\": \"Television\",\n",
    "        \"Appliance9\": \"Unknown\",\n",
    "    },\n",
    ")\n",
    "df_Stacked.index = hours\n",
    "df_Stacked.index = pd.to_datetime(df_Stacked.index, format=\"%H:%M:%S\")\n",
    "df_Stacked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elect-print",
   "metadata": {},
   "source": [
    "**Step 2:** Create the stack plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baking-electron",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "colors = [\"b\", \"orange\", \"g\", \"r\", \"purple\", \"brown\", \"pink\", \"gray\", \"yellow\"]\n",
    "\n",
    "ax.stackplot(df_Stacked.index.values, np.transpose(df_Stacked.values), labels=df_Stacked.columns.values, colors=colors)\n",
    "ax.legend(loc=\"upper left\", fontsize=18)\n",
    "ax.set_xlim(df_Stacked.index.min(), df_Stacked.index.max())\n",
    "ax.xaxis.set_major_locator(md.HourLocator())\n",
    "ax.xaxis.set_major_formatter(md.DateFormatter(\"%H:%M\"))\n",
    "\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Average Power Consumption (Watts)\")\n",
    "plt.xticks(df_Stacked.index.values)\n",
    "\n",
    "fig.autofmt_xdate()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "danish-identifier",
   "metadata": {},
   "source": [
    "**Step 3:** Create individual plots for each of the IAMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bearing-providence",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 3)\n",
    "i = 0\n",
    "colors = [\"b\", \"orange\", \"g\", \"r\", \"purple\", \"brown\", \"pink\", \"gray\", \"yellow\"]\n",
    "\n",
    "list = []\n",
    "for x in range(0, 24, 6):\n",
    "    list.append(df_Stacked.index[x])\n",
    "list.append(df_Stacked.index[-1])\n",
    "\n",
    "for nrow in range(0, 3):\n",
    "    for ncol in range(0, 3):\n",
    "        axs[nrow, ncol].plot(df_Stacked.index, df_Stacked.iloc[:, i], color=colors[i])\n",
    "        axs[nrow, ncol].set_xlim(df_Stacked.index.min(), df_Stacked.index.max())\n",
    "        axs[nrow, ncol].set_title(df_Stacked.columns[i], fontsize=18)\n",
    "        axs[nrow, ncol].xaxis.set_major_locator(md.HourLocator())\n",
    "        axs[nrow, ncol].xaxis.set_major_formatter(md.DateFormatter(\"%H:%M\"))\n",
    "        axs[nrow, ncol].set_xticks(list)\n",
    "        i = i + 1\n",
    "\n",
    "fig.autofmt_xdate()\n",
    "fig.add_subplot(111, frameon=False)\n",
    "\n",
    "plt.grid(False)\n",
    "plt.tick_params(labelcolor=\"none\", top=False, bottom=False, left=False, right=False)\n",
    "plt.xlabel(\"Time\", labelpad=25)\n",
    "plt.ylabel(\"Average Power Consumption (Watts)\", labelpad=15)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pacific-russell",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### 2.2.3: Count plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knowing-punch",
   "metadata": {},
   "source": [
    "**Step 1:** Visualize the number of samples per day of the week over the entire data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conscious-missouri",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Merged_c = df_Merged.copy()\n",
    "df_Merged_c.insert(0, \"Weekday_name\", df_Merged.index.day_name())\n",
    "\n",
    "ax = sns.countplot(\n",
    "    x=df_Merged_c[\"Weekday_name\"],\n",
    "    data=df_Merged_c,\n",
    "    palette=\"icefire\",\n",
    "    order=[\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"],\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Weekday\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spectacular-advertiser",
   "metadata": {},
   "source": [
    "**Step 2:** Visualize the number of samples per month over the entire data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indie-caribbean",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Merged_c = df_Merged.copy()\n",
    "df_Merged_c.insert(0, \"Month_name\", df_Merged.index.month_name())\n",
    "\n",
    "ax = sns.countplot(\n",
    "    x=df_Merged_c[\"Month_name\"],\n",
    "    data=df_Merged_c,\n",
    "    palette=\"icefire\",\n",
    "    order=[\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"],\n",
    ")\n",
    "\n",
    "plt.setp(ax.get_xticklabels(), ha=\"right\", rotation=60)\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becoming-obligation",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### 2.2.4: Stationarity plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-anger",
   "metadata": {},
   "source": [
    "**Step 1:** Test for stationarity and plot the result using the pre-defined `test_stationarity` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "native-tractor",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stationarity(df_REFIT_resampled_c, 0.05, \"Aggregate\", \"Aggregate Power Consumption (Watts)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adult-margin",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### 2.2.5: Box and whisker plots/outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hairy-actor",
   "metadata": {},
   "source": [
    "**Step 1:** Box and whiskers plot for the aggregate as well as each of the IAM readings grouped by the months of the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behind-heating",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(len(cols_REFIT), 1, figsize=(16, 10 * (len(cols_REFIT))), sharex=True)\n",
    "\n",
    "for col, ax in zip([*cols_REFIT], axs):\n",
    "    sns.boxplot(\n",
    "        data=df_REFIT,\n",
    "        x=df_REFIT.index.month_name(),\n",
    "        y=col,\n",
    "        ax=ax,\n",
    "        order=[\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"],\n",
    "    )\n",
    "    ax.set_title(col)\n",
    "    ax.xaxis.set_label_text(\"\")\n",
    "\n",
    "plt.setp(ax.get_xticklabels(), ha=\"right\", rotation=60)\n",
    "plt.xlabel(\"Month\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "material-statement",
   "metadata": {},
   "source": [
    "**Step 2:** We remove outliers that are 3 standard deviations away from the mean and repeat **step 1**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "northern-monster",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = df_REFIT[(np.abs(stats.zscore(df_REFIT[\"Aggregate\"])) < 3)]\n",
    "print(f\"Number of outliers: {(len(df_REFIT) - len(df_out))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verified-roller",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(len(cols_REFIT), 1, figsize=(16, 10 * (len(cols_REFIT))), sharex=True)\n",
    "\n",
    "for col, ax in zip([*cols_REFIT], axs):\n",
    "    sns.boxplot(\n",
    "        data=df_out,\n",
    "        x=df_out.index.month_name(),\n",
    "        y=col,\n",
    "        ax=ax,\n",
    "        order=[\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"],\n",
    "    )\n",
    "    ax.xaxis.set_label_text(\"\")\n",
    "\n",
    "plt.setp(ax.get_xticklabels(), ha=\"right\", rotation=60)\n",
    "plt.xlabel(\"Month\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "square-macedonia",
   "metadata": {},
   "source": [
    "**Step 2.1:** Purely for the aggregate power consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitted-reset",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(\n",
    "    data=df_out,\n",
    "    x=df_out.index.month_name(),\n",
    "    y=df_out[\"Aggregate\"],\n",
    "    order=[\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"],\n",
    ")\n",
    "\n",
    "plt.setp(ax.get_xticklabels(), ha=\"right\", rotation=60)\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Aggregate Power Consumption (Watts)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monetary-sunday",
   "metadata": {},
   "source": [
    "**Step 3:** Box and whiskers plot for the aggregate as well as each of the IAM readings over the entirety of the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exotic-granny",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.boxplot(data=df_out, ax=ax)\n",
    "\n",
    "plt.setp(ax.get_xticklabels(), ha=\"right\", rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "necessary-commander",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## 2.3: Time series decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composed-closure",
   "metadata": {},
   "source": [
    "**Step 1:** Create a copy of our REFIT dataframe and reformat the index into a string so as to make sure that Matplotlib does not automatically interpolate missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "environmental-niagara",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_REFIT_resampled_c = df_REFIT_resampled.copy()\n",
    "df_REFIT_resampled_c.index = df_REFIT_resampled_c.index.strftime(\"%d-%m-%y %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adopted-annex",
   "metadata": {},
   "source": [
    "**Step 2:** Drop all columns barre the `Aggregate` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dress-spencer",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_REFIT_c = cols_REFIT.copy()\n",
    "cols_REFIT_c.remove(\"Aggregate\")\n",
    "df_REFIT_resampled_c.drop(cols_REFIT_c, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematical-stylus",
   "metadata": {},
   "source": [
    "**Step 3:** Define a period of 2-3 months (as an example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hairy-fossil",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_REFIT_resampled_c = df_REFIT_resampled_c.loc[\"2015-01-01\":\"2015-04-01\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mental-heart",
   "metadata": {},
   "source": [
    "**Step 4:** Perform time series decomposition using LOESS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "municipal-extent",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = (24 * 60) // 15\n",
    "stl_decompose_result = STL(df_REFIT_resampled_c, period=freq).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accurate-zambia",
   "metadata": {},
   "source": [
    "**Step 5:** Separate/plot the obtained results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "running-orbit",
   "metadata": {},
   "outputs": [],
   "source": [
    "observed = stl_decompose_result.observed\n",
    "trend = stl_decompose_result.trend.to_frame()\n",
    "seasonal = stl_decompose_result.seasonal.to_frame()\n",
    "noise = stl_decompose_result.resid.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filled-philip",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4, sharex=True)\n",
    "\n",
    "observed.plot(ax=axs[0], title=\"Observed\")\n",
    "trend.plot(ax=axs[1], title=\"Trend\")\n",
    "seasonal.plot(ax=axs[2], title=\"Seasonal\")\n",
    "noise.plot(ax=axs[3], title=\"Noise\")\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_xlim(0, right=len(observed))\n",
    "    ax.get_legend().remove()\n",
    "    ax.title.set_size(22)\n",
    "\n",
    "fig.text(0.03, 0.6, \"Aggregate Power Consumption (Watts)\", fontsize=\"22\", va=\"center\", rotation=\"vertical\")\n",
    "plt.setp(ax.get_xticklabels(), ha=\"right\", rotation=60)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fantastic-antarctica",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## 2.4: Causality and correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statewide-sheriff",
   "metadata": {},
   "source": [
    "**Step 1:** Perform the Augmented Dicky-Fuller test to determine whether our time series is stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threaded-standing",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, column in df_Merged.iteritems():\n",
    "    adfuller_test(column, name=column.name)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspected-holocaust",
   "metadata": {},
   "source": [
    "**Step 2:** Check for autocorrelation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suitable-better",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acf(df_REFIT_resampled[\"Aggregate\"], lags=50, zero=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleasant-shirt",
   "metadata": {},
   "source": [
    "**Step 3:** Check for partial autocorrelation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detected-morris",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pacf(df_REFIT_resampled[\"Aggregate\"], lags=50, zero=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "above-designation",
   "metadata": {},
   "source": [
    "**Step 4:** Check for highly correlated features in the Solcast dataframe and drop them from our merged dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generous-judge",
   "metadata": {},
   "outputs": [],
   "source": [
    "highly_correlated = correlation(df_Solcast, 0.7, df_Merged.Aggregate)\n",
    "A = len(df_Merged.columns)\n",
    "print(highly_correlated)\n",
    "#df_Merged.drop(highly_correlated, axis=1, inplace=True)\n",
    "B = len(df_Merged.columns) - len(highly_correlated)\n",
    "print(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dental-douglas",
   "metadata": {},
   "source": [
    "**Step 5:** Estimate mutual information of our independent variables on our target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authorized-tribute",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = (df_Merged.loc[:, df_Merged.columns != \"Aggregate\"], df_Merged[\"Aggregate\"])\n",
    "mutual_info = mutual_info_regression(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foster-storage",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutual_info = pd.Series(mutual_info)\n",
    "mutual_info.index = X.columns\n",
    "mutual_info = mutual_info.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wireless-southeast",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.barplot(x=mutual_info.values, y=mutual_info.index, palette=\"icefire\")\n",
    "ax.set_xlabel(\"Mutual information\")\n",
    "ax.set_ylabel(\"Independent variable\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chicken-sheffield",
   "metadata": {},
   "source": [
    "**Step 6:** Granger causality test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-competition",
   "metadata": {},
   "outputs": [],
   "source": [
    "gm = grangers_causation_matrix(df_Merged, variables = df_Merged.columns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "given-child",
   "metadata": {},
   "outputs": [],
   "source": [
    "gm2 = gm.transpose()[\"Aggregate_y\"].iloc[0:18].to_frame()\n",
    "ax = sns.heatmap(gm2, mask=gm2 < 0.05, annot=True, fmt=\".2f\", square=True, cmap=\"rocket_r\", vmin=0.0, vmax=1.0)\n",
    "ax2 = sns.heatmap(gm2, mask=gm2 >= 0.05, annot=True, fmt=\".2f\", square=True, cmap=\"rocket_r\", vmin=0.0, vmax=1.0, annot_kws={\"weight\": \"bold\"}, cbar=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesser-brother",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.heatmap(gm.transpose(), annot=True, fmt=\".2f\", square=True, cmap=\"rocket_r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reserved-walnut",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gm2 = gm.where(np.tril(np.ones(gm.shape)).astype(bool))\n",
    "# cmap = sns.diverging_palette(250, 15, s=75, l=40, n=9, center=\"light\", as_cmap=True)\n",
    "\n",
    "# sns.heatmap(gm2, center=0, annot=True, fmt=\".2f\", square=True, cmap=cmap)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personalized-amateur",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "# Part 3: Clustering <a id=\"Part3_REFIT\"> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "similar-teach",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## 3.1: Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "possible-cambodia",
   "metadata": {},
   "source": [
    "**Step 1:** Create a copy of our REFIT dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-chester",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_REFIT_resampled_c = df_REFIT_resampled.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "athletic-valuation",
   "metadata": {},
   "source": [
    "**Step 2:** Drop all columns barre the `Aggregate` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "junior-growing",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_REFIT_c = cols_REFIT.copy()\n",
    "cols_REFIT_c.remove(\"Aggregate\")\n",
    "df_REFIT_resampled_c.drop(cols_REFIT_c, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outside-pointer",
   "metadata": {},
   "source": [
    "**Step 3:** Reshape our dataframe as 96 columns that represent the 96 15-minute chunks of each day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expanded-seeker",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_REFIT_resampled_c.index = pd.MultiIndex.from_arrays([df_REFIT_resampled_c.index.date, df_REFIT_resampled_c.index.time], names=['Date','Time'])\n",
    "df_REFIT_resampled_c = df_REFIT_resampled_c.unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wired-retention",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### 3.1.1: Statistical parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifty-oklahoma",
   "metadata": {},
   "source": [
    "**Step 1:** Split our day into 5 periods:\n",
    " - `LEEM`: Late evening/early morning (23:30-06:00)\n",
    " - `MR`: Morning (06:00-11:00)\n",
    " - `LMAF`: Late morning/afternoon (11:00-15:00)\n",
    " - `LAEE`: Late afternoon/early evening (15:00-20:30)\n",
    " - `EV`: Evening (20:30-23:30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "available-saudi",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEEM = df_REFIT_resampled_c.iloc[:, np.r_[0:24, 92:96]]\n",
    "MR = df_REFIT_resampled_c.iloc[:, 24:45]\n",
    "LMAF = df_REFIT_resampled_c.iloc[:, 44:61]\n",
    "LAEE = df_REFIT_resampled_c.iloc[:, 60:83]\n",
    "EV = df_REFIT_resampled_c.iloc[:, 82:95]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accredited-failing",
   "metadata": {},
   "source": [
    "**Step 2:** Create a new dataframe that consists of the mean, min, max and standard deviation of each of our 5 periods per day. We now represent each day with 20 variables rather than 96."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiovascular-century",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SP = LEEM.mean(axis=1).to_frame(name=\"LEEM_Mean\")\n",
    "df_SP.insert(len(df_SP.columns), \"LEEM_Min\", LEEM.min(axis=1))\n",
    "df_SP.insert(len(df_SP.columns), \"LEEM_Max\", LEEM.max(axis=1))\n",
    "df_SP.insert(len(df_SP.columns), \"LEEM_STD\", LEEM.std(axis=1))\n",
    "df_SP.insert(len(df_SP.columns), \"MR_Mean\", MR.mean(axis=1))\n",
    "df_SP.insert(len(df_SP.columns), \"MR_Min\", MR.min(axis=1))\n",
    "df_SP.insert(len(df_SP.columns), \"MR_Max\", MR.max(axis=1))\n",
    "df_SP.insert(len(df_SP.columns), \"MR_STD\", MR.std(axis=1))\n",
    "df_SP.insert(len(df_SP.columns), \"LMAF_Mean\", LMAF.mean(axis=1))\n",
    "df_SP.insert(len(df_SP.columns), \"LMAF_Min\", LMAF.min(axis=1))\n",
    "df_SP.insert(len(df_SP.columns), \"LMAF_Max\", LMAF.max(axis=1))\n",
    "df_SP.insert(len(df_SP.columns), \"LMAF_STD\", LMAF.std(axis=1))\n",
    "df_SP.insert(len(df_SP.columns), \"LAEE_Mean\", LAEE.mean(axis=1))\n",
    "df_SP.insert(len(df_SP.columns), \"LAEE_Min\", LAEE.min(axis=1))\n",
    "df_SP.insert(len(df_SP.columns), \"LAEE_Max\", LAEE.max(axis=1))\n",
    "df_SP.insert(len(df_SP.columns), \"LAEE_STD\", LAEE.std(axis=1))\n",
    "df_SP.insert(len(df_SP.columns), \"EV_Mean\", EV.mean(axis=1))\n",
    "df_SP.insert(len(df_SP.columns), \"EV_Min\", EV.min(axis=1))\n",
    "df_SP.insert(len(df_SP.columns), \"EV_Max\", EV.max(axis=1))\n",
    "df_SP.insert(len(df_SP.columns), \"EV_STD\", EV.std(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simple-minneapolis",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### 3.1.2: PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statistical-capture",
   "metadata": {},
   "source": [
    "**Step 1:** Determine the cumulative explained variance ratio as a function of the number of variabbles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thermal-reach",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA().fit(df_REFIT_resampled_c)\n",
    "\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel(\"Number of variables\")\n",
    "plt.ylabel(\"Cumulative explained variance\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "known-desert",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## 3.2: HDBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "widespread-species",
   "metadata": {},
   "source": [
    "**Step 1:** Further reduce the dimensionality of our data by using t-distributed Stochastic Neighbor Embedding (t-SNE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subtle-breeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "projection = TSNE().fit_transform(df_SP)\n",
    "plt.scatter(*projection.T)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocational-request",
   "metadata": {},
   "source": [
    "**Step 2:** Define our HDBSCAN clusterer with the appropriate hyperparameters and fit it to our 2-dimensional projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "committed-albuquerque",
   "metadata": {},
   "outputs": [],
   "source": [
    "HDB = hdbscan.HDBSCAN(min_cluster_size=48, min_samples=4)\n",
    "HDB = HDB.fit(projection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "previous-bernard",
   "metadata": {},
   "source": [
    "**Step 3:** Plot/visualize our clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governmental-tyler",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = HDB.labels_\n",
    "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise = list(labels).count(-1)\n",
    "core_samples_mask = np.zeros_like(labels, dtype=bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "objective-scoop",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = set(labels)\n",
    "colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k == -1:\n",
    "        # Black used for noise.\n",
    "        col = [0, 0, 0, 1]\n",
    "    class_member_mask = labels == k\n",
    "\n",
    "    xy = projection[class_member_mask & core_samples_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], \"o\", markerfacecolor=tuple(col), markeredgecolor=\"k\", markersize=14)\n",
    "\n",
    "    xy = projection[class_member_mask & ~core_samples_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], \"o\", markerfacecolor=tuple(col), markeredgecolor=\"k\", markersize=6)\n",
    "plt.title(\"Estimated number of clusters: %d\" % n_clusters)\n",
    "plt.show()\n",
    "\n",
    "print(\"Estimated number of clusters: %d\" % n_clusters)\n",
    "print(\"Estimated number of noise points: %d\" % n_noise)\n",
    "print(\"Silhouette Coefficient: %0.3f\" % metrics.silhouette_score(projection, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deluxe-crawford",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "# Part 4: Forecasting <a id=\"Part4_REFIT\"> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biblical-second",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honest-quarterly",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## Miscellaneous functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painful-monster",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### 1) Augmented Dickey–Fuller test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tough-green",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adfuller_test(series, signif=0.05, name=\"\"):\n",
    "    r = adfuller(series, autolag=\"AIC\")\n",
    "    output = {\"test_statistic\": round(r[0], 4), \"pvalue\": round(r[1], 4), \"n_lags\": round(r[2], 4), \"n_obs\": r[3]}\n",
    "    p_value = output[\"pvalue\"]\n",
    "\n",
    "    def adjust(val, length=6):\n",
    "        return str(val).ljust(length)\n",
    "\n",
    "    print(f'      Augmented Dickey-Fuller Test on \"{name}\"', \"\\n   \", \"-\" * 47)\n",
    "    print(f\" Null Hypothesis: Data has unit root. Non-Stationary.\")\n",
    "    print(f\" Significance Level    = {signif}\")\n",
    "    print(f' Test Statistic        = {output[\"test_statistic\"]}')\n",
    "    print(f' No. Lags Chosen       = {output[\"n_lags\"]}')\n",
    "\n",
    "    for key, val in r[4].items():\n",
    "        print(f\" Critical value {adjust(key)} = {round(val, 3)}\")\n",
    "    if p_value <= signif:\n",
    "        print(f\" => P-Value = {p_value}. Rejecting Null Hypothesis.\")\n",
    "        print(f\" => Series is Stationary.\")\n",
    "    else:\n",
    "        print(f\" => P-Value = {p_value}. Weak evidence to reject the Null Hypothesis.\")\n",
    "        print(f\" => Series is Non-Stationary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grave-defendant",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### 2) Augmented Dickey–Fuller test w/plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manual-talent",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_stationarity(series, signif=0.05, name=\"\", ylabel=\"\"):\n",
    "    def adjust(val, length=6):\n",
    "        return str(val).ljust(length)\n",
    "\n",
    "    rolmean = series.rolling(12).mean()\n",
    "    rolstd = series.rolling(12).std()\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    series.plot(ax=ax, alpha=0.5)\n",
    "    rolmean.plot(ax=ax, alpha=0.7)\n",
    "    rolstd.plot(ax=ax, alpha=0.7)\n",
    "\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_xlim(left=0, right=len(series))\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.title(\"Rolling Mean & Standard Deviation\")\n",
    "    plt.setp(ax.get_xticklabels(), ha=\"right\", rotation=60)\n",
    "\n",
    "    leg = plt.legend()\n",
    "    leg.get_texts()[0].set_text(name)\n",
    "    leg.get_texts()[1].set_text(\"Rolling Mean\")\n",
    "    leg.get_texts()[2].set_text(\"Rolling STD\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show(block=False)\n",
    "\n",
    "    adfuller_test(series, 0.05, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dated-glory",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### 3) Granger Causality test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaged-argentina",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grangers_causation_matrix(data, variables, test=\"ssr_chi2test\", maxlag=12, verbose=False):\n",
    "    df = pd.DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)\n",
    "    for c in df.columns:\n",
    "        for r in df.index:\n",
    "            test_result = grangercausalitytests(data[[r, c]], maxlag=maxlag, verbose=False)\n",
    "            p_values = ([round(test_result[i + 1][0][test][1], 2) for i in range(maxlag)])\n",
    "            if verbose:\n",
    "                print(f\"Y = {r}, X = {c}, P Values = {p_values}\")\n",
    "            min_p_value = np.min(p_values)\n",
    "            df.loc[r, c] = min_p_value\n",
    "    df.columns = [var + \"_x\" for var in variables]\n",
    "    df.index = [var + \"_y\" for var in variables]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleasant-leadership",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### 4) Determine which highly correlated independent variables have a stronger correlation with our target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fantastic-raleigh",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(df, threshold, target_variable):\n",
    "    col_corr = set()\n",
    "    corr_matrix = df.corr()\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold:\n",
    "                colname = corr_matrix.columns[i]\n",
    "                rowname = corr_matrix.index[j]\n",
    "                cor1 = abs(df[colname].corr(target_variable))\n",
    "                cor2 = abs(df[rowname].corr(target_variable))\n",
    "                if  cor1 > cor2:\n",
    "                    col_corr.add(corr_matrix.index[j])\n",
    "                else:\n",
    "                    col_corr.add(corr_matrix.columns[i])\n",
    "    return col_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "declared-cricket",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### 5) Reshape correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "color-english",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_corr(df):\n",
    "    df_corr = df.corr().stack().reset_index()\n",
    "    df_corr.columns = [\"Feature 1\", \"Feature 2\", \"Correlation\"]\n",
    "    mask_dups = (df_corr[[\"Feature 1\", \"Feature 2\"]].apply(frozenset, axis=1).duplicated()) | (df_corr[\"Feature 1\"] == df_corr[\"Feature 2\"])\n",
    "    df_corr = df_corr[~mask_dups]\n",
    "\n",
    "    return df_corr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
