{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "instrumental-image",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "# Part 1: Setup <a id=\"Part1_UCID\"> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "domestic-raise",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## 1.1: Initial setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "union-boxing",
   "metadata": {},
   "source": [
    "**Step 1:** Import the relevant packages and set Seaborn/Matplotlib hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developing-conservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import graphviz\n",
    "import hdbscan\n",
    "import holidays\n",
    "import os\n",
    "import sklearn\n",
    "import tensorflow\n",
    "import umap\n",
    "\n",
    "import matplotlib.dates as md\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy import stats\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFECV, mutual_info_regression\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import MaxAbsScaler, MinMaxScaler, StandardScaler\n",
    "from sklearn import metrics, tree\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.seasonal import STL, seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller, grangercausalitytests\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import LSTM, Conv1D, ConvLSTM2D, Dense, Flatten, LeakyReLU, MaxPooling1D, MaxPooling2D, RepeatVector, TimeDistributed\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tqdm import notebook\n",
    "\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "sns.set(style=\"whitegrid\", palette=\"muted\")\n",
    "\n",
    "plt.rcParams[\"font.size\"] = 24\n",
    "plt.rcParams[\"axes.labelsize\"] = 26\n",
    "plt.rcParams[\"axes.titlesize\"] = 26\n",
    "plt.rcParams[\"figure.figsize\"] = 16, 10\n",
    "plt.rcParams[\"figure.dpi\"] = 300\n",
    "plt.rcParams[\"xtick.labelsize\"] = 22\n",
    "plt.rcParams[\"ytick.labelsize\"] = 22\n",
    "plt.rcParams[\"legend.title_fontsize\"] = 22\n",
    "plt.rcParams[\"legend.fontsize\"] = 22\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "pd.options.display.float_format = \"{:.2f}\".format\n",
    "\n",
    "RANDOM_SEED=3141589\n",
    "\n",
    "tensorflow.random.set_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manual-pizza",
   "metadata": {},
   "source": [
    "**Step 2:** Define the location of our data as well as the relevant columns that we would like to include."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automotive-christian",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cols_UCID = [\n",
    "    \"Date\",\n",
    "    \"Time\",\n",
    "    \"Global_active_power\",\n",
    "    \"Global_reactive_power\",\n",
    "    \"Voltage\",\n",
    "    \"Global_intensity\",\n",
    "    \"Sub_metering_1\",\n",
    "    \"Sub_metering_2\",\n",
    "    \"Sub_metering_3\",\n",
    "]\n",
    "\n",
    "data_directory_UCID = os.path.join(\"Data\", \"UCI\")\n",
    "data_directory_Solcast = os.path.join(\"Data\", \"Solcast_UCID\")\n",
    "\n",
    "house = \"household_power_consumption.txt\"\n",
    "solcast_15 = \"Solcast_UCID_15.csv\"\n",
    "\n",
    "file_destination_UCID = os.path.join(data_directory_UCID, house)\n",
    "file_destination_Solcast = os.path.join(data_directory_Solcast, solcast_15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immune-middle",
   "metadata": {},
   "source": [
    "**Step 3:** Read in the data and save it to a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diverse-symphony",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_UCID = pd.read_csv(file_destination_UCID, parse_dates=True,  delimiter = \";\", usecols=cols_UCID)\n",
    "\n",
    "df_UCID[\"DT\"] = df_UCID[\"Date\"].str.cat(df_UCID[\"Time\"], sep=\" \")\n",
    "df_UCID[\"DT\"] = pd.to_datetime(df_UCID[\"DT\"], format=\"%d/%m/%Y %H:%M:%S\")\n",
    "\n",
    "df_UCID = df_UCID.reset_index()\n",
    "df_UCID = df_UCID.set_index(\"DT\")\n",
    "df_UCID.index = pd.to_datetime(df_UCID.index)\n",
    "\n",
    "cols_NA = [\n",
    "    \"index\",\n",
    "    \"Date\",\n",
    "    \"Time\",\n",
    "]\n",
    "\n",
    "df_UCID.drop(cols_NA, axis=1, inplace=True)\n",
    "\n",
    "cols = df_UCID.columns\n",
    "df_UCID[cols] = df_UCID[cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "cols_UCID.remove(\"Date\")\n",
    "cols_UCID.remove(\"Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "right-modem",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Solcast = pd.read_csv(file_destination_Solcast, index_col=0, parse_dates=True)\n",
    "\n",
    "df_Solcast.index = df_Solcast.index.rename(\"Time\")\n",
    "df_Solcast.index = pd.to_datetime(df_Solcast.index).tz_localize(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "humanitarian-float",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## 1.2: Scale the data in the dataframe(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intensive-telling",
   "metadata": {},
   "source": [
    "**Step 1.1:** Scale the data in a range between 0 and 1 (optional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immediate-arbitration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# minmax_UCID = MinMaxScaler()\n",
    "# minmax_Solcast = MinMaxScaler()\n",
    "\n",
    "# df_UCID[cols_UCID] = minmax_UCID.fit_transform(df_REFIT[cols_UCID])\n",
    "# df_Solcast[cols_Solcast] = minmax_Solcast.fit_transform(df_Solcast[cols_Solcast])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solar-farming",
   "metadata": {},
   "source": [
    "**Step 1.2:** Standardize the data by removing the mean and scaling to unit variance (optional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precious-diving",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardscale_UCID = StandardScaler()\n",
    "# standardscale_Solcast = StandardScaler()\n",
    "\n",
    "# df_UCID[cols_UCID] = standardscale_UCID.fit_transform(df_UCID[cols_UCID])\n",
    "# df_Solcast[cols_Solcast] = standardscale_Solcast.fit_transform(df_Solcast[cols_Solcast])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "communist-aquarium",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## 1.3: Merge the dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hundred-appraisal",
   "metadata": {},
   "source": [
    "**Step 1:** Create a copy of our REFIT dataframe that is resampled into a resolution of 15 minutes and drop any days that contain an incomplete number of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atlantic-phoenix",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_UCID_resampled = df_UCID.resample(\"15min\").mean()\n",
    "df_UCID_resampled = df_UCID_resampled.dropna()\n",
    "\n",
    "mask = df_UCID_resampled.groupby(df_UCID_resampled.index.date).size()\n",
    "mask = mask[mask < 96].index.to_list()\n",
    "\n",
    "df_UCID_resampled = df_UCID_resampled[~df_UCID_resampled.index.floor(\"D\").isin(mask)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extra-ontario",
   "metadata": {},
   "source": [
    "**Step 2:** Create a third dataframe that is the result of merging the Solcast dataframe with the REFIT dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diverse-adult",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Merged = pd.merge(left=df_Solcast, left_on=df_Solcast.index, right=df_UCID_resampled, right_on=df_UCID_resampled.index)\n",
    "\n",
    "cols_Merged = [\n",
    "    \"PeriodStart\",\n",
    "    \"Period\",\n",
    "    \"Global_reactive_power\",\n",
    "    \"Voltage\",\n",
    "    \"Global_intensity\",\n",
    "    \"Sub_metering_1\",\n",
    "    \"Sub_metering_2\",\n",
    "    \"Sub_metering_3\",\n",
    "]\n",
    "\n",
    "df_Merged.drop(cols_Merged, axis=1, inplace=True)\n",
    "df_Merged.rename(columns={\"key_0\": \"Time\"}, inplace=True)\n",
    "df_Merged = df_Merged.set_index(\"Time\")\n",
    "df_Merged.index = pd.to_datetime(df_Merged.index)\n",
    "df_Merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fancy-forth",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## 1.4: Append temporal features to our merged dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "herbal-commission",
   "metadata": {},
   "source": [
    "**Step 1:** Append public holidays to our merged dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overhead-planning",
   "metadata": {},
   "outputs": [],
   "source": [
    "France_holidays = holidays.France()\n",
    "df_Merged.insert(0, \"Holiday\", [1 if str(val).split()[0] in France_holidays else 0 for val in df_Merged.index.date])\n",
    "df_Merged[\"Holiday\"] = df_Merged[\"Holiday\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "textile-circulation",
   "metadata": {},
   "source": [
    "**Step 2:** Define day of the year ranges for each of the seasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twelve-roberts",
   "metadata": {},
   "outputs": [],
   "source": [
    "spring = range(60, 152)\n",
    "summer = range(152, 244)\n",
    "fall = range(244, 336)\n",
    "\n",
    "def season(doy):\n",
    "    if doy in spring:\n",
    "        return \"0\"\n",
    "    if doy in summer:\n",
    "        return \"1\"\n",
    "    if doy in fall:\n",
    "        return \"2\"\n",
    "    else:\n",
    "        return \"3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nutritional-sponsorship",
   "metadata": {},
   "source": [
    "**Step 3:** Append temporal data to our merged dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "delayed-underground",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Merged.insert(0, \"Year\", df_Merged.index.year)\n",
    "df_Merged.insert(1, \"Month\", df_Merged.index.month)\n",
    "df_Merged.insert(3, \"Day\", df_Merged.index.day)\n",
    "df_Merged.insert(4, \"Hour\", df_Merged.index.hour)\n",
    "df_Merged.insert(5, \"Minute\", df_Merged.index.minute)\n",
    "df_Merged.insert(6, \"Weekday\", df_Merged.index.weekday)\n",
    "df_Merged.insert(7, \"Season\", df_Merged.index.dayofyear.map(season))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d30e2b5-7ae3-4d2c-9874-c2de80d9115f",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## Miscellaneous functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65d73f9-cc9a-4655-8c99-de1fc1a2553b",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### 1) Augmented Dickey–Fuller test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5da5d68-83c1-4445-bd1b-a7115876e8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adfuller_test(series, signif=0.05, name=\"\"):\n",
    "    r = adfuller(series, autolag=\"AIC\")\n",
    "    output = {\"test_statistic\": round(r[0], 4), \"pvalue\": round(r[1], 4), \"n_lags\": round(r[2], 4), \"n_obs\": r[3]}\n",
    "    p_value = output[\"pvalue\"]\n",
    "\n",
    "    def adjust(val, length=6):\n",
    "        return str(val).ljust(length)\n",
    "\n",
    "    print(f'      Augmented Dickey-Fuller Test on \"{name}\"', \"\\n   \", \"-\" * 47)\n",
    "    print(f\" Null Hypothesis: Data has unit root. Non-Stationary.\")\n",
    "    print(f\" Significance Level    = {signif}\")\n",
    "    print(f' Test Statistic        = {output[\"test_statistic\"]}')\n",
    "    print(f' No. Lags Chosen       = {output[\"n_lags\"]}')\n",
    "\n",
    "    for key, val in r[4].items():\n",
    "        print(f\" Critical value {adjust(key)} = {round(val, 3)}\")\n",
    "    if p_value <= signif:\n",
    "        print(f\" => P-Value = {p_value}. Rejecting Null Hypothesis.\")\n",
    "        print(f\" => Series is Stationary.\")\n",
    "    else:\n",
    "        print(f\" => P-Value = {p_value}. Weak evidence to reject the Null Hypothesis.\")\n",
    "        print(f\" => Series is Non-Stationary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b031aee-d36d-4441-a6db-19256cb0b597",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### 2) Augmented Dickey–Fuller test w/plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033c3c6e-5efa-44ae-a7b4-e4bbd22114ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_stationarity(series, signif=0.05, name=\"\", ylabel=\"\"):\n",
    "    def adjust(val, length=6):\n",
    "        return str(val).ljust(length)\n",
    "\n",
    "    rolmean = series.rolling(12).mean()\n",
    "    rolstd = series.rolling(12).std()\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    series.plot(ax=ax, alpha=0.5)\n",
    "    rolmean.plot(ax=ax, alpha=0.7)\n",
    "    rolstd.plot(ax=ax, alpha=0.7)\n",
    "\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_xlim(left=0, right=len(series))\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.title(\"Rolling Mean & Standard Deviation\")\n",
    "    plt.setp(ax.get_xticklabels(), ha=\"right\", rotation=60)\n",
    "\n",
    "    leg = plt.legend()\n",
    "    leg.get_texts()[0].set_text(name)\n",
    "    leg.get_texts()[1].set_text(\"Rolling Mean\")\n",
    "    leg.get_texts()[2].set_text(\"Rolling STD\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show(block=False)\n",
    "\n",
    "    adfuller_test(series, 0.05, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139dca84-eddf-4b6f-9989-d2868f8877aa",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### 3) Granger Causality test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b7c15d-931b-4837-8bed-6bbd78358467",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grangers_causation_matrix(data, variables, test=\"ssr_chi2test\", maxlag=12, verbose=False):\n",
    "    df = pd.DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)\n",
    "    for c in df.columns:\n",
    "        for r in df.index:\n",
    "            test_result = grangercausalitytests(data[[r, c]], maxlag=maxlag, verbose=False)\n",
    "            p_values = ([round(test_result[i + 1][0][test][1], 2) for i in range(maxlag)])\n",
    "            if verbose:\n",
    "                print(f\"Y = {r}, X = {c}, P Values = {p_values}\")\n",
    "            min_p_value = np.min(p_values)\n",
    "            df.loc[r, c] = min_p_value\n",
    "    df.columns = [var + \"_x\" for var in variables]\n",
    "    df.index = [var + \"_y\" for var in variables]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f81f2a-a75b-4179-8921-8fd51bffcc02",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### 4) Determine which highly correlated independent variables have a stronger correlation with our target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641d6c5b-d23e-4f55-bfb4-dc38158f6cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(df, threshold, target_variable):\n",
    "    col_corr = set()\n",
    "    corr_matrix = df.corr()\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold:\n",
    "                colname = corr_matrix.columns[i]\n",
    "                rowname = corr_matrix.index[j]\n",
    "                cor1 = abs(df[colname].corr(target_variable))\n",
    "                cor2 = abs(df[rowname].corr(target_variable))\n",
    "                if  cor1 > cor2:\n",
    "                    col_corr.add(corr_matrix.index[j])\n",
    "                else:\n",
    "                    col_corr.add(corr_matrix.columns[i])\n",
    "    return col_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee79f04-4652-4513-98f1-4931c6aecde1",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### 5) Reshape correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d93aaa-8fad-4d42-80fe-b2be5964919c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_corr(df):\n",
    "    df_corr = df.corr().stack().reset_index()\n",
    "    df_corr.columns = [\"Feature 1\", \"Feature 2\", \"Correlation\"]\n",
    "    mask_dups = (df_corr[[\"Feature 1\", \"Feature 2\"]].apply(frozenset, axis=1).duplicated()) | (df_corr[\"Feature 1\"] == df_corr[\"Feature 2\"])\n",
    "    df_corr = df_corr[~mask_dups]\n",
    "\n",
    "    return df_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfa8ba2-366a-4e6d-b4cd-bf546674f07a",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### 6) Forecasting accuracy metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f79295-2773-41f1-877f-c32328305c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_accuracy(forecast, actual):\n",
    "    mape = np.round(np.mean(np.abs(forecast - actual) / np.abs(actual)) * 100, 2)\n",
    "    mae = np.round(np.mean(np.abs(forecast - actual)), 2)\n",
    "    rmse = np.round(np.mean((forecast - actual) ** 2) ** 0.5, 2)\n",
    "\n",
    "    print(\"Forecasting accuracy metrics:\")\n",
    "    print(f\"\\t - MAPE = {mape}%\")\n",
    "    print(f\"\\t - MAE = {mae}\")\n",
    "    print(f\"\\t - RMSE = {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b36631-3617-4a5d-8979-f9fc90fd468e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(forecast, actual):\n",
    "    mape = np.round(np.mean(np.abs(forecast - actual) / np.abs(actual)) * 100, 2)\n",
    "    return mape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f268b3b8-f6d4-4944-bf4f-0308272d1882",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### 7) Reshape data into a suitable format for single step forecasting using our CNN-LSTM architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7ebaeb-4067-4e45-a2a3-0fbe44151881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_Supervised(X, y, time_steps=1):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        v = X.iloc[i : (i + time_steps)].values\n",
    "        Xs.append(v)\n",
    "        ys.append(y.iloc[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd4a6f3-fd41-4ab1-bd86-c24c367ee648",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### 8) Reshape data into a suitable format for multi-step forecasting using our CNN-LSTM architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ace9d4-58f2-4367-8461-d9f61ccb2d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_Supervised_ms(ts: np.array, lag=1, n_ahead=1, target_index=0) -> tuple:\n",
    "    n_features = ts.shape[1]\n",
    "    X, Y = [], []\n",
    "\n",
    "    if len(ts) - lag <= 0:\n",
    "        X.append(ts)\n",
    "    else:\n",
    "        for i in range(len(ts) - lag - n_ahead):\n",
    "            Y.append(ts[(i + lag) : (i + lag + n_ahead), target_index])\n",
    "            X.append(ts[i : (i + lag)])\n",
    "    X, Y = np.array(X), np.array(Y)\n",
    "\n",
    "    X = np.reshape(X, (X.shape[0], lag, n_features))\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "included-nickname",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "# Part 2: Clustering <a id=\"Part2_UCID\"> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modern-sleeping",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## 2.1: Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simplified-solution",
   "metadata": {},
   "source": [
    "**Step 1:** Create a copy of our REFIT dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gross-profession",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_UCID_resampled_c = df_UCID_resampled.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equal-measurement",
   "metadata": {},
   "source": [
    "**Step 2:** Drop all columns barre the `Global_active_power` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standing-hungarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_UCID_c = cols_UCID.copy()\n",
    "cols_UCID_c.remove(\"Global_active_power\")\n",
    "df_UCID_resampled_c.drop(cols_UCID_c, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparable-disabled",
   "metadata": {},
   "source": [
    "**Step 3:** Reshape our dataframe as 96 columns that represent the 96 15-minute chunks of each day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civil-profession",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_UCID_resampled_c.index = pd.MultiIndex.from_arrays([df_UCID_resampled_c.index.date, df_UCID_resampled_c.index.time], names=[\"Date\", \"Time\"])\n",
    "df_UCID_resampled_c = df_UCID_resampled_c.unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parental-thriller",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### 2.1.1: Statistical parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instant-leeds",
   "metadata": {},
   "source": [
    "**Step 1:** Split our day into 5 periods:\n",
    " - `LEEM`: Late evening/early morning (23:30-06:00)\n",
    " - `MR`: Morning (06:00-11:00)\n",
    " - `LMAF`: Late morning/afternoon (11:00-15:00)\n",
    " - `LAEE`: Late afternoon/early evening (15:00-20:30)\n",
    " - `EV`: Evening (20:30-23:30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fancy-belarus",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEEM = df_UCID_resampled_c.iloc[:, np.r_[0:24, 92:96]]\n",
    "MR = df_UCID_resampled_c.iloc[:, 24:45]\n",
    "LMAF = df_UCID_resampled_c.iloc[:, 44:61]\n",
    "LAEE = df_UCID_resampled_c.iloc[:, 60:83]\n",
    "EV = df_UCID_resampled_c.iloc[:, 82:95]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retired-wagner",
   "metadata": {},
   "source": [
    "**Step 2:** Create a new dataframe that consists of the mean, min, max and standard deviation of each of our 5 periods per day. We now represent each day with 20 variables rather than 96."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "western-computer",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SP = LEEM.mean(axis=1).to_frame(name=\"LEEM_Mean\")\n",
    "df_SP.insert(len(df_SP.columns), \"LEEM_Min\", LEEM.min(axis=1))\n",
    "df_SP.insert(len(df_SP.columns), \"LEEM_Max\", LEEM.max(axis=1))\n",
    "df_SP.insert(len(df_SP.columns), \"LEEM_STD\", LEEM.std(axis=1))\n",
    "df_SP.insert(len(df_SP.columns), \"MR_Mean\", MR.mean(axis=1))\n",
    "df_SP.insert(len(df_SP.columns), \"MR_Min\", MR.min(axis=1))\n",
    "df_SP.insert(len(df_SP.columns), \"MR_Max\", MR.max(axis=1))\n",
    "df_SP.insert(len(df_SP.columns), \"MR_STD\", MR.std(axis=1))\n",
    "df_SP.insert(len(df_SP.columns), \"LMAF_Mean\", LMAF.mean(axis=1))\n",
    "df_SP.insert(len(df_SP.columns), \"LMAF_Min\", LMAF.min(axis=1))\n",
    "df_SP.insert(len(df_SP.columns), \"LMAF_Max\", LMAF.max(axis=1))\n",
    "df_SP.insert(len(df_SP.columns), \"LMAF_STD\", LMAF.std(axis=1))\n",
    "df_SP.insert(len(df_SP.columns), \"LAEE_Mean\", LAEE.mean(axis=1))\n",
    "df_SP.insert(len(df_SP.columns), \"LAEE_Min\", LAEE.min(axis=1))\n",
    "df_SP.insert(len(df_SP.columns), \"LAEE_Max\", LAEE.max(axis=1))\n",
    "df_SP.insert(len(df_SP.columns), \"LAEE_STD\", LAEE.std(axis=1))\n",
    "df_SP.insert(len(df_SP.columns), \"EV_Mean\", EV.mean(axis=1))\n",
    "df_SP.insert(len(df_SP.columns), \"EV_Min\", EV.min(axis=1))\n",
    "df_SP.insert(len(df_SP.columns), \"EV_Max\", EV.max(axis=1))\n",
    "df_SP.insert(len(df_SP.columns), \"EV_STD\", EV.std(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungarian-stadium",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### 2.1.2: tSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "figured-natural",
   "metadata": {},
   "source": [
    "**Step 1:** Apply tSNE to our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "royal-privilege",
   "metadata": {},
   "outputs": [],
   "source": [
    "projection = TSNE(random_state=3141589, perplexity=np.power(len(df_UCID_resampled_c), 0.5).astype(int), learning_rate=550, n_iter=5000).fit_transform(df_SP)\n",
    "projection = TSNE(random_state=3141589).fit_transform(projection)\n",
    "plt.scatter(*projection.T)\n",
    "plt.xticks(color=\"w\")\n",
    "plt.yticks(color=\"w\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf25fd1-e42c-4d17-bec6-145ebd33dc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Split_CL = int(len(projection) * 0.8)\n",
    "projection = projection[0:Split_CL]\n",
    "df_UCID_resampled_CL = df_UCID_resampled_c[0:Split_CL].copy()\n",
    "df_UCID_resampled_TE = df_UCID_resampled_c[Split_CL:].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liquid-meter",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## 2.2: HDBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-booking",
   "metadata": {},
   "source": [
    "**Step 1:** Define our HDBSCAN clusterer with the appropriate hyperparameters and fit it to our 2-dimensional projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minus-housing",
   "metadata": {},
   "outputs": [],
   "source": [
    "HDB = hdbscan.HDBSCAN(min_cluster_size=(len(projection) // 10), min_samples=7)\n",
    "HDB = HDB.fit(projection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regional-finder",
   "metadata": {},
   "outputs": [],
   "source": [
    "HDB.condensed_tree_.plot(select_clusters=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plastic-photographer",
   "metadata": {},
   "source": [
    "**Step 2:** Plot/visualize our clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "freelance-seafood",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = HDB.labels_\n",
    "labels = [label + 1 for label in labels]\n",
    "n_clusters = len(set(labels)) - (1 if 0 in labels else 0)\n",
    "n_noise = list(labels).count(0)\n",
    "core_samples_mask = np.zeros_like(labels, dtype=bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "editorial-encyclopedia",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = set(labels)\n",
    "colors = np.array(sns.color_palette(\"bright\", len(unique_labels)))\n",
    "colors[0] = [0, 0, 0]\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k == 0:\n",
    "        # Black used for noise.\n",
    "        col = [0, 0, 0, 1]\n",
    "    class_member_mask = labels == k\n",
    "\n",
    "    xy = projection[class_member_mask & core_samples_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], \"o\", markerfacecolor=tuple(col), markeredgecolor=\"k\", markersize=14)\n",
    "\n",
    "    xy = projection[class_member_mask & ~core_samples_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], \"o\", markerfacecolor=tuple(col), markeredgecolor=\"k\", markersize=6)\n",
    "plt.xticks(color=\"w\")\n",
    "plt.yticks(color=\"w\")\n",
    "plt.title(\"Estimated number of clusters: %d\" % n_clusters)\n",
    "plt.show()\n",
    "\n",
    "print(\"Estimated number of clusters: %d\" % n_clusters)\n",
    "print(\"Estimated number of noise points: %d\" % n_noise)\n",
    "print(\"Silhouette Coefficient: %0.3f\" % metrics.silhouette_score(projection, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "familiar-newspaper",
   "metadata": {},
   "source": [
    "**Step 3:** Plot/visualize the average pattern per cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banned-separation",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_UCID_resampled_CL[\"Labels\"] = labels\n",
    "Cx = []\n",
    "\n",
    "for i in range(0, n_clusters + 1):\n",
    "    if(i == 0):\n",
    "        Cx.append(\"Noise\")\n",
    "    else:\n",
    "        Cx.append(f\"Cluster \" + str(i))\n",
    "    globals()[\"C\" + str(i)] = (df_UCID_resampled_CL.loc[df_UCID_resampled_CL[\"Labels\"] == i]).mean(axis=0)\n",
    "    globals()[\"C\" + str(i)] = globals()[\"C\" + str(i)].reset_index()\n",
    "    globals()[\"C\" + str(i)].drop(\"level_0\", axis=1, inplace=True)\n",
    "    globals()[\"C\" + str(i)].drop(globals()[\"C\" + str(i)].tail(1).index, inplace=True)\n",
    "    globals()[\"C\" + str(i)][\"Time\"] = globals()[\"C\" + str(i)][\"Time\"].astype(\"str\")\n",
    "    globals()[\"C\" + str(i)][\"Time\"] = pd.to_datetime(globals()[\"C\" + str(i)][\"Time\"])\n",
    "    globals()[\"C\" + str(i)] = globals()[\"C\" + str(i)].set_index(\"Time\")\n",
    "    globals()[\"C\" + str(i)].index = globals()[\"C\" + str(i)].index.strftime(\"%H:%M:%S\")\n",
    "    globals()[\"C\" + str(i)] = globals()[\"C\" + str(i)].rename(columns={0: \"Aggregate\"})\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(0, n_clusters + 1):\n",
    "    globals()[\"C\" + str(i)].plot(ax=ax, color=colors[i], linewidth=2)\n",
    "\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.set_ylabel(\"Aggregate Power Consumption (Kilowatts)\")\n",
    "ax.set_xlim(left=0, right=95)\n",
    "plt.legend([*Cx], loc=\"best\", fontsize=18)\n",
    "plt.setp(ax.get_xticklabels(), ha=\"right\", rotation=60)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broken-medline",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_UCID_resampled_CL.index = pd.DatetimeIndex(df_UCID_resampled_CL.index)\n",
    "df_UCID_resampled_CL.insert(0, \"Month_name\", df_UCID_resampled_CL.index.month_name())\n",
    "df_UCID_resampled_CL.insert(0, \"Day_name\", df_UCID_resampled_CL.index.day_name())\n",
    "\n",
    "Cx1 = Cx.copy()\n",
    "Cx1.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff30d59a-f9db-45a8-be19-2a081c6fd63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = df_UCID_resampled_CL.copy()\n",
    "x1.index = x1.index.month_name()\n",
    "x1 = x1[~x1[\"Labels\"].isin([0])]\n",
    "\n",
    "kde_kws = {\"bw_adjust\": 1.0, \"bw_method\": \"silverman\"}\n",
    "line_kws = {\"linewidth\": 2, \"alpha\": 1.0}\n",
    "colors2 = np.delete(colors, 0, 0)\n",
    "order = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n",
    "\n",
    "ax = sns.histplot(\n",
    "    data=x1.loc[order],\n",
    "    x=\"Month_name\",\n",
    "    hue=\"Labels\",\n",
    "    alpha=0.75,\n",
    "    palette=list(colors2),\n",
    "    multiple=\"dodge\",\n",
    "    kde=True,\n",
    "    kde_kws=kde_kws,\n",
    "    shrink=0.75,\n",
    "    line_kws=line_kws,\n",
    "    legend='reverse'\n",
    ")\n",
    "ax.set_xlim(-0.75, 11.75)\n",
    "legend = ax.get_legend()\n",
    "handles = legend.legendHandles\n",
    "legend.remove()\n",
    "ax.legend(handles, [*Cx1], title=\"Labels\")\n",
    "plt.setp(ax.get_xticklabels(), ha=\"right\", rotation=60)\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0c5dde-969b-4f87-8fe0-f3643c0e35b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = df_UCID_resampled_CL.copy()\n",
    "x1.index = x1.index.day_name()\n",
    "x1 = x1[~x1[\"Labels\"].isin([0])]\n",
    "\n",
    "kde_kws = {\"bw_adjust\": 1.0, \"bw_method\": \"silverman\"}\n",
    "line_kws = {\"linewidth\": 2, \"alpha\": 1.0}\n",
    "colors2 = np.delete(colors, 0, 0)\n",
    "order = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
    "\n",
    "ax = sns.histplot(\n",
    "    data=x1.loc[order],\n",
    "    x=\"Day_name\",\n",
    "    hue=\"Labels\",\n",
    "    alpha=0.75,\n",
    "    palette=list(colors2),\n",
    "    multiple=\"dodge\",\n",
    "    kde=True,\n",
    "    kde_kws=kde_kws,\n",
    "    shrink=0.75,\n",
    "    line_kws=line_kws,\n",
    ")\n",
    "ax.set_xlim(-0.75, 6.75)\n",
    "legend = ax.get_legend()\n",
    "handles = legend.legendHandles\n",
    "legend.remove()\n",
    "ax.legend(handles, [*Cx1], title=\"Labels\", loc=\"lower right\")\n",
    "plt.setp(ax.get_xticklabels(), ha=\"right\", rotation=60)\n",
    "plt.xlabel(\"Day\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9e20ea-1acc-4791-9e5c-dab7b8acbcb9",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "# Part 3: CART <a id=\"Part3_UCID\"> </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be915802-961f-402a-b057-d42d7432dc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "CART_CL = df_UCID_resampled_CL.copy()\n",
    "CART_CL.drop(CART_CL.iloc[:, 0:2], axis=1, inplace=True)\n",
    "CART_CL.drop(CART_CL.iloc[:, 0:96], axis=1, inplace=True)\n",
    "CART_CL.columns = CART_CL.columns.droplevel(1)\n",
    "CART_CL = CART_CL[~CART_CL[\"Labels\"].isin([0])]\n",
    "colors2 = np.delete(colors, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda2b528-71a0-49e9-989c-2ded7c0921f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Merged2 = df_Merged.copy()\n",
    "df_Merged2.drop(\"Global_active_power\", axis=1, inplace=True)\n",
    "df_Merged2.drop(df_Merged2.iloc[:, 0:8], axis=1, inplace=True)\n",
    "df_Merged2 = df_Merged2.resample(\"1D\").agg([\"min\", \"max\",\"std\", \"mean\"]).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2b9124-b9f0-4971-9bdf-e93705da9a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "CART_CL = pd.merge(left=df_Merged2, left_on=df_Merged2.index, right=CART_CL, right_on=CART_CL.index)\n",
    "CART_CL.rename(columns={\"key_0\": \"Date\"}, inplace=True)\n",
    "CART_CL = CART_CL.set_index(\"Date\")\n",
    "CART_CL.index = pd.to_datetime(CART_CL.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b5b271-1fd5-4249-8f47-0301dcf7b670",
   "metadata": {},
   "outputs": [],
   "source": [
    "CART2 = CART_CL.copy()\n",
    "CART2 = CART2.loc[:, CART2.columns != \"Labels\"].rename(columns='_'.join)\n",
    "CART2.insert(len(CART2.columns), \"Labels\", CART_CL.Labels)\n",
    "CART_CL = CART2.copy()\n",
    "del CART2\n",
    "CART_CL.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3d7300-eb20-4eab-893b-0ae1afe8c485",
   "metadata": {},
   "outputs": [],
   "source": [
    "CART_CL.insert(len(CART_CL.columns), \"Day\", CART_CL.index.day)\n",
    "CART_CL.insert(len(CART_CL.columns), \"Month\", CART_CL.index.month)\n",
    "CART_CL.insert(len(CART_CL.columns), \"Year\", CART_CL.index.year)\n",
    "CART_CL.insert(len(CART_CL.columns), \"Day Of Week\", CART_CL.index.dayofweek)\n",
    "CART_CL.insert(len(CART_CL.columns), \"Day Of Year\", CART_CL.index.dayofyear)\n",
    "\n",
    "CART_CL[\"Day\"] = CART_CL[\"Day\"].astype(\"category\")\n",
    "CART_CL[\"Month\"] = CART_CL[\"Month\"].astype(\"category\")\n",
    "CART_CL[\"Year\"] = CART_CL[\"Year\"].astype(\"category\")\n",
    "CART_CL[\"Day Of Week\"] = CART_CL[\"Day Of Week\"].astype(\"category\")\n",
    "CART_CL[\"Day Of Year\"] = CART_CL[\"Day Of Year\"].astype(\"category\")\n",
    "CART_CL[\"Labels\"] = CART_CL[\"Labels\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72c2b86-d24f-4928-8fb1-7411df9f892b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.countplot(x=CART_CL[\"Labels\"], data=CART_CL, palette=colors2)\n",
    "plt.title(\"Number of days per cluster\")\n",
    "plt.xlabel(\"Label\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d68153-fa97-41a2-b2f8-412b958ae5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = CART_CL.select_dtypes(include=[\"category\"]).columns\n",
    "cols = [CART_CL.columns.get_loc(col) for col in cols]\n",
    "del cols[0]\n",
    "cols = [col - 1 for col in cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c349aa-5492-408e-8aee-a35eb6d7c9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "smotenc = SMOTENC(random_state=RANDOM_SEED, categorical_features=[*cols])\n",
    "X, y = (CART_CL.loc[:, CART_CL.columns != \"Labels\"], CART_CL[\"Labels\"].to_frame())\n",
    "X, y = smotenc.fit_resample(X, y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y[\"Labels\"], random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455fece2-5a33-406a-8dfd-40ed89994e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.countplot(x=y[\"Labels\"], data=y, palette=colors2)\n",
    "plt.title(\"Number of days per cluster\")\n",
    "plt.xlabel(\"Label\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fa4f36-6c1d-4432-a181-fd4626b6dbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF = RandomForestClassifier(random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fb8792-cb10-4448-a558-aaeea4e1dd91",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## 3.1 - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4aa71b-03cd-4c85-8fa9-1f0f5e321d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_estimators = [int(x) for x in np.linspace(200, 1600, num=8)]\n",
    "# max_features = ['auto', 'log2']\n",
    "# max_depth = [int(x) for x in np.linspace(10, 110, num=11)]\n",
    "# max_depth.append(None)\n",
    "# min_samples_split = [2, 5, 10]\n",
    "# min_samples_leaf = [1, 2, 4, 6, 8, 10]\n",
    "# bootstrap = [True, False]\n",
    "\n",
    "# random_grid = {\n",
    "#     \"n_estimators\": n_estimators,\n",
    "#     \"max_features\": max_features,\n",
    "#     \"max_depth\": max_depth,\n",
    "#     \"min_samples_split\": min_samples_split,\n",
    "#     \"min_samples_leaf\": min_samples_leaf,\n",
    "#     \"bootstrap\": bootstrap,\n",
    "# }\n",
    "\n",
    "# RF = RandomForestClassifier(random_state=RANDOM_SEED)\n",
    "# RF_CV = RandomizedSearchCV(RF, random_grid, n_iter=50, verbose=2)\n",
    "# RF_CV.fit(X, y.values.ravel())\n",
    "\n",
    "# print(f\"Tuned Random Forest Parameters: {RF_CV.best_params_}\")\n",
    "# print(f\"Best score is {RF_CV.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a702f9-920f-423f-886a-dc2523616bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF = RandomForestClassifier(random_state=RANDOM_SEED)\n",
    "#RF.set_params(**RF_CV.best_params_)\n",
    "RF.fit(X_train, y_train.values.ravel())\n",
    "RF.score(X_test, y_test), RF.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0530554-2a94-4330-92c6-f8918c9c012a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CART_TE = df_UCID_resampled_TE.copy()\n",
    "CART_TE.drop(CART_TE.iloc[:, 0:96], axis=1, inplace=True)\n",
    "CART_TE.columns = CART_TE.columns.droplevel(1)\n",
    "colors2 = np.delete(colors, 0, 0)\n",
    "CART_TE.index = pd.DatetimeIndex(CART_TE.index)\n",
    "CART_TE.insert(len(CART_TE.columns), \"Labels\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734c857e-995c-44df-8528-26da7e141239",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Merged2 = df_Merged.copy()\n",
    "df_Merged2.drop(\"Global_active_power\", axis=1, inplace=True)\n",
    "df_Merged2.drop(df_Merged2.iloc[:, 0:8], axis=1, inplace=True)\n",
    "df_Merged2 = df_Merged2.resample(\"1D\").agg([\"min\", \"max\",\"std\", \"mean\"]).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b647ca52-e6be-49b3-958e-c378cdd353a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "CART_TE = pd.merge(left=df_Merged2, left_on=df_Merged2.index, right=CART_TE, right_on=CART_TE.index)\n",
    "CART_TE.rename(columns={\"key_0\": \"Date\"}, inplace=True)\n",
    "CART_TE = CART_TE.set_index(\"Date\")\n",
    "CART_TE.index = pd.to_datetime(CART_TE.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b299339d-2a70-40df-a8e4-d3701d069cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "CART2 = CART_TE.copy()\n",
    "CART2 = CART2.loc[:, CART2.columns != \"Labels\"].rename(columns='_'.join)\n",
    "CART_TE = CART2.copy()\n",
    "del CART2\n",
    "CART_TE.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66f884f-2fb2-4400-86bd-6a5302d86f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "CART_TE.insert(len(CART_TE.columns), \"Day\", CART_TE.index.day)\n",
    "CART_TE.insert(len(CART_TE.columns), \"Month\", CART_TE.index.month)\n",
    "CART_TE.insert(len(CART_TE.columns), \"Year\", CART_TE.index.year)\n",
    "CART_TE.insert(len(CART_TE.columns), \"Day Of Week\", CART_TE.index.dayofweek)\n",
    "CART_TE.insert(len(CART_TE.columns), \"Day Of Year\", CART_TE.index.dayofyear)\n",
    "\n",
    "CART_TE[\"Day\"] = CART_TE[\"Day\"].astype(\"category\")\n",
    "CART_TE[\"Month\"] = CART_TE[\"Month\"].astype(\"category\")\n",
    "CART_TE[\"Year\"] = CART_TE[\"Year\"].astype(\"category\")\n",
    "CART_TE[\"Day Of Week\"] = CART_TE[\"Day Of Week\"].astype(\"category\")\n",
    "CART_TE[\"Day Of Year\"] = CART_TE[\"Day Of Year\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca29a02a-268f-474b-94f3-29a98e65ca4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Labels_test = RF.predict(CART_TE)\n",
    "CART_TE.insert(len(CART_TE.columns), \"Labels\", Labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f7e28f-50e6-4ff9-bb8b-2f2ffc83694d",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "# Part 4: Forecasting <a id=\"Part4_UCID\"> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05caec57-cb24-4559-b1a3-d10a5063f230",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## 4.1: Trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d463ee58-0621-4178-8e01-e4044ff4b9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "C1_Merge_train = CART_CL.loc[:, \"Labels\"].to_frame()\n",
    "df_UCID_resampled_train = df_UCID_resampled_CL.copy()\n",
    "df_UCID_resampled_train.Labels = C1_Merge_train.Labels\n",
    "df_UCID_resampled_train.dropna(inplace=True)\n",
    "C1_Merge_train = df_UCID_resampled_train.loc[df_UCID_resampled_train[\"Labels\"] == 1].copy()\n",
    "C1_Merge_train = C1_Merge_train.stack().reset_index()\n",
    "C1_Merge_train[\"Date\"] = C1_Merge_train[\"Date\"].astype(\"str\")\n",
    "C1_Merge_train[\"Time\"] = C1_Merge_train[\"Time\"].astype(\"str\")\n",
    "C1_Merge_train[\"DT\"] = C1_Merge_train[\"Date\"].str.cat(C1_Merge_train[\"Time\"], sep=\" \")\n",
    "\n",
    "cols_NA = [\n",
    "    \"Date\",\n",
    "    \"Time\",\n",
    "    \"Labels\",\n",
    "    \"Day_name\",\n",
    "    \"Month_name\"\n",
    "]\n",
    "\n",
    "C1_Merge_train.drop(cols_NA, axis=1, inplace=True)\n",
    "C1_Merge_train[\"DT\"] = pd.to_datetime(C1_Merge_train[\"DT\"])\n",
    "C1_Merge_train = C1_Merge_train.set_index(\"DT\")\n",
    "C1_Merge_train.dropna(inplace=True)\n",
    "C1_Merge_train = df_Merged.reindex(C1_Merge_train.index)\n",
    "C1_Merge_train = C1_Merge_train.dropna()\n",
    "C1_Merge_train[\"Holiday\"] = C1_Merge_train[\"Holiday\"].astype('int32')\n",
    "C1_Merge_train[\"Season\"] = C1_Merge_train[\"Season\"].astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242093ef-2179-48f6-a404-a382fd2e77fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "C1_Merge_test = CART_TE.loc[:, \"Labels\"].to_frame()\n",
    "df_UCID_resampled_test = df_UCID_resampled_TE.copy()\n",
    "df_UCID_resampled_test.insert(len(df_UCID_resampled_test.columns), \"Labels\", C1_Merge_test.Labels)\n",
    "df_UCID_resampled_test.dropna(inplace=True)\n",
    "C1_Merge_test = df_UCID_resampled_test.loc[df_UCID_resampled_test[\"Labels\"] == 1].copy()\n",
    "C1_Merge_test = C1_Merge_test.stack().reset_index()\n",
    "C1_Merge_test[\"Date\"] = C1_Merge_test[\"Date\"].astype(\"str\")\n",
    "C1_Merge_test[\"Time\"] = C1_Merge_test[\"Time\"].astype(\"str\")\n",
    "C1_Merge_test[\"DT\"] = C1_Merge_test[\"Date\"].str.cat(C1_Merge_test[\"Time\"], sep=\" \")\n",
    "\n",
    "cols_NA = [\n",
    "    \"Date\",\n",
    "    \"Time\",\n",
    "    \"Labels\",\n",
    "]\n",
    "\n",
    "C1_Merge_test.drop(cols_NA, axis=1, inplace=True)\n",
    "C1_Merge_test[\"DT\"] = pd.to_datetime(C1_Merge_test[\"DT\"])\n",
    "C1_Merge_test = C1_Merge_test.set_index(\"DT\")\n",
    "C1_Merge_test.dropna(inplace=True)\n",
    "C1_Merge_test = df_Merged.reindex(C1_Merge_test.index)\n",
    "C1_Merge_test = C1_Merge_test.dropna()\n",
    "C1_Merge_test[\"Holiday\"] = C1_Merge_test[\"Holiday\"].astype('int32')\n",
    "C1_Merge_test[\"Season\"] = C1_Merge_test[\"Season\"].astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c75f1e-390c-43e2-881e-da016cd482f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "day = 24 * 60 * 60\n",
    "year = (365.2425) * day\n",
    "timestamp_train = C1_Merge_train.index.map(datetime.datetime.timestamp)\n",
    "timestamp_test = C1_Merge_test.index.map(datetime.datetime.timestamp)\n",
    "\n",
    "day_sin_tr = np.sin(timestamp_train * (2 * np.pi / day))\n",
    "day_cos_tr = np.cos(timestamp_train * (2 * np.pi / day))\n",
    "year_sin_tr = np.sin(timestamp_train * (2 * np.pi / year))\n",
    "year_cos_tr = np.cos(timestamp_train * (2 * np.pi / year))\n",
    "\n",
    "day_sin_te = np.sin(timestamp_test * (2 * np.pi / day))\n",
    "day_cos_te = np.cos(timestamp_test * (2 * np.pi / day))\n",
    "year_sin_te = np.sin(timestamp_test * (2 * np.pi / year))\n",
    "year_cos_te = np.cos(timestamp_test * (2 * np.pi / year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f2444a-fb1b-4556-8982-0244d617fa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_NA = [\"Year\", \"Month\", \"Holiday\", \"Day\", \"Hour\", \"Minute\", \"Weekday\", \"Season\"]\n",
    "\n",
    "C1_Merge_train.drop(cols_NA, axis=1, inplace=True)\n",
    "C1_Merge_test.drop(cols_NA, axis=1, inplace=True)\n",
    "\n",
    "C1_Merge_train.insert(0, \"Day_Sin\", day_sin_tr)\n",
    "C1_Merge_train.insert(0, \"Day_Cos\", day_cos_tr)\n",
    "C1_Merge_train.insert(0, \"Year_Sin\", year_sin_tr)\n",
    "C1_Merge_train.insert(0, \"Year_Cos\", year_cos_tr)\n",
    "\n",
    "C1_Merge_test.insert(0, \"Day_Sin\", day_sin_te)\n",
    "C1_Merge_test.insert(0, \"Day_Cos\", day_cos_te)\n",
    "C1_Merge_test.insert(0, \"Year_Sin\", year_sin_te)\n",
    "C1_Merge_test.insert(0, \"Year_Cos\", year_cos_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ffb92d-2e1e-43ef-981f-a8fe5627a11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = (24 * 60) // 15\n",
    "stl_decompose_result = STL(C1_Merge_train.Global_active_power, period=freq).fit()\n",
    "C1_Merge_train.insert(len(C1_Merge_train.columns), \"Trend\", stl_decompose_result.trend.values)\n",
    "\n",
    "stl_decompose_result = STL(C1_Merge_test.Global_active_power, period=freq).fit()\n",
    "C1_Merge_test.insert(len(C1_Merge_test.columns), \"Trend\", stl_decompose_result.trend.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4486f6e-b231-4799-a373-898d5e87a4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_tr = MaxAbsScaler()\n",
    "scaler_te = MaxAbsScaler()\n",
    "\n",
    "C1_Merge_train[C1_Merge_train.columns] = scaler_tr.fit_transform(C1_Merge_train[C1_Merge_train.columns])\n",
    "scaler_tr.max_abs_, scaler_tr.scale_ = scaler_tr.max_abs_[len(C1_Merge_train.columns) - 1], scaler_tr.scale_[len(C1_Merge_train.columns) - 1]\n",
    "\n",
    "C1_Merge_test[C1_Merge_test.columns] = scaler_te.fit_transform(C1_Merge_test[C1_Merge_test.columns])\n",
    "scaler_te.max_abs_, scaler_te.scale_ = scaler_te.max_abs_[len(C1_Merge_test.columns) - 1], scaler_te.scale_[len(C1_Merge_test.columns) - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124f6e6e-60e7-46e3-8292-4e903c9df058",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = C1_Merge_train.shape[1]\n",
    "time_steps = 24\n",
    "Split = len(C1_Merge_train)\n",
    "\n",
    "s_Train = C1_Merge_train.iloc[0 : int(Split * 0.8)]\n",
    "s_Val = C1_Merge_train.iloc[int(Split * 0.8) :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c316cb86-6dff-434a-8193-0b5a83edaaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = to_Supervised(s_Train, s_Train.Trend, time_steps)\n",
    "X_val, Y_val = to_Supervised(s_Val, s_Val.Trend, time_steps)\n",
    "X_test, Y_test = to_Supervised(C1_Merge_test, C1_Merge_test.Trend, time_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f9a91c-df5d-4d0a-90ca-37d67303ff6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(\n",
    "    [\n",
    "        Conv1D(filters=64, kernel_size=3, activation=LeakyReLU(), padding=\"same\", input_shape=(time_steps, n_features)),\n",
    "        MaxPooling1D(2),\n",
    "        Conv1D(filters=32, kernel_size=2, activation=LeakyReLU(), padding=\"same\"),\n",
    "        MaxPooling1D(2),\n",
    "        Conv1D(filters=16, kernel_size=2, activation=LeakyReLU(), padding=\"same\"),\n",
    "        MaxPooling1D(2),\n",
    "        LSTM(256, activation=LeakyReLU(), return_sequences=True),\n",
    "        LSTM(128, activation=LeakyReLU()),\n",
    "        Dense(64),\n",
    "        Dense(1),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "model.compile(optimizer=\"NAdam\", loss=\"logcosh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9c1248-e819-4998-afb4-743bac850c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_learning = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.2, patience=3, verbose=1, mode=\"auto\", min_delta=0.000025, cooldown=4, min_lr=0)\n",
    "eary_stopping = EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=7, verbose=1, mode=\"auto\")\n",
    "\n",
    "callbacks = [reduce_learning, eary_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6666fcc-13c4-4763-9e10-ec1928cfbfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val, Y_val),\n",
    "    verbose=1,\n",
    "    shuffle=False,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d159891e-af53-4e82-a3b2-e13fb733c07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "epochs_graph = np.arange(1, len(loss) + 1)\n",
    "\n",
    "x_ticks = np.arange(0, len(loss) + 1, 5)\n",
    "x_ticks = np.insert(x_ticks, 1, 1)\n",
    "x_ticks = np.insert(x_ticks, len(x_ticks), len(loss))\n",
    "\n",
    "plt.xticks(x_ticks)\n",
    "plt.xlim(1, len(loss))\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(epochs_graph, loss, \"red\", label=\"Training loss\")\n",
    "plt.plot(epochs_graph, val_loss, \"blue\", label=\"Validation loss\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04acc09-bca3-4286-8410-e05e27bdcf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"Models/UCID-Trend.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbbadcb-6620-4815-bb25-c5fe42b86a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = load_model(\"Models/UCID-Trend.h5\", custom_objects={\"LeakyReLU\": tensorflow.keras.layers.LeakyReLU})\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269a013e-10eb-46ca-bfeb-e9c788d821b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Predictions = model.predict(X_test)\n",
    "Predictions = scaler_te.inverse_transform(Predictions.reshape(-1, 1))\n",
    "\n",
    "s_Pred = C1_Merge_test[\"Trend\"].to_frame().copy()\n",
    "s_Pred = s_Pred.iloc[time_steps:]\n",
    "s_Pred[s_Pred.columns] = scaler_te.inverse_transform(s_Pred[s_Pred.columns])\n",
    "s_Pred.insert(len(s_Pred.columns), \"Predictions\", Predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e15616d-eaf8-4378-a6cf-bff21333fca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_accuracy(s_Pred.Predictions.ravel(), s_Pred.Trend.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfdbc6b-18ea-416c-857d-22eb467ce96a",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## 4.2: Raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ff7db4-d474-423c-a4d4-b44eb30b5f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "C1_Merge_train = CART_CL.loc[:, \"Labels\"].to_frame()\n",
    "df_UCID_resampled_train = df_UCID_resampled_CL.copy()\n",
    "df_UCID_resampled_train.Labels = C1_Merge_train.Labels\n",
    "df_UCID_resampled_train.dropna(inplace=True)\n",
    "C1_Merge_train = df_UCID_resampled_train.loc[df_UCID_resampled_train[\"Labels\"] == 1].copy()\n",
    "C1_Merge_train = C1_Merge_train.stack().reset_index()\n",
    "C1_Merge_train[\"Date\"] = C1_Merge_train[\"Date\"].astype(\"str\")\n",
    "C1_Merge_train[\"Time\"] = C1_Merge_train[\"Time\"].astype(\"str\")\n",
    "C1_Merge_train[\"DT\"] = C1_Merge_train[\"Date\"].str.cat(C1_Merge_train[\"Time\"], sep=\" \")\n",
    "\n",
    "cols_NA = [\n",
    "    \"Date\",\n",
    "    \"Time\",\n",
    "    \"Labels\",\n",
    "    \"Day_name\",\n",
    "    \"Month_name\"\n",
    "]\n",
    "\n",
    "C1_Merge_train.drop(cols_NA, axis=1, inplace=True)\n",
    "C1_Merge_train[\"DT\"] = pd.to_datetime(C1_Merge_train[\"DT\"])\n",
    "C1_Merge_train = C1_Merge_train.set_index(\"DT\")\n",
    "C1_Merge_train.dropna(inplace=True)\n",
    "C1_Merge_train = df_Merged.reindex(C1_Merge_train.index)\n",
    "C1_Merge_train = C1_Merge_train.dropna()\n",
    "C1_Merge_train[\"Holiday\"] = C1_Merge_train[\"Holiday\"].astype('int32')\n",
    "C1_Merge_train[\"Season\"] = C1_Merge_train[\"Season\"].astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c8f07d-5ede-451a-9753-1f930b6b259e",
   "metadata": {},
   "outputs": [],
   "source": [
    "C1_Merge_test = CART_TE.loc[:, \"Labels\"].to_frame()\n",
    "df_UCID_resampled_test = df_UCID_resampled_TE.copy()\n",
    "df_UCID_resampled_test.insert(len(df_UCID_resampled_test.columns), \"Labels\", C1_Merge_test.Labels)\n",
    "df_UCID_resampled_test.dropna(inplace=True)\n",
    "C1_Merge_test = df_UCID_resampled_test.loc[df_UCID_resampled_test[\"Labels\"] == 1].copy()\n",
    "C1_Merge_test = C1_Merge_test.stack().reset_index()\n",
    "C1_Merge_test[\"Date\"] = C1_Merge_test[\"Date\"].astype(\"str\")\n",
    "C1_Merge_test[\"Time\"] = C1_Merge_test[\"Time\"].astype(\"str\")\n",
    "C1_Merge_test[\"DT\"] = C1_Merge_test[\"Date\"].str.cat(C1_Merge_test[\"Time\"], sep=\" \")\n",
    "\n",
    "cols_NA = [\n",
    "    \"Date\",\n",
    "    \"Time\",\n",
    "    \"Labels\",\n",
    "]\n",
    "\n",
    "C1_Merge_test.drop(cols_NA, axis=1, inplace=True)\n",
    "C1_Merge_test[\"DT\"] = pd.to_datetime(C1_Merge_test[\"DT\"])\n",
    "C1_Merge_test = C1_Merge_test.set_index(\"DT\")\n",
    "C1_Merge_test.dropna(inplace=True)\n",
    "C1_Merge_test = df_Merged.reindex(C1_Merge_test.index)\n",
    "C1_Merge_test = C1_Merge_test.dropna()\n",
    "C1_Merge_test[\"Holiday\"] = C1_Merge_test[\"Holiday\"].astype('int32')\n",
    "C1_Merge_test[\"Season\"] = C1_Merge_test[\"Season\"].astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cb3da4-78ec-4cbb-abe3-83ccf4772051",
   "metadata": {},
   "outputs": [],
   "source": [
    "day = 24 * 60 * 60\n",
    "year = (365.2425) * day\n",
    "timestamp_train = C1_Merge_train.index.map(datetime.datetime.timestamp)\n",
    "timestamp_test = C1_Merge_test.index.map(datetime.datetime.timestamp)\n",
    "\n",
    "day_sin_tr = np.sin(timestamp_train * (2 * np.pi / day))\n",
    "day_cos_tr = np.cos(timestamp_train * (2 * np.pi / day))\n",
    "year_sin_tr = np.sin(timestamp_train * (2 * np.pi / year))\n",
    "year_cos_tr = np.cos(timestamp_train * (2 * np.pi / year))\n",
    "\n",
    "day_sin_te = np.sin(timestamp_test * (2 * np.pi / day))\n",
    "day_cos_te = np.cos(timestamp_test * (2 * np.pi / day))\n",
    "year_sin_te = np.sin(timestamp_test * (2 * np.pi / year))\n",
    "year_cos_te = np.cos(timestamp_test * (2 * np.pi / year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a05ae7-dc77-460d-9db2-1b5c26812cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_NA = [\"Year\", \"Month\", \"Holiday\", \"Day\", \"Hour\", \"Minute\", \"Weekday\", \"Season\"]\n",
    "\n",
    "C1_Merge_train.drop(cols_NA, axis=1, inplace=True)\n",
    "C1_Merge_test.drop(cols_NA, axis=1, inplace=True)\n",
    "\n",
    "C1_Merge_train.insert(0, \"Day_Sin\", day_sin_tr)\n",
    "C1_Merge_train.insert(0, \"Day_Cos\", day_cos_tr)\n",
    "C1_Merge_train.insert(0, \"Year_Sin\", year_sin_tr)\n",
    "C1_Merge_train.insert(0, \"Year_Cos\", year_cos_tr)\n",
    "\n",
    "C1_Merge_test.insert(0, \"Day_Sin\", day_sin_te)\n",
    "C1_Merge_test.insert(0, \"Day_Cos\", day_cos_te)\n",
    "C1_Merge_test.insert(0, \"Year_Sin\", year_sin_te)\n",
    "C1_Merge_test.insert(0, \"Year_Cos\", year_cos_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899595d9-93d8-4a0a-b9b3-91037e2ee421",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = (24 * 60) // 15\n",
    "stl_decompose_result = STL(C1_Merge_train.Global_active_power, period=freq).fit()\n",
    "C1_Merge_train.insert(len(C1_Merge_train.columns), \"Trend\", stl_decompose_result.trend.values)\n",
    "\n",
    "stl_decompose_result = STL(C1_Merge_test.Global_active_power, period=freq).fit()\n",
    "C1_Merge_test.insert(len(C1_Merge_test.columns), \"Trend\", stl_decompose_result.trend.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82137739-353f-45b0-a3af-45b136d38ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "C1_GAP_train = C1_Merge_train.Global_active_power.copy().to_frame()\n",
    "C1_Merge_train.Global_active_power = savgol_filter(C1_Merge_train.Global_active_power, 5, 3)\n",
    "\n",
    "C1_GAP_test = C1_Merge_test.Global_active_power.copy().to_frame()\n",
    "C1_Merge_test.Global_active_power = savgol_filter(C1_Merge_test.Global_active_power, 5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439c67d4-7407-411f-905d-9bda935c18aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_tr = MaxAbsScaler()\n",
    "scaler_te = MaxAbsScaler()\n",
    "\n",
    "C1_Merge_train[C1_Merge_train.columns] = scaler_tr.fit_transform(C1_Merge_train[C1_Merge_train.columns])\n",
    "scaler_tr.max_abs_, scaler_tr.scale_ = scaler_tr.max_abs_[len(C1_Merge_train.columns) - 2], scaler_tr.scale_[len(C1_Merge_train.columns) - 2]\n",
    "\n",
    "C1_Merge_test[C1_Merge_test.columns] = scaler_te.fit_transform(C1_Merge_test[C1_Merge_test.columns])\n",
    "scaler_te.max_abs_, scaler_te.scale_ = scaler_te.max_abs_[len(C1_Merge_test.columns) - 2], scaler_te.scale_[len(C1_Merge_test.columns) - 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc52b07-bbb2-40b7-ba45-ee2c13b4dad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = C1_Merge_train.shape[1]\n",
    "time_steps = 24\n",
    "Split = len(C1_Merge_train)\n",
    "\n",
    "s_Train = C1_Merge_train.iloc[0 : int(Split * 0.8)]\n",
    "s_Val = C1_Merge_train.iloc[int(Split * 0.8) :]\n",
    "o_Test = C1_GAP_test.iloc[time_steps :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1448133e-f7d1-457b-8b6d-50da2a63ee72",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = to_Supervised(s_Train, s_Train.Global_active_power, time_steps)\n",
    "X_val, Y_val = to_Supervised(s_Val, s_Val.Global_active_power, time_steps)\n",
    "X_test, Y_test = to_Supervised(C1_Merge_test, C1_Merge_test.Global_active_power, time_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be78116-83c9-47ee-9081-95ae071f19b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(\n",
    "    [\n",
    "        Conv1D(filters=64, kernel_size=3, activation=LeakyReLU(), padding=\"same\", input_shape=(time_steps, n_features)),\n",
    "        MaxPooling1D(2),\n",
    "        Conv1D(filters=32, kernel_size=2, activation=LeakyReLU(), padding=\"same\"),\n",
    "        MaxPooling1D(2),\n",
    "        Conv1D(filters=16, kernel_size=2, activation=LeakyReLU(), padding=\"same\"),\n",
    "        MaxPooling1D(2),\n",
    "        LSTM(256, activation=LeakyReLU(), return_sequences=True),\n",
    "        LSTM(128, activation=LeakyReLU()),\n",
    "        Dense(64),\n",
    "        Dense(1),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "model.compile(optimizer=\"NAdam\", loss=\"logcosh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957e7c34-0fa0-41be-9f01-54e23aeba64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_learning = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.2, patience=3, verbose=1, mode=\"auto\", min_delta=0.000025, cooldown=4, min_lr=0)\n",
    "eary_stopping = EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=7, verbose=1, mode=\"auto\")\n",
    "\n",
    "callbacks = [reduce_learning, eary_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3b86b3-fbe5-4db1-b3a7-d3af86a53f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val, Y_val),\n",
    "    verbose=1,\n",
    "    shuffle=False,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f929add-1aa6-425d-a250-636a40c753b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "epochs_graph = np.arange(1, len(loss) + 1)\n",
    "\n",
    "x_ticks = np.arange(0, len(loss) + 1, 5)\n",
    "x_ticks = np.insert(x_ticks, 1, 1)\n",
    "x_ticks = np.insert(x_ticks, len(x_ticks), len(loss))\n",
    "\n",
    "plt.xticks(x_ticks)\n",
    "plt.xlim(1, len(loss))\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(epochs_graph, loss, \"red\", label=\"Training loss\")\n",
    "plt.plot(epochs_graph, val_loss, \"blue\", label=\"Validation loss\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3dfff9-8b55-41b1-a1e0-d89bb228ac1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"Models/UCID-Raw.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d754f0-1c16-4af4-b949-76f731b0e692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = load_model(\"Models/UCID-Raw.h5\", custom_objects={\"LeakyReLU\": tensorflow.keras.layers.LeakyReLU})\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc3ea1c-9d63-4737-9e4f-1af3ddfe8ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Predictions = model.predict(X_test)\n",
    "Predictions = scaler_te.inverse_transform(Predictions.reshape(-1, 1))\n",
    "\n",
    "s_Pred = C1_Merge_test[\"Global_active_power\"].to_frame().copy()\n",
    "s_Pred = s_Pred.iloc[time_steps:]\n",
    "s_Pred[s_Pred.columns] = scaler_te.inverse_transform(s_Pred[s_Pred.columns])\n",
    "s_Pred.insert(len(s_Pred.columns), \"Predictions\", Predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ae31c7-5347-47c1-9b9c-518533e1eca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_accuracy(s_Pred.Predictions.ravel(), s_Pred.Global_active_power.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae1035c-e0ab-489b-b539-6586cae0e4ed",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## 4.3: 12 steps ahead - Trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ce7d99-9826-489c-816a-d76883daa487",
   "metadata": {},
   "outputs": [],
   "source": [
    "C1_Merge_train = CART_CL.loc[:, \"Labels\"].to_frame()\n",
    "df_UCID_resampled_train = df_UCID_resampled_CL.copy()\n",
    "df_UCID_resampled_train.Labels = C1_Merge_train.Labels\n",
    "df_UCID_resampled_train.dropna(inplace=True)\n",
    "C1_Merge_train = df_UCID_resampled_train.loc[df_UCID_resampled_train[\"Labels\"] == 1].copy()\n",
    "C1_Merge_train = C1_Merge_train.stack().reset_index()\n",
    "C1_Merge_train[\"Date\"] = C1_Merge_train[\"Date\"].astype(\"str\")\n",
    "C1_Merge_train[\"Time\"] = C1_Merge_train[\"Time\"].astype(\"str\")\n",
    "C1_Merge_train[\"DT\"] = C1_Merge_train[\"Date\"].str.cat(C1_Merge_train[\"Time\"], sep=\" \")\n",
    "\n",
    "cols_NA = [\n",
    "    \"Date\",\n",
    "    \"Time\",\n",
    "    \"Labels\",\n",
    "    \"Day_name\",\n",
    "    \"Month_name\"\n",
    "]\n",
    "\n",
    "C1_Merge_train.drop(cols_NA, axis=1, inplace=True)\n",
    "C1_Merge_train[\"DT\"] = pd.to_datetime(C1_Merge_train[\"DT\"])\n",
    "C1_Merge_train = C1_Merge_train.set_index(\"DT\")\n",
    "C1_Merge_train.dropna(inplace=True)\n",
    "C1_Merge_train = df_Merged.reindex(C1_Merge_train.index)\n",
    "C1_Merge_train = C1_Merge_train.dropna()\n",
    "C1_Merge_train[\"Holiday\"] = C1_Merge_train[\"Holiday\"].astype('int32')\n",
    "C1_Merge_train[\"Season\"] = C1_Merge_train[\"Season\"].astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d08d809-1b7a-4b78-bda6-9c5bdad9e313",
   "metadata": {},
   "outputs": [],
   "source": [
    "C1_Merge_test = CART_TE.loc[:, \"Labels\"].to_frame()\n",
    "df_UCID_resampled_test = df_UCID_resampled_TE.copy()\n",
    "df_UCID_resampled_test.insert(len(df_UCID_resampled_test.columns), \"Labels\", C1_Merge_test.Labels)\n",
    "df_UCID_resampled_test.dropna(inplace=True)\n",
    "C1_Merge_test = df_UCID_resampled_test.loc[df_UCID_resampled_test[\"Labels\"] == 1].copy()\n",
    "C1_Merge_test = C1_Merge_test.stack().reset_index()\n",
    "C1_Merge_test[\"Date\"] = C1_Merge_test[\"Date\"].astype(\"str\")\n",
    "C1_Merge_test[\"Time\"] = C1_Merge_test[\"Time\"].astype(\"str\")\n",
    "C1_Merge_test[\"DT\"] = C1_Merge_test[\"Date\"].str.cat(C1_Merge_test[\"Time\"], sep=\" \")\n",
    "\n",
    "cols_NA = [\n",
    "    \"Date\",\n",
    "    \"Time\",\n",
    "    \"Labels\",\n",
    "]\n",
    "\n",
    "C1_Merge_test.drop(cols_NA, axis=1, inplace=True)\n",
    "C1_Merge_test[\"DT\"] = pd.to_datetime(C1_Merge_test[\"DT\"])\n",
    "C1_Merge_test = C1_Merge_test.set_index(\"DT\")\n",
    "C1_Merge_test.dropna(inplace=True)\n",
    "C1_Merge_test = df_Merged.reindex(C1_Merge_test.index)\n",
    "C1_Merge_test = C1_Merge_test.dropna()\n",
    "C1_Merge_test[\"Holiday\"] = C1_Merge_test[\"Holiday\"].astype('int32')\n",
    "C1_Merge_test[\"Season\"] = C1_Merge_test[\"Season\"].astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9932dd1-aa65-4cf1-aff0-92fd7e2402a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "day = 24 * 60 * 60\n",
    "year = (365.2425) * day\n",
    "timestamp_train = C1_Merge_train.index.map(datetime.datetime.timestamp)\n",
    "timestamp_test = C1_Merge_test.index.map(datetime.datetime.timestamp)\n",
    "\n",
    "day_sin_tr = np.sin(timestamp_train * (2 * np.pi / day))\n",
    "day_cos_tr = np.cos(timestamp_train * (2 * np.pi / day))\n",
    "year_sin_tr = np.sin(timestamp_train * (2 * np.pi / year))\n",
    "year_cos_tr = np.cos(timestamp_train * (2 * np.pi / year))\n",
    "\n",
    "day_sin_te = np.sin(timestamp_test * (2 * np.pi / day))\n",
    "day_cos_te = np.cos(timestamp_test * (2 * np.pi / day))\n",
    "year_sin_te = np.sin(timestamp_test * (2 * np.pi / year))\n",
    "year_cos_te = np.cos(timestamp_test * (2 * np.pi / year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81267259-2cc6-4a54-9b6f-10440770ffe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_NA = [\"Year\", \"Month\", \"Holiday\", \"Day\", \"Hour\", \"Minute\", \"Weekday\", \"Season\"]\n",
    "\n",
    "C1_Merge_train.drop(cols_NA, axis=1, inplace=True)\n",
    "C1_Merge_test.drop(cols_NA, axis=1, inplace=True)\n",
    "\n",
    "C1_Merge_train.insert(0, \"Day_Sin\", day_sin_tr)\n",
    "C1_Merge_train.insert(0, \"Day_Cos\", day_cos_tr)\n",
    "C1_Merge_train.insert(0, \"Year_Sin\", year_sin_tr)\n",
    "C1_Merge_train.insert(0, \"Year_Cos\", year_cos_tr)\n",
    "\n",
    "C1_Merge_test.insert(0, \"Day_Sin\", day_sin_te)\n",
    "C1_Merge_test.insert(0, \"Day_Cos\", day_cos_te)\n",
    "C1_Merge_test.insert(0, \"Year_Sin\", year_sin_te)\n",
    "C1_Merge_test.insert(0, \"Year_Cos\", year_cos_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbc8f80-1cae-4e97-90ee-7dc5d900ad30",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = (24 * 60) // 15\n",
    "stl_decompose_result = STL(C1_Merge_train.Global_active_power, period=freq).fit()\n",
    "C1_Merge_train.insert(len(C1_Merge_train.columns), \"Trend\", stl_decompose_result.trend.values)\n",
    "\n",
    "stl_decompose_result = STL(C1_Merge_test.Global_active_power, period=freq).fit()\n",
    "C1_Merge_test.insert(len(C1_Merge_test.columns), \"Trend\", stl_decompose_result.trend.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d0a5b2-9e3e-490d-8a9d-88e41426119f",
   "metadata": {},
   "outputs": [],
   "source": [
    "C1_GAP_train = C1_Merge_train.Global_active_power.copy().to_frame()\n",
    "C1_Merge_train.Global_active_power = savgol_filter(C1_Merge_train.Global_active_power, 5, 3)\n",
    "\n",
    "C1_GAP_test = C1_Merge_test.Global_active_power.copy().to_frame()\n",
    "C1_Merge_test.Global_active_power = savgol_filter(C1_Merge_test.Global_active_power, 5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a877185e-bb3b-4c85-821b-872e48fe2918",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_tr = MaxAbsScaler()\n",
    "scaler_te = MaxAbsScaler()\n",
    "\n",
    "C1_Merge_train[C1_Merge_train.columns] = scaler_tr.fit_transform(C1_Merge_train[C1_Merge_train.columns])\n",
    "scaler_tr.max_abs_, scaler_tr.scale_ = scaler_tr.max_abs_[len(C1_Merge_train.columns) - 1], scaler_tr.scale_[len(C1_Merge_train.columns) - 1]\n",
    "\n",
    "C1_Merge_test[C1_Merge_test.columns] = scaler_te.fit_transform(C1_Merge_test[C1_Merge_test.columns])\n",
    "scaler_te.max_abs_, scaler_te.scale_ = scaler_te.max_abs_[len(C1_Merge_test.columns) - 1], scaler_te.scale_[len(C1_Merge_test.columns) - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838e0e6a-d5b5-4f70-9b03-5b2a10b5079f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = C1_Merge_train.shape[1]\n",
    "time_steps = 24\n",
    "Split = len(C1_Merge_train)\n",
    "\n",
    "s_Train = C1_Merge_train.iloc[0 : int(Split * 0.8)]\n",
    "s_Val = C1_Merge_train.iloc[int(Split * 0.8) :]\n",
    "o_Test = C1_GAP_test.iloc[time_steps :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925f8f7a-dce2-4da0-b9d6-2ba9b44e3c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = to_Supervised_ms(s_Train.values, lag=24, n_ahead=12, target_index=len(s_Train.columns) - 1)\n",
    "X_val, Y_val = to_Supervised_ms(s_Val.values, lag=24, n_ahead=12, target_index=len(s_Val.columns) - 1)\n",
    "X_test, Y_test = to_Supervised_ms(C1_Merge_test.values, lag=24, n_ahead=12, target_index=len(C1_Merge_test.columns) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86cc295-19a2-4b60-8550-a946574869f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(\n",
    "    [\n",
    "        Conv1D(filters=64, kernel_size=3, activation=LeakyReLU(), padding=\"same\", input_shape=(time_steps, n_features)),\n",
    "        MaxPooling1D(2),\n",
    "        Conv1D(filters=32, kernel_size=2, activation=LeakyReLU(), padding=\"same\"),\n",
    "        MaxPooling1D(2),\n",
    "        Conv1D(filters=16, kernel_size=2, activation=LeakyReLU(), padding=\"same\"),\n",
    "        MaxPooling1D(2),\n",
    "        LSTM(256, activation=LeakyReLU(), return_sequences=True),\n",
    "        LSTM(128, activation=LeakyReLU()),\n",
    "        Dense(64),\n",
    "        Dense(12),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "model.compile(optimizer=\"NAdam\", loss=\"logcosh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c073eb-e10e-4e93-a222-099954c54b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_learning = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.2, patience=3, verbose=1, mode=\"auto\", min_delta=0.000025, cooldown=4, min_lr=0)\n",
    "eary_stopping = EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=7, verbose=1, mode=\"auto\")\n",
    "\n",
    "callbacks = [reduce_learning, eary_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5109b517-04dc-4eab-89e0-176ad6a0656b",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val, Y_val),\n",
    "    verbose=1,\n",
    "    shuffle=False,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b4da80-6c07-4d16-954f-252f5f3a4e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "epochs_graph = np.arange(1, len(loss) + 1)\n",
    "\n",
    "x_ticks = np.arange(0, len(loss) + 1, 5)\n",
    "x_ticks = np.insert(x_ticks, 1, 1)\n",
    "x_ticks = np.insert(x_ticks, len(x_ticks), len(loss))\n",
    "\n",
    "plt.xticks(x_ticks)\n",
    "plt.xlim(1, len(loss))\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(epochs_graph, loss, \"red\", label=\"Training loss\")\n",
    "plt.plot(epochs_graph, val_loss, \"blue\", label=\"Validation loss\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d40f26-5458-4cab-8f5e-2889d3355f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"Models/UCID-Raw-12-Steps-Trend.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f2de2d-b54c-4a84-a987-76053d400cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = load_model(\"Models/UCID-Raw-12-Steps.h5\", custom_objects={\"LeakyReLU\": tensorflow.keras.layers.LeakyReLU})\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4386db-7311-4016-8350-69e6092e1fbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Predictions = model.predict(X_test)\n",
    "Predictions = scaler_te.inverse_transform(Predictions.reshape(-1, 1))\n",
    "Y_test = scaler_te.inverse_transform(Y_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac8b0ca-efe9-4e3b-96c3-4eef8119de6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_accuracy(Predictions, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5ae49b-63e3-4d56-b85b-ed21390d9ca7",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## 4.3: 12 steps ahead - Raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624627b8-ffbf-4770-a4be-a3dc3908467b",
   "metadata": {},
   "outputs": [],
   "source": [
    "C1_Merge_train = CART_CL.loc[:, \"Labels\"].to_frame()\n",
    "df_UCID_resampled_train = df_UCID_resampled_CL.copy()\n",
    "df_UCID_resampled_train.Labels = C1_Merge_train.Labels\n",
    "df_UCID_resampled_train.dropna(inplace=True)\n",
    "C1_Merge_train = df_UCID_resampled_train.loc[df_UCID_resampled_train[\"Labels\"] == 1].copy()\n",
    "C1_Merge_train = C1_Merge_train.stack().reset_index()\n",
    "C1_Merge_train[\"Date\"] = C1_Merge_train[\"Date\"].astype(\"str\")\n",
    "C1_Merge_train[\"Time\"] = C1_Merge_train[\"Time\"].astype(\"str\")\n",
    "C1_Merge_train[\"DT\"] = C1_Merge_train[\"Date\"].str.cat(C1_Merge_train[\"Time\"], sep=\" \")\n",
    "\n",
    "cols_NA = [\n",
    "    \"Date\",\n",
    "    \"Time\",\n",
    "    \"Labels\",\n",
    "    \"Day_name\",\n",
    "    \"Month_name\"\n",
    "]\n",
    "\n",
    "C1_Merge_train.drop(cols_NA, axis=1, inplace=True)\n",
    "C1_Merge_train[\"DT\"] = pd.to_datetime(C1_Merge_train[\"DT\"])\n",
    "C1_Merge_train = C1_Merge_train.set_index(\"DT\")\n",
    "C1_Merge_train.dropna(inplace=True)\n",
    "C1_Merge_train = df_Merged.reindex(C1_Merge_train.index)\n",
    "C1_Merge_train = C1_Merge_train.dropna()\n",
    "C1_Merge_train[\"Holiday\"] = C1_Merge_train[\"Holiday\"].astype('int32')\n",
    "C1_Merge_train[\"Season\"] = C1_Merge_train[\"Season\"].astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9706d3-4f96-4ee1-9be5-720f0313e085",
   "metadata": {},
   "outputs": [],
   "source": [
    "C1_Merge_test = CART_TE.loc[:, \"Labels\"].to_frame()\n",
    "df_UCID_resampled_test = df_UCID_resampled_TE.copy()\n",
    "df_UCID_resampled_test.insert(len(df_UCID_resampled_test.columns), \"Labels\", C1_Merge_test.Labels)\n",
    "df_UCID_resampled_test.dropna(inplace=True)\n",
    "C1_Merge_test = df_UCID_resampled_test.loc[df_UCID_resampled_test[\"Labels\"] == 1].copy()\n",
    "C1_Merge_test = C1_Merge_test.stack().reset_index()\n",
    "C1_Merge_test[\"Date\"] = C1_Merge_test[\"Date\"].astype(\"str\")\n",
    "C1_Merge_test[\"Time\"] = C1_Merge_test[\"Time\"].astype(\"str\")\n",
    "C1_Merge_test[\"DT\"] = C1_Merge_test[\"Date\"].str.cat(C1_Merge_test[\"Time\"], sep=\" \")\n",
    "\n",
    "cols_NA = [\n",
    "    \"Date\",\n",
    "    \"Time\",\n",
    "    \"Labels\",\n",
    "]\n",
    "\n",
    "C1_Merge_test.drop(cols_NA, axis=1, inplace=True)\n",
    "C1_Merge_test[\"DT\"] = pd.to_datetime(C1_Merge_test[\"DT\"])\n",
    "C1_Merge_test = C1_Merge_test.set_index(\"DT\")\n",
    "C1_Merge_test.dropna(inplace=True)\n",
    "C1_Merge_test = df_Merged.reindex(C1_Merge_test.index)\n",
    "C1_Merge_test = C1_Merge_test.dropna()\n",
    "C1_Merge_test[\"Holiday\"] = C1_Merge_test[\"Holiday\"].astype('int32')\n",
    "C1_Merge_test[\"Season\"] = C1_Merge_test[\"Season\"].astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd94abc-464a-4d13-83b6-5087da6411d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "day = 24 * 60 * 60\n",
    "year = (365.2425) * day\n",
    "timestamp_train = C1_Merge_train.index.map(datetime.datetime.timestamp)\n",
    "timestamp_test = C1_Merge_test.index.map(datetime.datetime.timestamp)\n",
    "\n",
    "day_sin_tr = np.sin(timestamp_train * (2 * np.pi / day))\n",
    "day_cos_tr = np.cos(timestamp_train * (2 * np.pi / day))\n",
    "year_sin_tr = np.sin(timestamp_train * (2 * np.pi / year))\n",
    "year_cos_tr = np.cos(timestamp_train * (2 * np.pi / year))\n",
    "\n",
    "day_sin_te = np.sin(timestamp_test * (2 * np.pi / day))\n",
    "day_cos_te = np.cos(timestamp_test * (2 * np.pi / day))\n",
    "year_sin_te = np.sin(timestamp_test * (2 * np.pi / year))\n",
    "year_cos_te = np.cos(timestamp_test * (2 * np.pi / year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425819b3-76b1-455f-b4b0-17d5a42ccc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_NA = [\"Year\", \"Month\", \"Holiday\", \"Day\", \"Hour\", \"Minute\", \"Weekday\", \"Season\"]\n",
    "\n",
    "C1_Merge_train.drop(cols_NA, axis=1, inplace=True)\n",
    "C1_Merge_test.drop(cols_NA, axis=1, inplace=True)\n",
    "\n",
    "C1_Merge_train.insert(0, \"Day_Sin\", day_sin_tr)\n",
    "C1_Merge_train.insert(0, \"Day_Cos\", day_cos_tr)\n",
    "C1_Merge_train.insert(0, \"Year_Sin\", year_sin_tr)\n",
    "C1_Merge_train.insert(0, \"Year_Cos\", year_cos_tr)\n",
    "\n",
    "C1_Merge_test.insert(0, \"Day_Sin\", day_sin_te)\n",
    "C1_Merge_test.insert(0, \"Day_Cos\", day_cos_te)\n",
    "C1_Merge_test.insert(0, \"Year_Sin\", year_sin_te)\n",
    "C1_Merge_test.insert(0, \"Year_Cos\", year_cos_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec678047-0535-42e2-9239-7104c16758f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = (24 * 60) // 15\n",
    "stl_decompose_result = STL(C1_Merge_train.Global_active_power, period=freq).fit()\n",
    "C1_Merge_train.insert(len(C1_Merge_train.columns), \"Trend\", stl_decompose_result.trend.values)\n",
    "\n",
    "stl_decompose_result = STL(C1_Merge_test.Global_active_power, period=freq).fit()\n",
    "C1_Merge_test.insert(len(C1_Merge_test.columns), \"Trend\", stl_decompose_result.trend.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c451760d-d1f1-463c-8b74-1b5fd0f4e703",
   "metadata": {},
   "outputs": [],
   "source": [
    "C1_GAP_train = C1_Merge_train.Global_active_power.copy().to_frame()\n",
    "C1_Merge_train.Global_active_power = savgol_filter(C1_Merge_train.Global_active_power, 5, 3)\n",
    "\n",
    "C1_GAP_test = C1_Merge_test.Global_active_power.copy().to_frame()\n",
    "C1_Merge_test.Global_active_power = savgol_filter(C1_Merge_test.Global_active_power, 5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5e434f-776f-4012-b6d9-b921834cef20",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_tr = MaxAbsScaler()\n",
    "scaler_te = MaxAbsScaler()\n",
    "\n",
    "C1_Merge_train[C1_Merge_train.columns] = scaler_tr.fit_transform(C1_Merge_train[C1_Merge_train.columns])\n",
    "scaler_tr.max_abs_, scaler_tr.scale_ = scaler_tr.max_abs_[len(C1_Merge_train.columns) - 2], scaler_tr.scale_[len(C1_Merge_train.columns) - 2]\n",
    "\n",
    "C1_Merge_test[C1_Merge_test.columns] = scaler_te.fit_transform(C1_Merge_test[C1_Merge_test.columns])\n",
    "scaler_te.max_abs_, scaler_te.scale_ = scaler_te.max_abs_[len(C1_Merge_test.columns) - 2], scaler_te.scale_[len(C1_Merge_test.columns) - 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b37d6a-ccb1-4a36-a845-c04814ae5adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = C1_Merge_train.shape[1]\n",
    "time_steps = 24\n",
    "Split = len(C1_Merge_train)\n",
    "\n",
    "s_Train = C1_Merge_train.iloc[0 : int(Split * 0.8)]\n",
    "s_Val = C1_Merge_train.iloc[int(Split * 0.8) :]\n",
    "o_Test = C1_GAP_test.iloc[time_steps :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b46da1-5ccb-44a7-bf71-e2fe6f928422",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = to_Supervised_ms(s_Train.values, lag=24, n_ahead=12, target_index=len(s_Train.columns) - 2)\n",
    "X_val, Y_val = to_Supervised_ms(s_Val.values, lag=24, n_ahead=12, target_index=len(s_Val.columns) - 2)\n",
    "X_test, Y_test = to_Supervised_ms(C1_Merge_test.values, lag=24, n_ahead=12, target_index=len(C1_Merge_test.columns) - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b3f1a2-d643-41b3-b131-f17e8f98b01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(\n",
    "    [\n",
    "        Conv1D(filters=64, kernel_size=3, activation=LeakyReLU(), padding=\"same\", input_shape=(time_steps, n_features)),\n",
    "        MaxPooling1D(2),\n",
    "        Conv1D(filters=32, kernel_size=2, activation=LeakyReLU(), padding=\"same\"),\n",
    "        MaxPooling1D(2),\n",
    "        Conv1D(filters=16, kernel_size=2, activation=LeakyReLU(), padding=\"same\"),\n",
    "        MaxPooling1D(2),\n",
    "        LSTM(256, activation=LeakyReLU(), return_sequences=True),\n",
    "        LSTM(128, activation=LeakyReLU()),\n",
    "        Dense(64),\n",
    "        Dense(12),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "model.compile(optimizer=\"NAdam\", loss=\"logcosh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3721d2d7-abde-474d-9e54-fe7c18f391c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_learning = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.2, patience=3, verbose=1, mode=\"auto\", min_delta=0.000025, cooldown=4, min_lr=0)\n",
    "eary_stopping = EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=7, verbose=1, mode=\"auto\")\n",
    "\n",
    "callbacks = [reduce_learning, eary_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3102c8da-2834-4a14-8014-f91bcf83cdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val, Y_val),\n",
    "    verbose=1,\n",
    "    shuffle=False,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb2d7cc-5b59-4e11-8ebb-51d5e00c6f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "epochs_graph = np.arange(1, len(loss) + 1)\n",
    "\n",
    "x_ticks = np.arange(0, len(loss) + 1, 5)\n",
    "x_ticks = np.insert(x_ticks, 1, 1)\n",
    "x_ticks = np.insert(x_ticks, len(x_ticks), len(loss))\n",
    "\n",
    "plt.xticks(x_ticks)\n",
    "plt.xlim(1, len(loss))\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(epochs_graph, loss, \"red\", label=\"Training loss\")\n",
    "plt.plot(epochs_graph, val_loss, \"blue\", label=\"Validation loss\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857a76be-1c94-4b65-a11a-67fc3e13369b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"Models/UCID-Raw-12-Steps-Trend.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04227df1-69ae-4fd5-87ff-210b914b6a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = load_model(\"Models/UCID-Raw-12-Steps.h5\", custom_objects={\"LeakyReLU\": tensorflow.keras.layers.LeakyReLU})\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef62b6a-f386-43da-bd8e-b717ce24045b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Predictions = model.predict(X_test)\n",
    "Predictions = scaler_te.inverse_transform(Predictions.reshape(-1, 1))\n",
    "Y_test = scaler_te.inverse_transform(Y_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ff15e7-d127-4240-9acf-dba68664df43",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_accuracy(Predictions, Y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
