{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "partial-trail",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "# Part 1: Setup <a id=\"Part1_REFIT\"> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "small-cleaning",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## 1.1: Initial setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solar-strain",
   "metadata": {},
   "source": [
    "**Step 1:** Import the relevant packages and set Seaborn/Matplotlib hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rental-portal",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import graphviz\n",
    "import hdbscan\n",
    "import holidays\n",
    "import os\n",
    "import sklearn\n",
    "import tensorflow\n",
    "import umap\n",
    "\n",
    "import matplotlib.dates as md\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy import stats\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFECV, mutual_info_regression\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import MaxAbsScaler, MinMaxScaler, StandardScaler\n",
    "from sklearn import metrics, tree\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.seasonal import STL, seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller, grangercausalitytests\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import LSTM, Conv1D, ConvLSTM2D, Dense, Flatten, LeakyReLU, MaxPooling1D, MaxPooling2D, RepeatVector, TimeDistributed\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tqdm import notebook\n",
    "\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "sns.set(style=\"whitegrid\", palette=\"muted\")\n",
    "\n",
    "plt.rcParams[\"font.size\"] = 24\n",
    "plt.rcParams[\"axes.labelsize\"] = 26\n",
    "plt.rcParams[\"axes.titlesize\"] = 26\n",
    "plt.rcParams[\"figure.figsize\"] = 16, 10\n",
    "plt.rcParams[\"figure.dpi\"] = 300\n",
    "plt.rcParams[\"xtick.labelsize\"] = 22\n",
    "plt.rcParams[\"ytick.labelsize\"] = 22\n",
    "plt.rcParams[\"legend.fontsize\"] = 22\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "pd.options.display.float_format = \"{:.2f}\".format\n",
    "\n",
    "RANDOM_SEED=3141589\n",
    "\n",
    "tensorflow.random.set_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equivalent-breakdown",
   "metadata": {},
   "source": [
    "**Step 2:** Define the location of our data as well as the relevant columns that we would like to include."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "warming-objective",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "house_number = 12\n",
    "\n",
    "cols_REFIT = [\n",
    "    \"Time\",\n",
    "    \"Aggregate\",\n",
    "    \"Appliance1\",\n",
    "    \"Appliance2\",\n",
    "    \"Appliance3\",\n",
    "    \"Appliance4\",\n",
    "    \"Appliance5\",\n",
    "    \"Appliance6\",\n",
    "    \"Appliance7\",\n",
    "    \"Appliance8\",\n",
    "    \"Appliance9\",\n",
    "]\n",
    "\n",
    "data_directory_REFIT = os.path.join(\"Data\", \"REFIT\")\n",
    "data_directory_Solcast = os.path.join(\"Data\", \"Solcast_REFIT\")\n",
    "\n",
    "house = f\"CLEAN_House{house_number}.csv\"\n",
    "solcast_15 = \"Solcast_REFIT_15.csv\"\n",
    "\n",
    "file_destination_REFIT = os.path.join(data_directory_REFIT, house)\n",
    "file_destination_Solcast = os.path.join(data_directory_Solcast, solcast_15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependent-walter",
   "metadata": {},
   "source": [
    "**Step 3:** Read in the data and save it to a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprised-thanks",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_REFIT = pd.read_csv(file_destination_REFIT, index_col=0, parse_dates=True, usecols=cols_REFIT)\n",
    "df_Solcast = pd.read_csv(file_destination_Solcast, index_col=0, parse_dates=True)\n",
    "\n",
    "df_Solcast.index = df_Solcast.index.rename(\"Time\")\n",
    "df_Solcast.index = pd.to_datetime(df_Solcast.index).tz_localize(None)\n",
    "\n",
    "cols_REFIT.remove(\"Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b96f5c6-0c09-476e-b3b0-6040df3cfb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_REFIT = df_REFIT / 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closed-joseph",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## 1.2: Scale the data in the dataframe(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excellent-finnish",
   "metadata": {},
   "source": [
    "**Step 1.1:** Scale the data in a range between 0 and 1 (optional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personal-adoption",
   "metadata": {},
   "outputs": [],
   "source": [
    "# minmax_REFIT = MinMaxScaler()\n",
    "# minmax_Solcast = MinMaxScaler()\n",
    "\n",
    "# df_REFIT[cols_REFIT] = minmax_REFIT.fit_transform(df_REFIT[cols_REFIT])\n",
    "# df_Solcast[cols_Solcast] = minmax_Solcast.fit_transform(df_Solcast[cols_Solcast])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "municipal-politics",
   "metadata": {},
   "source": [
    "**Step 1.2:** Standardize the data by removing the mean and scaling to unit variance (optional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "found-illinois",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardscale_REFIT = StandardScaler()\n",
    "# standardscale_Solcast = StandardScaler()\n",
    "\n",
    "# df_REFIT[cols_REFIT] = standardscale_REFIT.fit_transform(df_REFIT[cols_REFIT])\n",
    "# df_Solcast[cols_Solcast] = standardscale_Solcast.fit_transform(df_Solcast[cols_Solcast])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-poster",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## 1.3: Merge the dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manufactured-health",
   "metadata": {},
   "source": [
    "**Step 1:** Create a copy of our REFIT dataframe that is resampled into a resolution of 15 minutes and drop any days that contain an incomplete number of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complicated-haven",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_REFIT_resampled = df_REFIT.resample(\"15min\").mean()\n",
    "df_REFIT_resampled = df_REFIT_resampled.dropna()\n",
    "\n",
    "mask = df_REFIT_resampled.groupby(df_REFIT_resampled.index.date).size()\n",
    "mask = mask[mask < 96].index.to_list()\n",
    "\n",
    "df_REFIT_resampled = df_REFIT_resampled[~df_REFIT_resampled.index.floor(\"D\").isin(mask)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-design",
   "metadata": {},
   "source": [
    "**Step 2:** Create a third dataframe that is the result of merging the Solcast dataframe with the REFIT dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optical-prerequisite",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Merged = pd.merge(left=df_Solcast, left_on=df_Solcast.index, right=df_REFIT_resampled, right_on=df_REFIT_resampled.index)\n",
    "\n",
    "cols_Merged = [\n",
    "    \"PeriodStart\",\n",
    "    \"Period\",\n",
    "    \"Appliance1\",\n",
    "    \"Appliance2\",\n",
    "    \"Appliance3\",\n",
    "    \"Appliance4\",\n",
    "    \"Appliance5\",\n",
    "    \"Appliance6\",\n",
    "    \"Appliance7\",\n",
    "    \"Appliance8\",\n",
    "    \"Appliance9\",\n",
    "]\n",
    "\n",
    "df_Merged.drop(cols_Merged, axis=1, inplace=True)\n",
    "df_Merged.rename(columns={\"key_0\": \"Time\"}, inplace=True)\n",
    "df_Merged = df_Merged.set_index(\"Time\")\n",
    "df_Merged.index = pd.to_datetime(df_Merged.index)\n",
    "df_Merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prostate-subscription",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## 1.4: Append temporal features to our merged dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinated-appointment",
   "metadata": {},
   "source": [
    "**Step 1:** Append public holidays to our merged dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disabled-application",
   "metadata": {},
   "outputs": [],
   "source": [
    "UK_holidays = holidays.UnitedKingdom()\n",
    "df_Merged.insert(0, \"Holiday\", [1 if str(val).split()[0] in UK_holidays else 0 for val in df_Merged.index.date])\n",
    "df_Merged[\"Holiday\"] = df_Merged[\"Holiday\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "higher-orlando",
   "metadata": {},
   "source": [
    "**Step 2:** Define day of the year ranges for each of the seasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "differential-tuition",
   "metadata": {},
   "outputs": [],
   "source": [
    "spring = range(79, 172)\n",
    "summer = range(172, 266)\n",
    "fall = range(266, 355)\n",
    "\n",
    "def season(doy):\n",
    "    if doy in spring:\n",
    "        return \"0\"\n",
    "    if doy in summer:\n",
    "        return \"1\"\n",
    "    if doy in fall:\n",
    "        return \"2\"\n",
    "    else:\n",
    "        return \"3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designed-positive",
   "metadata": {},
   "source": [
    "**Step 3:** Append temporal data to our merged dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "obvious-sapphire",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Merged.insert(0, \"Year\", df_Merged.index.year)\n",
    "df_Merged.insert(1, \"Month\", df_Merged.index.month)\n",
    "df_Merged.insert(3, \"Day\", df_Merged.index.day)\n",
    "df_Merged.insert(4, \"Hour\", df_Merged.index.hour)\n",
    "df_Merged.insert(5, \"Minute\", df_Merged.index.minute)\n",
    "df_Merged.insert(6, \"Weekday\", df_Merged.index.weekday)\n",
    "df_Merged.insert(7, \"Season\", df_Merged.index.dayofyear.map(season))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "humanitarian-dependence",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## Miscellaneous functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "logical-elephant",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### 1) Augmented Dickey–Fuller test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominant-click",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adfuller_test(series, signif=0.05, name=\"\"):\n",
    "    r = adfuller(series, autolag=\"AIC\")\n",
    "    output = {\"test_statistic\": round(r[0], 4), \"pvalue\": round(r[1], 4), \"n_lags\": round(r[2], 4), \"n_obs\": r[3]}\n",
    "    p_value = output[\"pvalue\"]\n",
    "\n",
    "    def adjust(val, length=6):\n",
    "        return str(val).ljust(length)\n",
    "\n",
    "    print(f'      Augmented Dickey-Fuller Test on \"{name}\"', \"\\n   \", \"-\" * 47)\n",
    "    print(f\" Null Hypothesis: Data has unit root. Non-Stationary.\")\n",
    "    print(f\" Significance Level    = {signif}\")\n",
    "    print(f' Test Statistic        = {output[\"test_statistic\"]}')\n",
    "    print(f' No. Lags Chosen       = {output[\"n_lags\"]}')\n",
    "\n",
    "    for key, val in r[4].items():\n",
    "        print(f\" Critical value {adjust(key)} = {round(val, 3)}\")\n",
    "    if p_value <= signif:\n",
    "        print(f\" => P-Value = {p_value}. Rejecting Null Hypothesis.\")\n",
    "        print(f\" => Series is Stationary.\")\n",
    "    else:\n",
    "        print(f\" => P-Value = {p_value}. Weak evidence to reject the Null Hypothesis.\")\n",
    "        print(f\" => Series is Non-Stationary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sonic-vegetarian",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### 2) Augmented Dickey–Fuller test w/plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "circular-making",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_stationarity(series, signif=0.05, name=\"\", ylabel=\"\"):\n",
    "    def adjust(val, length=6):\n",
    "        return str(val).ljust(length)\n",
    "\n",
    "    rolmean = series.rolling(12).mean()\n",
    "    rolstd = series.rolling(12).std()\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    series.plot(ax=ax, alpha=0.5)\n",
    "    rolmean.plot(ax=ax, alpha=0.7)\n",
    "    rolstd.plot(ax=ax, alpha=0.7)\n",
    "\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_xlim(left=0, right=len(series))\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.title(\"Rolling Mean & Standard Deviation\")\n",
    "    plt.setp(ax.get_xticklabels(), ha=\"right\", rotation=60)\n",
    "\n",
    "    leg = plt.legend()\n",
    "    leg.get_texts()[0].set_text(name)\n",
    "    leg.get_texts()[1].set_text(\"Rolling Mean\")\n",
    "    leg.get_texts()[2].set_text(\"Rolling STD\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show(block=False)\n",
    "\n",
    "    adfuller_test(series, 0.05, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vietnamese-knowing",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### 3) Granger Causality test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turkish-heather",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grangers_causation_matrix(data, variables, test=\"ssr_chi2test\", maxlag=12, verbose=False):\n",
    "    df = pd.DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)\n",
    "    for c in df.columns:\n",
    "        for r in df.index:\n",
    "            test_result = grangercausalitytests(data[[r, c]], maxlag=maxlag, verbose=False)\n",
    "            p_values = ([round(test_result[i + 1][0][test][1], 2) for i in range(maxlag)])\n",
    "            if verbose:\n",
    "                print(f\"Y = {r}, X = {c}, P Values = {p_values}\")\n",
    "            min_p_value = np.min(p_values)\n",
    "            df.loc[r, c] = min_p_value\n",
    "    df.columns = [var + \"_x\" for var in variables]\n",
    "    df.index = [var + \"_y\" for var in variables]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "differential-audit",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### 4) Determine which highly correlated independent variables have a stronger correlation with our target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boxed-cedar",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(df, threshold, target_variable):\n",
    "    col_corr = set()\n",
    "    corr_matrix = df.corr()\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold:\n",
    "                colname = corr_matrix.columns[i]\n",
    "                rowname = corr_matrix.index[j]\n",
    "                cor1 = abs(df[colname].corr(target_variable))\n",
    "                cor2 = abs(df[rowname].corr(target_variable))\n",
    "                if  cor1 > cor2:\n",
    "                    col_corr.add(corr_matrix.index[j])\n",
    "                else:\n",
    "                    col_corr.add(corr_matrix.columns[i])\n",
    "    return col_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imported-malawi",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### 5) Reshape correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filled-affect",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_corr(df):\n",
    "    df_corr = df.corr().stack().reset_index()\n",
    "    df_corr.columns = [\"Feature 1\", \"Feature 2\", \"Correlation\"]\n",
    "    mask_dups = (df_corr[[\"Feature 1\", \"Feature 2\"]].apply(frozenset, axis=1).duplicated()) | (df_corr[\"Feature 1\"] == df_corr[\"Feature 2\"])\n",
    "    df_corr = df_corr[~mask_dups]\n",
    "\n",
    "    return df_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alien-youth",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### 6) Forecasting accuracy metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c688d1fd-a40d-4e0c-92b8-cdfd25b176cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(forecast, actual):\n",
    "    mape = np.round(np.mean(np.abs(forecast - actual) / np.abs(actual)) * 100, 2)\n",
    "    return mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authentic-pathology",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_accuracy(forecast, actual):\n",
    "    mape = np.round(np.mean(np.abs(forecast - actual) / np.abs(actual)) * 100, 2)\n",
    "    mae = np.round(np.mean(np.abs(forecast - actual)), 2)\n",
    "    rmse = np.round(np.mean((forecast - actual) ** 2) ** 0.5, 2)\n",
    "\n",
    "    print(\"Forecasting accuracy metrics:\")\n",
    "    print(f\"\\t - MAPE = {mape}%\")\n",
    "    print(f\"\\t - MAE = {mae}\")\n",
    "    print(f\"\\t - RMSE = {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bizarre-leader",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### 7) Reshape data into a suitable format for single step forecasting using our CNN-LSTM architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "useful-rally",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_Supervised(X, y, time_steps=1):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        v = X.iloc[i : (i + time_steps)].values\n",
    "        Xs.append(v)\n",
    "        ys.append(y.iloc[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be30a2f-7165-475f-99b3-d8be8f418cfc",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "### 8) Reshape data into a suitable format for multi-step forecasting using our CNN-LSTM architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b54559-9490-4f99-8b8e-22cfe88d73fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_Supervised_ms(ts: np.array, lag=1, n_ahead=1, target_index=0) -> tuple:\n",
    "    n_features = ts.shape[1]\n",
    "    X, Y = [], []\n",
    "\n",
    "    if len(ts) - lag <= 0:\n",
    "        X.append(ts)\n",
    "    else:\n",
    "        for i in range(len(ts) - lag - n_ahead):\n",
    "            Y.append(ts[(i + lag) : (i + lag + n_ahead), target_index])\n",
    "            X.append(ts[i : (i + lag)])\n",
    "    X, Y = np.array(X), np.array(Y)\n",
    "\n",
    "    X = np.reshape(X, (X.shape[0], lag, n_features))\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optical-relief",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "# Part 2: Clustering <a id=\"Part2_REFIT\"> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thick-cowboy",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## 2.1: Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aware-christopher",
   "metadata": {},
   "source": [
    "**Step 1:** Create a copy of our REFIT dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-desktop",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_REFIT_resampled_c = df_REFIT_resampled.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identified-employment",
   "metadata": {},
   "source": [
    "**Step 2:** Drop all columns barre the `Aggregate` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "humanitarian-creativity",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_REFIT_c = cols_REFIT.copy()\n",
    "cols_REFIT_c.remove(\"Aggregate\")\n",
    "df_REFIT_resampled_c.drop(cols_REFIT_c, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggressive-racing",
   "metadata": {},
   "source": [
    "**Step 3:** Reshape our dataframe as 96 columns that represent the 96 15-minute chunks of each day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supposed-traveler",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_REFIT_resampled_c.index = pd.MultiIndex.from_arrays([df_REFIT_resampled_c.index.date, df_REFIT_resampled_c.index.time], names=[\"Date\", \"Time\"])\n",
    "df_REFIT_resampled_c = df_REFIT_resampled_c.unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thorough-valley",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.1.1: Statistical parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "massive-floating",
   "metadata": {},
   "source": [
    "**Step 1:** Split our day into 5 periods:\n",
    " - `LEEM`: Late evening/early morning (23:30-06:00)\n",
    " - `MR`: Morning (06:00-11:00)\n",
    " - `LMAF`: Late morning/afternoon (11:00-15:00)\n",
    " - `LAEE`: Late afternoon/early evening (15:00-20:30)\n",
    " - `EV`: Evening (20:30-23:30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-indication",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEEM = df_REFIT_resampled_c.iloc[:, np.r_[0:24, 92:96]]\n",
    "MR = df_REFIT_resampled_c.iloc[:, 24:45]\n",
    "LMAF = df_REFIT_resampled_c.iloc[:, 44:61]\n",
    "LAEE = df_REFIT_resampled_c.iloc[:, 60:83]\n",
    "EV = df_REFIT_resampled_c.iloc[:, 82:95]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepting-religious",
   "metadata": {},
   "source": [
    "**Step 2:** Create a new dataframe that consists of the mean, min, max and standard deviation of each of our 5 periods per day. We now represent each day with 20 variables rather than 96."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "realistic-insurance",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SP = LEEM.mean(axis=1).to_frame(name=\"LEEM_Mean\")\n",
    "df_SP.insert(len(df_SP.columns), \"LEEM_Min\", LEEM.min(axis=1))\n",
    "df_SP.insert(len(df_SP.columns), \"LEEM_Max\", LEEM.max(axis=1))\n",
    "df_SP.insert(len(df_SP.columns), \"LEEM_STD\", LEEM.std(axis=1))\n",
    "df_SP.insert(len(df_SP.columns), \"MR_Mean\", MR.mean(axis=1))\n",
    "df_SP.insert(len(df_SP.columns), \"MR_Min\", MR.min(axis=1))\n",
    "df_SP.insert(len(df_SP.columns), \"MR_Max\", MR.max(axis=1))\n",
    "df_SP.insert(len(df_SP.columns), \"MR_STD\", MR.std(axis=1))\n",
    "df_SP.insert(len(df_SP.columns), \"LMAF_Mean\", LMAF.mean(axis=1))\n",
    "df_SP.insert(len(df_SP.columns), \"LMAF_Min\", LMAF.min(axis=1))\n",
    "df_SP.insert(len(df_SP.columns), \"LMAF_Max\", LMAF.max(axis=1))\n",
    "df_SP.insert(len(df_SP.columns), \"LMAF_STD\", LMAF.std(axis=1))\n",
    "df_SP.insert(len(df_SP.columns), \"LAEE_Mean\", LAEE.mean(axis=1))\n",
    "df_SP.insert(len(df_SP.columns), \"LAEE_Min\", LAEE.min(axis=1))\n",
    "df_SP.insert(len(df_SP.columns), \"LAEE_Max\", LAEE.max(axis=1))\n",
    "df_SP.insert(len(df_SP.columns), \"LAEE_STD\", LAEE.std(axis=1))\n",
    "df_SP.insert(len(df_SP.columns), \"EV_Mean\", EV.mean(axis=1))\n",
    "df_SP.insert(len(df_SP.columns), \"EV_Min\", EV.min(axis=1))\n",
    "df_SP.insert(len(df_SP.columns), \"EV_Max\", EV.max(axis=1))\n",
    "df_SP.insert(len(df_SP.columns), \"EV_STD\", EV.std(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "returning-advantage",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.1.2: PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessible-rogers",
   "metadata": {},
   "source": [
    "**Step 1:** Determine the cumulative explained variance ratio as a function of the number of variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liquid-conference",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA().fit(df_SP)\n",
    "\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel(\"Number of variables\")\n",
    "plt.ylabel(\"Cumulative explained variance\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "published-basket",
   "metadata": {},
   "source": [
    "**Step 2:** Apply PCA to our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governmental-telephone",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(random_state=RANDOM_SEED, n_components = 8)\n",
    "df_SP1 = pca.fit_transform(df_SP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfied-essay",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.1.3: UMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "potential-firewall",
   "metadata": {},
   "source": [
    "**Step 1:** Apply UMAP to our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "german-highlight",
   "metadata": {},
   "outputs": [],
   "source": [
    "projection = umap.UMAP(random_state=RANDOM_SEED, n_neighbors=np.power(len(df_REFIT_resampled_c), 0.5).astype(int), min_dist=0.1, n_components=2).fit_transform(df_SP)\n",
    "projection = TSNE(random_state=RANDOM_SEED).fit_transform(projection)\n",
    "plt.scatter(*projection.T)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ecd7dd-d98e-461f-a04e-3b51003432e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Split_CL = int(len(projection) * 0.8)\n",
    "projection = projection[0:Split_CL]\n",
    "df_REFIT_resampled_CL = df_REFIT_resampled_c[0:Split_CL].copy()\n",
    "df_REFIT_resampled_TE = df_REFIT_resampled_c[Split_CL:].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dated-sydney",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## 2.2: HDBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "other-there",
   "metadata": {},
   "source": [
    "**Step 1:** Define our HDBSCAN clusterer with the appropriate hyperparameters and fit it to our 2-dimensional projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agreed-young",
   "metadata": {},
   "outputs": [],
   "source": [
    "HDB = hdbscan.HDBSCAN(min_cluster_size=(len(projection) // 10), min_samples=15)\n",
    "HDB = HDB.fit(projection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competitive-costa",
   "metadata": {},
   "outputs": [],
   "source": [
    "HDB.condensed_tree_.plot(select_clusters=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solved-steam",
   "metadata": {},
   "source": [
    "**Step 2:** Plot/visualize our clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "major-immunology",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = HDB.labels_\n",
    "labels = [label + 1 for label in labels]\n",
    "n_clusters = len(set(labels)) - (1 if 0 in labels else 0)\n",
    "n_noise = list(labels).count(0)\n",
    "core_samples_mask = np.zeros_like(labels, dtype=bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exotic-brunei",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = set(labels)\n",
    "colors = np.array(sns.color_palette(\"bright\", len(unique_labels)))\n",
    "colors[0] = [0, 0, 0]\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k == 0:\n",
    "        # Black used for noise.\n",
    "        col = [0, 0, 0, 1]\n",
    "    class_member_mask = labels == k\n",
    "\n",
    "    xy = projection[class_member_mask & core_samples_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], \"o\", markerfacecolor=tuple(col), markeredgecolor=\"k\", markersize=14)\n",
    "\n",
    "    xy = projection[class_member_mask & ~core_samples_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], \"o\", markerfacecolor=tuple(col), markeredgecolor=\"k\", markersize=6)\n",
    "plt.xticks(color=\"w\")\n",
    "plt.yticks(color=\"w\")\n",
    "plt.title(\"Estimated number of clusters: %d\" % n_clusters)\n",
    "plt.show()\n",
    "\n",
    "print(\"Estimated number of clusters: %d\" % n_clusters)\n",
    "print(\"Estimated number of noise points: %d\" % n_noise)\n",
    "print(\"Silhouette Coefficient: %0.3f\" % metrics.silhouette_score(projection, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minor-antarctica",
   "metadata": {},
   "source": [
    "**Step 3:** Plot/visualize the average pattern per cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a570875-9d69-47fc-b432-9552d6d24317",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_REFIT_resampled_CL[\"Labels\"] = labels\n",
    "Cx = []\n",
    "\n",
    "for i in range(0, n_clusters + 1):\n",
    "    if(i == 0):\n",
    "        Cx.append(\"Noise\")\n",
    "    else:\n",
    "        Cx.append(f\"Cluster \" + str(i))\n",
    "    globals()[\"C\" + str(i)] = (df_REFIT_resampled_CL.loc[df_REFIT_resampled_CL[\"Labels\"] == i]).mean(axis=0)\n",
    "    globals()[\"C\" + str(i)] = globals()[\"C\" + str(i)].reset_index()\n",
    "    globals()[\"C\" + str(i)].drop(\"level_0\", axis=1, inplace=True)\n",
    "    globals()[\"C\" + str(i)].drop(globals()[\"C\" + str(i)].tail(1).index, inplace=True)\n",
    "    globals()[\"C\" + str(i)][\"Time\"] = globals()[\"C\" + str(i)][\"Time\"].astype(\"str\")\n",
    "    globals()[\"C\" + str(i)][\"Time\"] = pd.to_datetime(globals()[\"C\" + str(i)][\"Time\"])\n",
    "    globals()[\"C\" + str(i)] = globals()[\"C\" + str(i)].set_index(\"Time\")\n",
    "    globals()[\"C\" + str(i)].index = globals()[\"C\" + str(i)].index.strftime(\"%H:%M:%S\")\n",
    "    globals()[\"C\" + str(i)] = globals()[\"C\" + str(i)].rename(columns={0: \"Aggregate\"})\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(0, n_clusters + 1):\n",
    "    globals()[\"C\" + str(i)].plot(ax=ax, color=colors[i], linewidth=2)\n",
    "\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.set_ylabel(\"Aggregate Power Consumption (Kilowatts)\")\n",
    "ax.set_xlim(left=0, right=95)\n",
    "plt.legend([*Cx], loc=\"best\", fontsize=18)\n",
    "plt.setp(ax.get_xticklabels(), ha=\"right\", rotation=60)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrative-baker",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_REFIT_resampled_CL.index = pd.DatetimeIndex(df_REFIT_resampled_CL.index)\n",
    "df_REFIT_resampled_CL.insert(0, \"Day_name\", df_REFIT_resampled_CL.index.day_name())\n",
    "df_REFIT_resampled_CL.insert(0, \"Month_name\", df_REFIT_resampled_CL.index.month_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-deadline",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.countplot(\n",
    "    x=\"Month_name\",\n",
    "    hue=\"Labels\",\n",
    "    data=df_REFIT_resampled_CL,\n",
    "    palette=colors,\n",
    "    order=[\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"],\n",
    ")\n",
    "\n",
    "plt.setp(ax.get_xticklabels(), ha=\"right\", rotation=60)\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extensive-flashing",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.countplot(\n",
    "    x=\"Day_name\",\n",
    "    hue=\"Labels\",\n",
    "    data=df_REFIT_resampled_CL,\n",
    "    palette=colors,\n",
    "    order=[\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"],\n",
    ")\n",
    "\n",
    "plt.setp(ax.get_xticklabels(), ha=\"right\", rotation=60)\n",
    "plt.xlabel(\"Day\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c261745-458a-4626-bc6d-0e0f2e7f18ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = df_REFIT_resampled_CL.copy()\n",
    "x1.index = x1.index.month_name()\n",
    "x1 = x1[~x1[\"Labels\"].isin([0])]\n",
    "\n",
    "kde_kws = {\"bw_adjust\": 1.0, \"bw_method\": \"silverman\"}\n",
    "line_kws = {\"linewidth\": 2, \"alpha\": 1.0}\n",
    "colors2 = np.delete(colors, 0, 0)\n",
    "order = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n",
    "\n",
    "ax = sns.histplot(\n",
    "    data=x1.loc[order],\n",
    "    x=\"Month_name\",\n",
    "    hue=\"Labels\",\n",
    "    alpha=0.75,\n",
    "    palette=list(colors2),\n",
    "    multiple=\"dodge\",\n",
    "    kde=True,\n",
    "    kde_kws=kde_kws,\n",
    "    shrink=0.75,\n",
    "    line_kws=line_kws,\n",
    ")\n",
    "ax.set_xlim(-0.75, 11.75)\n",
    "ax.get_legend().remove()\n",
    "plt.setp(ax.get_xticklabels(), ha=\"right\", rotation=60)\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045c83a8-2135-4ee1-a688-378aca6269ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = df_REFIT_resampled_CL.copy()\n",
    "x1.index = x1.index.day_name()\n",
    "x1 = x1[~x1[\"Labels\"].isin([0])]\n",
    "\n",
    "kde_kws = {\"bw_adjust\": 1.0, \"bw_method\": \"silverman\"}\n",
    "line_kws = {\"linewidth\": 2, \"alpha\": 1.0}\n",
    "colors2 = np.delete(colors, 0, 0)\n",
    "order = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
    "\n",
    "ax = sns.histplot(\n",
    "    data=x1.loc[order],\n",
    "    x=\"Day_name\",\n",
    "    hue=\"Labels\",\n",
    "    alpha=0.75,\n",
    "    palette=list(colors2),\n",
    "    multiple=\"dodge\",\n",
    "    kde=True,\n",
    "    kde_kws=kde_kws,\n",
    "    shrink=0.75,\n",
    "    line_kws=line_kws,\n",
    ")\n",
    "ax.set_xlim(-0.75, 6.75)\n",
    "ax.get_legend().remove()\n",
    "plt.setp(ax.get_xticklabels(), ha=\"right\", rotation=60)\n",
    "plt.xlabel(\"Day\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3c04d7-6878-4e57-bef0-953a41a98918",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "# Part 3: CART <a id=\"Part3_UCID\"> </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009ab9c0-763c-41a8-9b46-a05318dbd304",
   "metadata": {},
   "outputs": [],
   "source": [
    "CART_CL = df_REFIT_resampled_CL.copy()\n",
    "CART_CL.drop(CART_CL.iloc[:, 0:2], axis=1, inplace=True)\n",
    "CART_CL.drop(CART_CL.iloc[:, 0:96], axis=1, inplace=True)\n",
    "CART_CL.columns = CART_CL.columns.droplevel(1)\n",
    "CART_CL = CART_CL[~CART_CL[\"Labels\"].isin([0])]\n",
    "colors2 = np.delete(colors, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ecb0d2-175d-428a-9d82-765dadecffe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Merged2 = df_Merged.copy()\n",
    "df_Merged2.drop(\"Aggregate\", axis=1, inplace=True)\n",
    "df_Merged2.drop(df_Merged2.iloc[:, 0:8], axis=1, inplace=True)\n",
    "df_Merged2 = df_Merged2.resample(\"1D\").agg([\"min\", \"max\",\"std\", \"mean\"]).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad8fb2d-7863-4ddf-b451-3a6589169e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "CART_CL = pd.merge(left=df_Merged2, left_on=df_Merged2.index, right=CART_CL, right_on=CART_CL.index)\n",
    "CART_CL.rename(columns={\"key_0\": \"Date\"}, inplace=True)\n",
    "CART_CL = CART_CL.set_index(\"Date\")\n",
    "CART_CL.index = pd.to_datetime(CART_CL.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae9276b-8566-47fd-be8b-861f24d411ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "CART2 = CART_CL.copy()\n",
    "CART2 = CART2.loc[:, CART2.columns != \"Labels\"].rename(columns='_'.join)\n",
    "CART2.insert(len(CART2.columns), \"Labels\", CART_CL.Labels)\n",
    "CART_CL = CART2.copy()\n",
    "del CART2\n",
    "CART_CL.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ef87e6-e549-4467-b75e-c6c88ce10c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "CART_CL.insert(len(CART_CL.columns), \"Day\", CART_CL.index.day)\n",
    "CART_CL.insert(len(CART_CL.columns), \"Month\", CART_CL.index.month)\n",
    "CART_CL.insert(len(CART_CL.columns), \"Year\", CART_CL.index.year)\n",
    "CART_CL.insert(len(CART_CL.columns), \"Day Of Week\", CART_CL.index.dayofweek)\n",
    "CART_CL.insert(len(CART_CL.columns), \"Day Of Year\", CART_CL.index.dayofyear)\n",
    "\n",
    "CART_CL[\"Day\"] = CART_CL[\"Day\"].astype(\"category\")\n",
    "CART_CL[\"Month\"] = CART_CL[\"Month\"].astype(\"category\")\n",
    "CART_CL[\"Year\"] = CART_CL[\"Year\"].astype(\"category\")\n",
    "CART_CL[\"Day Of Week\"] = CART_CL[\"Day Of Week\"].astype(\"category\")\n",
    "CART_CL[\"Day Of Year\"] = CART_CL[\"Day Of Year\"].astype(\"category\")\n",
    "CART_CL[\"Labels\"] = CART_CL[\"Labels\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e43924e-6f26-4b26-932f-5d47e7d094c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.countplot(x=CART_CL[\"Labels\"], data=CART_CL, palette=colors2)\n",
    "plt.title(\"Number of days per cluster\")\n",
    "plt.xlabel(\"Label\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885434da-d48d-4559-aaf6-501377ad52ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = CART_CL.select_dtypes(include=[\"category\"]).columns\n",
    "cols = [CART_CL.columns.get_loc(col) for col in cols]\n",
    "del cols[0]\n",
    "cols = [col - 1 for col in cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85348e7a-f684-44e6-b3dc-20546c13de04",
   "metadata": {},
   "outputs": [],
   "source": [
    "smotenc = SMOTENC(random_state=RANDOM_SEED, categorical_features=[*cols])\n",
    "X, y = (CART_CL.loc[:, CART_CL.columns != \"Labels\"], CART_CL[\"Labels\"].to_frame())\n",
    "X, y = smotenc.fit_resample(X, y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y[\"Labels\"], random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a166867-fd7b-46d1-9b0e-2d0aef05a360",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.countplot(x=y[\"Labels\"], data=y, palette=colors2)\n",
    "plt.title(\"Number of days per cluster\")\n",
    "plt.xlabel(\"Label\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86052047-14dc-4ead-9603-5c1e820b8779",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF = RandomForestClassifier(random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33bc011-d25d-4990-bffa-ebe3e310bd30",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3.1 - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379f8af1-d9f2-4c10-816c-6694455e1c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_estimators = [int(x) for x in np.linspace(200, 1600, num=8)]\n",
    "# max_features = ['auto', 'log2']\n",
    "# max_depth = [int(x) for x in np.linspace(10, 110, num=11)]\n",
    "# max_depth.append(None)\n",
    "# min_samples_split = [2, 5, 10]\n",
    "# min_samples_leaf = [1, 2, 4, 6, 8, 10]\n",
    "# bootstrap = [True, False]\n",
    "\n",
    "# random_grid = {\n",
    "#     \"n_estimators\": n_estimators,\n",
    "#     \"max_features\": max_features,\n",
    "#     \"max_depth\": max_depth,\n",
    "#     \"min_samples_split\": min_samples_split,\n",
    "#     \"min_samples_leaf\": min_samples_leaf,\n",
    "#     \"bootstrap\": bootstrap,\n",
    "# }\n",
    "\n",
    "# RF = RandomForestClassifier(random_state=RANDOM_SEED)\n",
    "# RF_CV = RandomizedSearchCV(RF, random_grid, n_iter=50, verbose=2)\n",
    "# RF_CV.fit(X, y.values.ravel())\n",
    "\n",
    "# print(f\"Tuned Random Forest Parameters: {RF_CV.best_params_}\")\n",
    "# print(f\"Best score is {RF_CV.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768f50c9-5c67-4cf0-aa87-0b4938b607df",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF = RandomForestClassifier(random_state=RANDOM_SEED)\n",
    "#RF.set_params(**RF_CV.best_params_)\n",
    "RF.fit(X_train, y_train.values.ravel())\n",
    "RF.score(X_test, y_test), RF.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d147b806-676d-4410-bb51-b968d7442a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "CART_TE = df_REFIT_resampled_TE.copy()\n",
    "CART_TE.drop(CART_TE.iloc[:, 0:96], axis=1, inplace=True)\n",
    "CART_TE.columns = CART_TE.columns.droplevel(1)\n",
    "colors2 = np.delete(colors, 0, 0)\n",
    "CART_TE.index = pd.DatetimeIndex(CART_TE.index)\n",
    "CART_TE.insert(len(CART_TE.columns), \"Labels\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870a500d-746b-4b24-91f0-22b57a8181d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Merged2 = df_Merged.copy()\n",
    "df_Merged2.drop(\"Aggregate\", axis=1, inplace=True)\n",
    "df_Merged2.drop(df_Merged2.iloc[:, 0:8], axis=1, inplace=True)\n",
    "df_Merged2 = df_Merged2.resample(\"1D\").agg([\"min\", \"max\",\"std\", \"mean\"]).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7c2bf0-ec1f-46ba-8c7a-98620a90db9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CART_TE = pd.merge(left=df_Merged2, left_on=df_Merged2.index, right=CART_TE, right_on=CART_TE.index)\n",
    "CART_TE.rename(columns={\"key_0\": \"Date\"}, inplace=True)\n",
    "CART_TE = CART_TE.set_index(\"Date\")\n",
    "CART_TE.index = pd.to_datetime(CART_TE.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de23d07-2a4b-4e0a-a90c-2f38dc8f30f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "CART2 = CART_TE.copy()\n",
    "CART2 = CART2.loc[:, CART2.columns != \"Labels\"].rename(columns='_'.join)\n",
    "CART_TE = CART2.copy()\n",
    "del CART2\n",
    "CART_TE.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc1374b-b320-46c7-ad78-611992042cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "CART_TE.insert(len(CART_TE.columns), \"Day\", CART_TE.index.day)\n",
    "CART_TE.insert(len(CART_TE.columns), \"Month\", CART_TE.index.month)\n",
    "CART_TE.insert(len(CART_TE.columns), \"Year\", CART_TE.index.year)\n",
    "CART_TE.insert(len(CART_TE.columns), \"Day Of Week\", CART_TE.index.dayofweek)\n",
    "CART_TE.insert(len(CART_TE.columns), \"Day Of Year\", CART_TE.index.dayofyear)\n",
    "\n",
    "CART_TE[\"Day\"] = CART_TE[\"Day\"].astype(\"category\")\n",
    "CART_TE[\"Month\"] = CART_TE[\"Month\"].astype(\"category\")\n",
    "CART_TE[\"Year\"] = CART_TE[\"Year\"].astype(\"category\")\n",
    "CART_TE[\"Day Of Week\"] = CART_TE[\"Day Of Week\"].astype(\"category\")\n",
    "CART_TE[\"Day Of Year\"] = CART_TE[\"Day Of Year\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d0c7d5-9011-4bbb-8185-585f3a288539",
   "metadata": {},
   "outputs": [],
   "source": [
    "Labels_test = RF.predict(CART_TE)\n",
    "CART_TE.insert(len(CART_TE.columns), \"Labels\", Labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70d8eb6-0855-432d-be08-c659ae9610d2",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "# Part 4: Forecasting <a id=\"Part3_REFIT\"> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d28b288-db12-4d3b-9433-18a8c36e0702",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## 4.1: Trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c30b5b-efb1-4c49-9056-a53559480729",
   "metadata": {},
   "outputs": [],
   "source": [
    "C1_Merge_train = CART_CL.loc[:, \"Labels\"].to_frame()\n",
    "df_REFIT_resampled_train = df_REFIT_resampled_CL.copy()\n",
    "df_REFIT_resampled_train.Labels = C1_Merge_train.Labels\n",
    "df_REFIT_resampled_train.dropna(inplace=True)\n",
    "C1_Merge_train = df_REFIT_resampled_train.loc[df_REFIT_resampled_train[\"Labels\"] == 1].copy()\n",
    "C1_Merge_train = C1_Merge_train.stack().reset_index()\n",
    "C1_Merge_train[\"Date\"] = C1_Merge_train[\"Date\"].astype(\"str\")\n",
    "C1_Merge_train[\"Time\"] = C1_Merge_train[\"Time\"].astype(\"str\")\n",
    "C1_Merge_train[\"DT\"] = C1_Merge_train[\"Date\"].str.cat(C1_Merge_train[\"Time\"], sep=\" \")\n",
    "\n",
    "cols_NA = [\n",
    "    \"Date\",\n",
    "    \"Time\",\n",
    "    \"Labels\",\n",
    "    \"Day_name\",\n",
    "    \"Month_name\"\n",
    "]\n",
    "\n",
    "C1_Merge_train.drop(cols_NA, axis=1, inplace=True)\n",
    "C1_Merge_train[\"DT\"] = pd.to_datetime(C1_Merge_train[\"DT\"])\n",
    "C1_Merge_train = C1_Merge_train.set_index(\"DT\")\n",
    "C1_Merge_train.dropna(inplace=True)\n",
    "C1_Merge_train = df_Merged.reindex(C1_Merge_train.index)\n",
    "C1_Merge_train = C1_Merge_train.dropna()\n",
    "C1_Merge_train[\"Holiday\"] = C1_Merge_train[\"Holiday\"].astype('int32')\n",
    "C1_Merge_train[\"Season\"] = C1_Merge_train[\"Season\"].astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1046a8a-827b-4771-9491-7609538ced45",
   "metadata": {},
   "outputs": [],
   "source": [
    "C1_Merge_test = CART_TE.loc[:, \"Labels\"].to_frame()\n",
    "df_REFIT_resampled_test = df_REFIT_resampled_TE.copy()\n",
    "df_REFIT_resampled_test.insert(len(df_REFIT_resampled_test.columns), \"Labels\", C1_Merge_test.Labels)\n",
    "df_REFIT_resampled_test.dropna(inplace=True)\n",
    "C1_Merge_test = df_REFIT_resampled_test.loc[df_REFIT_resampled_test[\"Labels\"] == 1].copy()\n",
    "C1_Merge_test = C1_Merge_test.stack().reset_index()\n",
    "C1_Merge_test[\"Date\"] = C1_Merge_test[\"Date\"].astype(\"str\")\n",
    "C1_Merge_test[\"Time\"] = C1_Merge_test[\"Time\"].astype(\"str\")\n",
    "C1_Merge_test[\"DT\"] = C1_Merge_test[\"Date\"].str.cat(C1_Merge_test[\"Time\"], sep=\" \")\n",
    "\n",
    "cols_NA = [\n",
    "    \"Date\",\n",
    "    \"Time\",\n",
    "    \"Labels\",\n",
    "]\n",
    "\n",
    "C1_Merge_test.drop(cols_NA, axis=1, inplace=True)\n",
    "C1_Merge_test[\"DT\"] = pd.to_datetime(C1_Merge_test[\"DT\"])\n",
    "C1_Merge_test = C1_Merge_test.set_index(\"DT\")\n",
    "C1_Merge_test.dropna(inplace=True)\n",
    "C1_Merge_test = df_Merged.reindex(C1_Merge_test.index)\n",
    "C1_Merge_test = C1_Merge_test.dropna()\n",
    "C1_Merge_test[\"Holiday\"] = C1_Merge_test[\"Holiday\"].astype('int32')\n",
    "C1_Merge_test[\"Season\"] = C1_Merge_test[\"Season\"].astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bfa2da-7b60-41d5-b87b-4d3aa1263d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "day = 24 * 60 * 60\n",
    "year = (365.2425) * day\n",
    "timestamp_train = C1_Merge_train.index.map(datetime.datetime.timestamp)\n",
    "timestamp_test = C1_Merge_test.index.map(datetime.datetime.timestamp)\n",
    "\n",
    "day_sin_tr = np.sin(timestamp_train * (2 * np.pi / day))\n",
    "day_cos_tr = np.cos(timestamp_train * (2 * np.pi / day))\n",
    "year_sin_tr = np.sin(timestamp_train * (2 * np.pi / year))\n",
    "year_cos_tr = np.cos(timestamp_train * (2 * np.pi / year))\n",
    "\n",
    "day_sin_te = np.sin(timestamp_test * (2 * np.pi / day))\n",
    "day_cos_te = np.cos(timestamp_test * (2 * np.pi / day))\n",
    "year_sin_te = np.sin(timestamp_test * (2 * np.pi / year))\n",
    "year_cos_te = np.cos(timestamp_test * (2 * np.pi / year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ecce64-adad-4dfa-af1a-5d70794b5211",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_NA = [\"Year\", \"Month\", \"Holiday\", \"Day\", \"Hour\", \"Minute\", \"Weekday\", \"Season\"]\n",
    "\n",
    "C1_Merge_train.drop(cols_NA, axis=1, inplace=True)\n",
    "C1_Merge_test.drop(cols_NA, axis=1, inplace=True)\n",
    "\n",
    "C1_Merge_train.insert(0, \"Day_Sin\", day_sin_tr)\n",
    "C1_Merge_train.insert(0, \"Day_Cos\", day_cos_tr)\n",
    "C1_Merge_train.insert(0, \"Year_Sin\", year_sin_tr)\n",
    "C1_Merge_train.insert(0, \"Year_Cos\", year_cos_tr)\n",
    "\n",
    "C1_Merge_test.insert(0, \"Day_Sin\", day_sin_te)\n",
    "C1_Merge_test.insert(0, \"Day_Cos\", day_cos_te)\n",
    "C1_Merge_test.insert(0, \"Year_Sin\", year_sin_te)\n",
    "C1_Merge_test.insert(0, \"Year_Cos\", year_cos_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71f8b5c-a516-46b0-b099-7ab9c5b594c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = (24 * 60) // 15\n",
    "stl_decompose_result = STL(C1_Merge_train.Aggregate, period=freq).fit()\n",
    "C1_Merge_train.insert(len(C1_Merge_train.columns), \"Trend\", stl_decompose_result.trend.values)\n",
    "\n",
    "stl_decompose_result = STL(C1_Merge_test.Aggregate, period=freq).fit()\n",
    "C1_Merge_test.insert(len(C1_Merge_test.columns), \"Trend\", stl_decompose_result.trend.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07873848-cc9c-48a6-a19d-0eb573469bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_tr = MaxAbsScaler()\n",
    "scaler_te = MaxAbsScaler()\n",
    "\n",
    "C1_Merge_train[C1_Merge_train.columns] = scaler_tr.fit_transform(C1_Merge_train[C1_Merge_train.columns])\n",
    "scaler_tr.max_abs_, scaler_tr.scale_ = scaler_tr.max_abs_[len(C1_Merge_train.columns) - 1], scaler_tr.scale_[len(C1_Merge_train.columns) - 1]\n",
    "\n",
    "C1_Merge_test[C1_Merge_test.columns] = scaler_te.fit_transform(C1_Merge_test[C1_Merge_test.columns])\n",
    "scaler_te.max_abs_, scaler_te.scale_ = scaler_te.max_abs_[len(C1_Merge_test.columns) - 1], scaler_te.scale_[len(C1_Merge_test.columns) - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bc1918-7b52-4238-b05c-7d5427c7c57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = C1_Merge_train.shape[1]\n",
    "time_steps = 24\n",
    "Split = len(C1_Merge_train)\n",
    "\n",
    "s_Train = C1_Merge_train.iloc[0 : int(Split * 0.8)]\n",
    "s_Val = C1_Merge_train.iloc[int(Split * 0.8) :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf49032f-38b7-4a88-bcc0-18496c0b3bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = to_Supervised(s_Train, s_Train.Trend, time_steps)\n",
    "X_val, Y_val = to_Supervised(s_Val, s_Val.Trend, time_steps)\n",
    "X_test, Y_test = to_Supervised(C1_Merge_test, C1_Merge_test.Trend, time_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58e012b-f21c-4a3d-a3a9-57aaa5e31dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(\n",
    "    [\n",
    "        Conv1D(filters=64, kernel_size=3, activation=LeakyReLU(), padding=\"same\", input_shape=(time_steps, n_features)),\n",
    "        MaxPooling1D(2),\n",
    "        Conv1D(filters=32, kernel_size=2, activation=LeakyReLU(), padding=\"same\"),\n",
    "        MaxPooling1D(2),\n",
    "        Conv1D(filters=16, kernel_size=2, activation=LeakyReLU(), padding=\"same\"),\n",
    "        MaxPooling1D(2),\n",
    "        LSTM(256, activation=LeakyReLU(), return_sequences=True),\n",
    "        LSTM(128, activation=LeakyReLU()),\n",
    "        Dense(64),\n",
    "        Dense(1),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "model.compile(optimizer=\"NAdam\", loss=\"logcosh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ef83e0-ba55-4e7e-88c8-073c369edfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_learning = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.2, patience=3, verbose=1, mode=\"auto\", min_delta=0.000025, cooldown=4, min_lr=0)\n",
    "eary_stopping = EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=7, verbose=1, mode=\"auto\")\n",
    "\n",
    "callbacks = [reduce_learning, eary_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caad4d2-4533-4b0d-844f-ceca582560e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val, Y_val),\n",
    "    verbose=1,\n",
    "    shuffle=False,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565ad1cb-a7c3-44fa-b474-794f0510bfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "epochs_graph = np.arange(1, len(loss) + 1)\n",
    "\n",
    "x_ticks = np.arange(0, len(loss) + 1, 5)\n",
    "x_ticks = np.insert(x_ticks, 1, 1)\n",
    "x_ticks = np.insert(x_ticks, len(x_ticks), len(loss))\n",
    "\n",
    "plt.xticks(x_ticks)\n",
    "plt.xlim(1, len(loss))\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(epochs_graph, loss, \"red\", label=\"Training loss\")\n",
    "plt.plot(epochs_graph, val_loss, \"blue\", label=\"Validation loss\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0241f23f-d76b-455e-bcaf-97003dd62782",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"Models/REFIT-Trend.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6af24e5-0c97-4410-864e-5dc64b3c2904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = load_model(\"Models/REFIT-Trend.h5\", custom_objects={\"LeakyReLU\": tensorflow.keras.layers.LeakyReLU})\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542bb37d-d62e-4c21-a3c0-96af5837a8fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Predictions = model.predict(X_test)\n",
    "Predictions = scaler_te.inverse_transform(Predictions.reshape(-1, 1))\n",
    "\n",
    "s_Pred = C1_Merge_test[\"Trend\"].to_frame().copy()\n",
    "s_Pred = s_Pred.iloc[time_steps:]\n",
    "s_Pred[s_Pred.columns] = scaler_te.inverse_transform(s_Pred[s_Pred.columns])\n",
    "s_Pred.insert(len(s_Pred.columns), \"Predictions\", Predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319ba574-ae11-4bb4-a583-6f20566b6030",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_accuracy(s_Pred.Predictions.ravel(), s_Pred.Trend.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b0745e-49eb-4f17-892e-3a59de4d4f27",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## 4.2: Raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f8166f-f59c-4348-9c3d-9d241c5a2ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "C1_Merge_train = CART_CL.loc[:, \"Labels\"].to_frame()\n",
    "df_REFIT_resampled_train = df_REFIT_resampled_CL.copy()\n",
    "df_REFIT_resampled_train.Labels = C1_Merge_train.Labels\n",
    "df_REFIT_resampled_train.dropna(inplace=True)\n",
    "C1_Merge_train = df_REFIT_resampled_train.loc[df_REFIT_resampled_train[\"Labels\"] == 1].copy()\n",
    "C1_Merge_train = C1_Merge_train.stack().reset_index()\n",
    "C1_Merge_train[\"Date\"] = C1_Merge_train[\"Date\"].astype(\"str\")\n",
    "C1_Merge_train[\"Time\"] = C1_Merge_train[\"Time\"].astype(\"str\")\n",
    "C1_Merge_train[\"DT\"] = C1_Merge_train[\"Date\"].str.cat(C1_Merge_train[\"Time\"], sep=\" \")\n",
    "\n",
    "cols_NA = [\n",
    "    \"Date\",\n",
    "    \"Time\",\n",
    "    \"Labels\",\n",
    "    \"Day_name\",\n",
    "    \"Month_name\"\n",
    "]\n",
    "\n",
    "C1_Merge_train.drop(cols_NA, axis=1, inplace=True)\n",
    "C1_Merge_train[\"DT\"] = pd.to_datetime(C1_Merge_train[\"DT\"])\n",
    "C1_Merge_train = C1_Merge_train.set_index(\"DT\")\n",
    "C1_Merge_train.dropna(inplace=True)\n",
    "C1_Merge_train = df_Merged.reindex(C1_Merge_train.index)\n",
    "C1_Merge_train = C1_Merge_train.dropna()\n",
    "C1_Merge_train[\"Holiday\"] = C1_Merge_train[\"Holiday\"].astype('int32')\n",
    "C1_Merge_train[\"Season\"] = C1_Merge_train[\"Season\"].astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e939cd-f9a8-465c-bd88-99b808dec18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "C1_Merge_test = CART_TE.loc[:, \"Labels\"].to_frame()\n",
    "df_REFIT_resampled_test = df_REFIT_resampled_TE.copy()\n",
    "df_REFIT_resampled_test.insert(len(df_REFIT_resampled_test.columns), \"Labels\", C1_Merge_test.Labels)\n",
    "df_REFIT_resampled_test.dropna(inplace=True)\n",
    "C1_Merge_test = df_REFIT_resampled_test.loc[df_REFIT_resampled_test[\"Labels\"] == 1].copy()\n",
    "C1_Merge_test = C1_Merge_test.stack().reset_index()\n",
    "C1_Merge_test[\"Date\"] = C1_Merge_test[\"Date\"].astype(\"str\")\n",
    "C1_Merge_test[\"Time\"] = C1_Merge_test[\"Time\"].astype(\"str\")\n",
    "C1_Merge_test[\"DT\"] = C1_Merge_test[\"Date\"].str.cat(C1_Merge_test[\"Time\"], sep=\" \")\n",
    "\n",
    "cols_NA = [\n",
    "    \"Date\",\n",
    "    \"Time\",\n",
    "    \"Labels\",\n",
    "]\n",
    "\n",
    "C1_Merge_test.drop(cols_NA, axis=1, inplace=True)\n",
    "C1_Merge_test[\"DT\"] = pd.to_datetime(C1_Merge_test[\"DT\"])\n",
    "C1_Merge_test = C1_Merge_test.set_index(\"DT\")\n",
    "C1_Merge_test.dropna(inplace=True)\n",
    "C1_Merge_test = df_Merged.reindex(C1_Merge_test.index)\n",
    "C1_Merge_test = C1_Merge_test.dropna()\n",
    "C1_Merge_test[\"Holiday\"] = C1_Merge_test[\"Holiday\"].astype('int32')\n",
    "C1_Merge_test[\"Season\"] = C1_Merge_test[\"Season\"].astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829d5433-8bb7-489f-bea4-f983ea46fb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "day = 24 * 60 * 60\n",
    "year = (365.2425) * day\n",
    "timestamp_train = C1_Merge_train.index.map(datetime.datetime.timestamp)\n",
    "timestamp_test = C1_Merge_test.index.map(datetime.datetime.timestamp)\n",
    "\n",
    "day_sin_tr = np.sin(timestamp_train * (2 * np.pi / day))\n",
    "day_cos_tr = np.cos(timestamp_train * (2 * np.pi / day))\n",
    "year_sin_tr = np.sin(timestamp_train * (2 * np.pi / year))\n",
    "year_cos_tr = np.cos(timestamp_train * (2 * np.pi / year))\n",
    "\n",
    "day_sin_te = np.sin(timestamp_test * (2 * np.pi / day))\n",
    "day_cos_te = np.cos(timestamp_test * (2 * np.pi / day))\n",
    "year_sin_te = np.sin(timestamp_test * (2 * np.pi / year))\n",
    "year_cos_te = np.cos(timestamp_test * (2 * np.pi / year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35701102-a0a0-4c21-ac2d-966d8d32a8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_NA = [\"Year\", \"Month\", \"Holiday\", \"Day\", \"Hour\", \"Minute\", \"Weekday\", \"Season\"]\n",
    "\n",
    "C1_Merge_train.drop(cols_NA, axis=1, inplace=True)\n",
    "C1_Merge_test.drop(cols_NA, axis=1, inplace=True)\n",
    "\n",
    "C1_Merge_train.insert(0, \"Day_Sin\", day_sin_tr)\n",
    "C1_Merge_train.insert(0, \"Day_Cos\", day_cos_tr)\n",
    "C1_Merge_train.insert(0, \"Year_Sin\", year_sin_tr)\n",
    "C1_Merge_train.insert(0, \"Year_Cos\", year_cos_tr)\n",
    "\n",
    "C1_Merge_test.insert(0, \"Day_Sin\", day_sin_te)\n",
    "C1_Merge_test.insert(0, \"Day_Cos\", day_cos_te)\n",
    "C1_Merge_test.insert(0, \"Year_Sin\", year_sin_te)\n",
    "C1_Merge_test.insert(0, \"Year_Cos\", year_cos_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1932a8a4-70d8-4e85-aca2-46e37b08ae7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = (24 * 60) // 15\n",
    "stl_decompose_result = STL(C1_Merge_train.Aggregate, period=freq).fit()\n",
    "C1_Merge_train.insert(len(C1_Merge_train.columns), \"Trend\", stl_decompose_result.trend.values)\n",
    "\n",
    "stl_decompose_result = STL(C1_Merge_test.Aggregate, period=freq).fit()\n",
    "C1_Merge_test.insert(len(C1_Merge_test.columns), \"Trend\", stl_decompose_result.trend.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9aba28-fc81-4566-a712-83420e9acb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "C1_GAP_train = C1_Merge_train.Aggregate.copy().to_frame()\n",
    "C1_Merge_train.Aggregate = savgol_filter(C1_Merge_train.Aggregate, 5, 3)\n",
    "\n",
    "C1_GAP_test = C1_Merge_test.Aggregate.copy().to_frame()\n",
    "C1_Merge_test.Aggregate = savgol_filter(C1_Merge_test.Aggregate, 5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253bd1ae-64b2-409d-8d11-ea07fe826955",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_tr = MinMaxScaler()\n",
    "scaler_te = MinMaxScaler()\n",
    "\n",
    "C1_Merge_train[C1_Merge_train.columns] = scaler_tr.fit_transform(C1_Merge_train[C1_Merge_train.columns])\n",
    "scaler_tr.min_, scaler_tr.scale_ = scaler_tr.min_[len(C1_Merge_train.columns) - 2], scaler_tr.scale_[len(C1_Merge_train.columns) - 2]\n",
    "\n",
    "C1_Merge_test[C1_Merge_test.columns] = scaler_te.fit_transform(C1_Merge_test[C1_Merge_test.columns])\n",
    "scaler_te.min_, scaler_te.scale_ = scaler_te.min_[len(C1_Merge_test.columns) - 2], scaler_te.scale_[len(C1_Merge_test.columns) - 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d80ec6-38b3-4de9-b66b-795322896cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = C1_Merge_train.shape[1]\n",
    "time_steps = 24\n",
    "Split = len(C1_Merge_train)\n",
    "\n",
    "s_Train = C1_Merge_train.iloc[0 : int(Split * 0.8)]\n",
    "s_Val = C1_Merge_train.iloc[int(Split * 0.8) :]\n",
    "o_Test = C1_GAP_test.iloc[time_steps :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccade1f-8fbf-4c22-b7f9-dacee6940ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = to_Supervised(s_Train, s_Train.Aggregate, time_steps)\n",
    "X_val, Y_val = to_Supervised(s_Val, s_Val.Aggregate, time_steps)\n",
    "X_test, Y_test = to_Supervised(C1_Merge_test, C1_Merge_test.Aggregate, time_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e84e720-6c3c-4390-99fc-3fc0088e6013",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(\n",
    "    [\n",
    "        Conv1D(filters=64, kernel_size=3, activation=LeakyReLU(), padding=\"same\", input_shape=(time_steps, n_features)),\n",
    "        MaxPooling1D(2),\n",
    "        Conv1D(filters=32, kernel_size=2, activation=LeakyReLU(), padding=\"same\"),\n",
    "        MaxPooling1D(2),\n",
    "        Conv1D(filters=16, kernel_size=2, activation=LeakyReLU(), padding=\"same\"),\n",
    "        MaxPooling1D(2),\n",
    "        LSTM(256, activation=LeakyReLU(), return_sequences=True),\n",
    "        LSTM(128, activation=LeakyReLU()),\n",
    "        Dense(64),\n",
    "        Dense(1),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "model.compile(optimizer=\"NAdam\", loss=\"logcosh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2368cc30-4b9d-40e1-949f-90830dfec1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_learning = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.2, patience=3, verbose=1, mode=\"auto\", min_delta=0.000025, cooldown=4, min_lr=0)\n",
    "eary_stopping = EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=7, verbose=1, mode=\"auto\")\n",
    "\n",
    "callbacks = [reduce_learning, eary_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382415c3-9e91-4d52-8e6e-3429f2b5e28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val, Y_val),\n",
    "    verbose=1,\n",
    "    shuffle=False,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55668e9e-8202-409a-a0c9-17e754139467",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "epochs_graph = np.arange(1, len(loss) + 1)\n",
    "\n",
    "x_ticks = np.arange(0, len(loss) + 1, 5)\n",
    "x_ticks = np.insert(x_ticks, 1, 1)\n",
    "x_ticks = np.insert(x_ticks, len(x_ticks), len(loss))\n",
    "\n",
    "plt.xticks(x_ticks)\n",
    "plt.xlim(1, len(loss))\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(epochs_graph, loss, \"red\", label=\"Training loss\")\n",
    "plt.plot(epochs_graph, val_loss, \"blue\", label=\"Validation loss\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1365e07d-9d49-4caa-b004-b704473320be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"Models/REFIT-Raw.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d2a8d7-792e-47b7-8dca-6a4c82549fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = load_model(\"Models/REFIT-Raw.h5\", custom_objects={\"LeakyReLU\": tensorflow.keras.layers.LeakyReLU})\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48031715-743b-45b2-ad44-f15035057f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Predictions = model.predict(X_test)\n",
    "Predictions = scaler_te.inverse_transform(Predictions.reshape(-1, 1))\n",
    "\n",
    "s_Pred = C1_Merge_test[\"Aggregate\"].to_frame().copy()\n",
    "s_Pred = s_Pred.iloc[time_steps:]\n",
    "s_Pred[s_Pred.columns] = scaler_te.inverse_transform(s_Pred[s_Pred.columns])\n",
    "s_Pred.insert(len(s_Pred.columns), \"Predictions\", Predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c087fc4-4481-41b8-a2ed-1f5c4f722e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_accuracy(s_Pred.Aggregate.ravel(), s_Pred.Predictions.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024197f4-5fe0-4ec7-bb14-efcd5287e46f",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## 4.3: 12 steps ahead - Trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dffcbe-0d9b-415f-9600-0c4c406ea2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "C1_Merge_train = CART_CL.loc[:, \"Labels\"].to_frame()\n",
    "df_REFIT_resampled_train = df_REFIT_resampled_CL.copy()\n",
    "df_REFIT_resampled_train.Labels = C1_Merge_train.Labels\n",
    "df_REFIT_resampled_train.dropna(inplace=True)\n",
    "C1_Merge_train = df_REFIT_resampled_train.loc[df_REFIT_resampled_train[\"Labels\"] == 1].copy()\n",
    "C1_Merge_train = C1_Merge_train.stack().reset_index()\n",
    "C1_Merge_train[\"Date\"] = C1_Merge_train[\"Date\"].astype(\"str\")\n",
    "C1_Merge_train[\"Time\"] = C1_Merge_train[\"Time\"].astype(\"str\")\n",
    "C1_Merge_train[\"DT\"] = C1_Merge_train[\"Date\"].str.cat(C1_Merge_train[\"Time\"], sep=\" \")\n",
    "\n",
    "cols_NA = [\n",
    "    \"Date\",\n",
    "    \"Time\",\n",
    "    \"Labels\",\n",
    "    \"Day_name\",\n",
    "    \"Month_name\"\n",
    "]\n",
    "\n",
    "C1_Merge_train.drop(cols_NA, axis=1, inplace=True)\n",
    "C1_Merge_train[\"DT\"] = pd.to_datetime(C1_Merge_train[\"DT\"])\n",
    "C1_Merge_train = C1_Merge_train.set_index(\"DT\")\n",
    "C1_Merge_train.dropna(inplace=True)\n",
    "C1_Merge_train = df_Merged.reindex(C1_Merge_train.index)\n",
    "C1_Merge_train = C1_Merge_train.dropna()\n",
    "C1_Merge_train[\"Holiday\"] = C1_Merge_train[\"Holiday\"].astype('int32')\n",
    "C1_Merge_train[\"Season\"] = C1_Merge_train[\"Season\"].astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b625d9-3359-4980-a722-df467bed3685",
   "metadata": {},
   "outputs": [],
   "source": [
    "C1_Merge_test = CART_TE.loc[:, \"Labels\"].to_frame()\n",
    "df_REFIT_resampled_test = df_REFIT_resampled_TE.copy()\n",
    "df_REFIT_resampled_test.insert(len(df_REFIT_resampled_test.columns), \"Labels\", C1_Merge_test.Labels)\n",
    "df_REFIT_resampled_test.dropna(inplace=True)\n",
    "C1_Merge_test = df_REFIT_resampled_test.loc[df_REFIT_resampled_test[\"Labels\"] == 1].copy()\n",
    "C1_Merge_test = C1_Merge_test.stack().reset_index()\n",
    "C1_Merge_test[\"Date\"] = C1_Merge_test[\"Date\"].astype(\"str\")\n",
    "C1_Merge_test[\"Time\"] = C1_Merge_test[\"Time\"].astype(\"str\")\n",
    "C1_Merge_test[\"DT\"] = C1_Merge_test[\"Date\"].str.cat(C1_Merge_test[\"Time\"], sep=\" \")\n",
    "\n",
    "cols_NA = [\n",
    "    \"Date\",\n",
    "    \"Time\",\n",
    "    \"Labels\",\n",
    "]\n",
    "\n",
    "C1_Merge_test.drop(cols_NA, axis=1, inplace=True)\n",
    "C1_Merge_test[\"DT\"] = pd.to_datetime(C1_Merge_test[\"DT\"])\n",
    "C1_Merge_test = C1_Merge_test.set_index(\"DT\")\n",
    "C1_Merge_test.dropna(inplace=True)\n",
    "C1_Merge_test = df_Merged.reindex(C1_Merge_test.index)\n",
    "C1_Merge_test = C1_Merge_test.dropna()\n",
    "C1_Merge_test[\"Holiday\"] = C1_Merge_test[\"Holiday\"].astype('int32')\n",
    "C1_Merge_test[\"Season\"] = C1_Merge_test[\"Season\"].astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb23597-2751-4973-91a1-858bd00f5490",
   "metadata": {},
   "outputs": [],
   "source": [
    "day = 24 * 60 * 60\n",
    "year = (365.2425) * day\n",
    "timestamp_train = C1_Merge_train.index.map(datetime.datetime.timestamp)\n",
    "timestamp_test = C1_Merge_test.index.map(datetime.datetime.timestamp)\n",
    "\n",
    "day_sin_tr = np.sin(timestamp_train * (2 * np.pi / day))\n",
    "day_cos_tr = np.cos(timestamp_train * (2 * np.pi / day))\n",
    "year_sin_tr = np.sin(timestamp_train * (2 * np.pi / year))\n",
    "year_cos_tr = np.cos(timestamp_train * (2 * np.pi / year))\n",
    "\n",
    "day_sin_te = np.sin(timestamp_test * (2 * np.pi / day))\n",
    "day_cos_te = np.cos(timestamp_test * (2 * np.pi / day))\n",
    "year_sin_te = np.sin(timestamp_test * (2 * np.pi / year))\n",
    "year_cos_te = np.cos(timestamp_test * (2 * np.pi / year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8208b7-5096-4b74-831c-25160c6dbaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_NA = [\"Year\", \"Month\", \"Holiday\", \"Day\", \"Hour\", \"Minute\", \"Weekday\", \"Season\"]\n",
    "\n",
    "C1_Merge_train.drop(cols_NA, axis=1, inplace=True)\n",
    "C1_Merge_test.drop(cols_NA, axis=1, inplace=True)\n",
    "\n",
    "C1_Merge_train.insert(0, \"Day_Sin\", day_sin_tr)\n",
    "C1_Merge_train.insert(0, \"Day_Cos\", day_cos_tr)\n",
    "C1_Merge_train.insert(0, \"Year_Sin\", year_sin_tr)\n",
    "C1_Merge_train.insert(0, \"Year_Cos\", year_cos_tr)\n",
    "\n",
    "C1_Merge_test.insert(0, \"Day_Sin\", day_sin_te)\n",
    "C1_Merge_test.insert(0, \"Day_Cos\", day_cos_te)\n",
    "C1_Merge_test.insert(0, \"Year_Sin\", year_sin_te)\n",
    "C1_Merge_test.insert(0, \"Year_Cos\", year_cos_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d7f103-f70f-4e15-bd5f-b0bd9b528e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = (24 * 60) // 15\n",
    "stl_decompose_result = STL(C1_Merge_train.Aggregate, period=freq).fit()\n",
    "C1_Merge_train.insert(len(C1_Merge_train.columns), \"Trend\", stl_decompose_result.trend.values)\n",
    "\n",
    "stl_decompose_result = STL(C1_Merge_test.Aggregate, period=freq).fit()\n",
    "C1_Merge_test.insert(len(C1_Merge_test.columns), \"Trend\", stl_decompose_result.trend.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4799ff0e-758a-4060-b7db-d2a4dd01d3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "C1_GAP_train = C1_Merge_train.Aggregate.copy().to_frame()\n",
    "C1_Merge_train.Aggregate = savgol_filter(C1_Merge_train.Aggregate, 5, 3)\n",
    "\n",
    "C1_GAP_test = C1_Merge_test.Aggregate.copy().to_frame()\n",
    "C1_Merge_test.Aggregate = savgol_filter(C1_Merge_test.Aggregate, 5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4221adcc-431b-4240-ba12-2fb84fd9c2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_tr = MaxAbsScaler()\n",
    "scaler_te = MaxAbsScaler()\n",
    "\n",
    "C1_Merge_train[C1_Merge_train.columns] = scaler_tr.fit_transform(C1_Merge_train[C1_Merge_train.columns])\n",
    "scaler_tr.max_abs_, scaler_tr.scale_ = scaler_tr.max_abs_[len(C1_Merge_train.columns) - 1], scaler_tr.scale_[len(C1_Merge_train.columns) - 1]\n",
    "\n",
    "C1_Merge_test[C1_Merge_test.columns] = scaler_te.fit_transform(C1_Merge_test[C1_Merge_test.columns])\n",
    "scaler_te.max_abs_, scaler_te.scale_ = scaler_te.max_abs_[len(C1_Merge_test.columns) - 1], scaler_te.scale_[len(C1_Merge_test.columns) - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b355a25-a33e-4c06-b5f3-acecf92199ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = C1_Merge_train.shape[1]\n",
    "time_steps = 24\n",
    "Split = len(C1_Merge_train)\n",
    "\n",
    "s_Train = C1_Merge_train.iloc[0 : int(Split * 0.8)]\n",
    "s_Val = C1_Merge_train.iloc[int(Split * 0.8) :]\n",
    "o_Test = C1_GAP_test.iloc[time_steps :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b0d67a-150d-48ca-b96c-a9667e895b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = to_Supervised_ms(s_Train.values, lag=24, n_ahead=12, target_index=len(s_Train.columns) - 1)\n",
    "X_val, Y_val = to_Supervised_ms(s_Val.values, lag=24, n_ahead=12, target_index=len(s_Val.columns) - 1)\n",
    "X_test, Y_test = to_Supervised_ms(C1_Merge_test.values, lag=24, n_ahead=12, target_index=len(C1_Merge_test.columns) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74841233-7aa7-4617-9483-93a8fa8bde66",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(\n",
    "    [\n",
    "        Conv1D(filters=64, kernel_size=3, activation=LeakyReLU(), padding=\"same\", input_shape=(time_steps, n_features)),\n",
    "        MaxPooling1D(2),\n",
    "        Conv1D(filters=32, kernel_size=2, activation=LeakyReLU(), padding=\"same\"),\n",
    "        MaxPooling1D(2),\n",
    "        Conv1D(filters=16, kernel_size=2, activation=LeakyReLU(), padding=\"same\"),\n",
    "        MaxPooling1D(2),\n",
    "        LSTM(256, activation=LeakyReLU(), return_sequences=True),\n",
    "        LSTM(128, activation=LeakyReLU()),\n",
    "        Dense(64),\n",
    "        Dense(12),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "model.compile(optimizer=\"NAdam\", loss=\"logcosh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1c334c-cdc9-472f-a013-81d74b074c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_learning = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.2, patience=3, verbose=1, mode=\"auto\", min_delta=0.000025, cooldown=4, min_lr=0)\n",
    "eary_stopping = EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=7, verbose=1, mode=\"auto\")\n",
    "\n",
    "callbacks = [reduce_learning, eary_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3904f058-eb11-4cf6-bf2c-353510e0f69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val, Y_val),\n",
    "    verbose=1,\n",
    "    shuffle=False,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18e7464-0cc9-4472-a914-0a134c82769b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "epochs_graph = np.arange(1, len(loss) + 1)\n",
    "\n",
    "x_ticks = np.arange(0, len(loss) + 1, 5)\n",
    "x_ticks = np.insert(x_ticks, 1, 1)\n",
    "x_ticks = np.insert(x_ticks, len(x_ticks), len(loss))\n",
    "\n",
    "plt.xticks(x_ticks)\n",
    "plt.xlim(1, len(loss))\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(epochs_graph, loss, \"red\", label=\"Training loss\")\n",
    "plt.plot(epochs_graph, val_loss, \"blue\", label=\"Validation loss\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb2139a-3a55-4279-84db-321ca8e7abe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"Models/UCID-Raw-12-Steps-Trend.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cff970-655a-4d60-96fd-448ecef2b386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = load_model(\"Models/UCID-Raw-12-Steps.h5\", custom_objects={\"LeakyReLU\": tensorflow.keras.layers.LeakyReLU})\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac4fb47-5e1f-4d9d-9ac0-bd1f72da9d4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Predictions = model.predict(X_test)\n",
    "Predictions = scaler_te.inverse_transform(Predictions.reshape(-1, 1))\n",
    "Y_test = scaler_te.inverse_transform(Y_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd6e169-dfc5-4d5e-8070-94c4af84f2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_accuracy(Predictions, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9bc58e-155d-43b9-ad7a-9bcbad5b4498",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## 4.3: 12 steps ahead - Raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8c3b6f-d85b-4d21-8f90-81efa63dc3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "C1_Merge_train = CART_CL.loc[:, \"Labels\"].to_frame()\n",
    "df_REFIT_resampled_train = df_REFIT_resampled_CL.copy()\n",
    "df_REFIT_resampled_train.Labels = C1_Merge_train.Labels\n",
    "df_REFIT_resampled_train.dropna(inplace=True)\n",
    "C1_Merge_train = df_REFIT_resampled_train.loc[df_REFIT_resampled_train[\"Labels\"] == 1].copy()\n",
    "C1_Merge_train = C1_Merge_train.stack().reset_index()\n",
    "C1_Merge_train[\"Date\"] = C1_Merge_train[\"Date\"].astype(\"str\")\n",
    "C1_Merge_train[\"Time\"] = C1_Merge_train[\"Time\"].astype(\"str\")\n",
    "C1_Merge_train[\"DT\"] = C1_Merge_train[\"Date\"].str.cat(C1_Merge_train[\"Time\"], sep=\" \")\n",
    "\n",
    "cols_NA = [\n",
    "    \"Date\",\n",
    "    \"Time\",\n",
    "    \"Labels\",\n",
    "    \"Day_name\",\n",
    "    \"Month_name\"\n",
    "]\n",
    "\n",
    "C1_Merge_train.drop(cols_NA, axis=1, inplace=True)\n",
    "C1_Merge_train[\"DT\"] = pd.to_datetime(C1_Merge_train[\"DT\"])\n",
    "C1_Merge_train = C1_Merge_train.set_index(\"DT\")\n",
    "C1_Merge_train.dropna(inplace=True)\n",
    "C1_Merge_train = df_Merged.reindex(C1_Merge_train.index)\n",
    "C1_Merge_train = C1_Merge_train.dropna()\n",
    "C1_Merge_train[\"Holiday\"] = C1_Merge_train[\"Holiday\"].astype('int32')\n",
    "C1_Merge_train[\"Season\"] = C1_Merge_train[\"Season\"].astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d626c4-5f60-4c9c-a91f-bbe61de02798",
   "metadata": {},
   "outputs": [],
   "source": [
    "C1_Merge_test = CART_TE.loc[:, \"Labels\"].to_frame()\n",
    "df_REFIT_resampled_test = df_REFIT_resampled_TE.copy()\n",
    "df_REFIT_resampled_test.insert(len(df_REFIT_resampled_test.columns), \"Labels\", C1_Merge_test.Labels)\n",
    "df_REFIT_resampled_test.dropna(inplace=True)\n",
    "C1_Merge_test = df_REFIT_resampled_test.loc[df_REFIT_resampled_test[\"Labels\"] == 1].copy()\n",
    "C1_Merge_test = C1_Merge_test.stack().reset_index()\n",
    "C1_Merge_test[\"Date\"] = C1_Merge_test[\"Date\"].astype(\"str\")\n",
    "C1_Merge_test[\"Time\"] = C1_Merge_test[\"Time\"].astype(\"str\")\n",
    "C1_Merge_test[\"DT\"] = C1_Merge_test[\"Date\"].str.cat(C1_Merge_test[\"Time\"], sep=\" \")\n",
    "\n",
    "cols_NA = [\n",
    "    \"Date\",\n",
    "    \"Time\",\n",
    "    \"Labels\",\n",
    "]\n",
    "\n",
    "C1_Merge_test.drop(cols_NA, axis=1, inplace=True)\n",
    "C1_Merge_test[\"DT\"] = pd.to_datetime(C1_Merge_test[\"DT\"])\n",
    "C1_Merge_test = C1_Merge_test.set_index(\"DT\")\n",
    "C1_Merge_test.dropna(inplace=True)\n",
    "C1_Merge_test = df_Merged.reindex(C1_Merge_test.index)\n",
    "C1_Merge_test = C1_Merge_test.dropna()\n",
    "C1_Merge_test[\"Holiday\"] = C1_Merge_test[\"Holiday\"].astype('int32')\n",
    "C1_Merge_test[\"Season\"] = C1_Merge_test[\"Season\"].astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4291e965-0e60-4452-be21-7cd66e770cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "day = 24 * 60 * 60\n",
    "year = (365.2425) * day\n",
    "timestamp_train = C1_Merge_train.index.map(datetime.datetime.timestamp)\n",
    "timestamp_test = C1_Merge_test.index.map(datetime.datetime.timestamp)\n",
    "\n",
    "day_sin_tr = np.sin(timestamp_train * (2 * np.pi / day))\n",
    "day_cos_tr = np.cos(timestamp_train * (2 * np.pi / day))\n",
    "year_sin_tr = np.sin(timestamp_train * (2 * np.pi / year))\n",
    "year_cos_tr = np.cos(timestamp_train * (2 * np.pi / year))\n",
    "\n",
    "day_sin_te = np.sin(timestamp_test * (2 * np.pi / day))\n",
    "day_cos_te = np.cos(timestamp_test * (2 * np.pi / day))\n",
    "year_sin_te = np.sin(timestamp_test * (2 * np.pi / year))\n",
    "year_cos_te = np.cos(timestamp_test * (2 * np.pi / year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a5ba2f-4ae3-42ed-802d-1bf37b9b9f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_NA = [\"Year\", \"Month\", \"Holiday\", \"Day\", \"Hour\", \"Minute\", \"Weekday\", \"Season\"]\n",
    "\n",
    "C1_Merge_train.drop(cols_NA, axis=1, inplace=True)\n",
    "C1_Merge_test.drop(cols_NA, axis=1, inplace=True)\n",
    "\n",
    "C1_Merge_train.insert(0, \"Day_Sin\", day_sin_tr)\n",
    "C1_Merge_train.insert(0, \"Day_Cos\", day_cos_tr)\n",
    "C1_Merge_train.insert(0, \"Year_Sin\", year_sin_tr)\n",
    "C1_Merge_train.insert(0, \"Year_Cos\", year_cos_tr)\n",
    "\n",
    "C1_Merge_test.insert(0, \"Day_Sin\", day_sin_te)\n",
    "C1_Merge_test.insert(0, \"Day_Cos\", day_cos_te)\n",
    "C1_Merge_test.insert(0, \"Year_Sin\", year_sin_te)\n",
    "C1_Merge_test.insert(0, \"Year_Cos\", year_cos_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc8868a-0543-4909-9abe-4c720d824077",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = (24 * 60) // 15\n",
    "stl_decompose_result = STL(C1_Merge_train.Aggregate, period=freq).fit()\n",
    "C1_Merge_train.insert(len(C1_Merge_train.columns), \"Trend\", stl_decompose_result.trend.values)\n",
    "\n",
    "stl_decompose_result = STL(C1_Merge_test.Aggregate, period=freq).fit()\n",
    "C1_Merge_test.insert(len(C1_Merge_test.columns), \"Trend\", stl_decompose_result.trend.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab629ca6-e3ee-414e-9e87-e430034c77e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "C1_GAP_train = C1_Merge_train.Aggregate.copy().to_frame()\n",
    "C1_Merge_train.Aggregate = savgol_filter(C1_Merge_train.Aggregate, 5, 3)\n",
    "\n",
    "C1_GAP_test = C1_Merge_test.Aggregate.copy().to_frame()\n",
    "C1_Merge_test.Aggregate = savgol_filter(C1_Merge_test.Aggregate, 5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96491031-53b0-49b3-815e-877a06c90f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_tr = MinMaxScaler()\n",
    "scaler_te = MinMaxScaler()\n",
    "\n",
    "C1_Merge_train[C1_Merge_train.columns] = scaler_tr.fit_transform(C1_Merge_train[C1_Merge_train.columns])\n",
    "scaler_tr.min_, scaler_tr.scale_ = scaler_tr.min_[len(C1_Merge_train.columns) - 2], scaler_tr.scale_[len(C1_Merge_train.columns) - 2]\n",
    "\n",
    "C1_Merge_test[C1_Merge_test.columns] = scaler_te.fit_transform(C1_Merge_test[C1_Merge_test.columns])\n",
    "scaler_te.min_, scaler_te.scale_ = scaler_te.min_[len(C1_Merge_test.columns) - 2], scaler_te.scale_[len(C1_Merge_test.columns) - 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715321d6-a01a-4203-a423-a216fe76d434",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = C1_Merge_train.shape[1]\n",
    "time_steps = 24\n",
    "Split = len(C1_Merge_train)\n",
    "\n",
    "s_Train = C1_Merge_train.iloc[0 : int(Split * 0.8)]\n",
    "s_Val = C1_Merge_train.iloc[int(Split * 0.8) :]\n",
    "o_Test = C1_GAP_test.iloc[time_steps :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c89f83-6108-427c-861b-ccee822d1e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = to_Supervised_ms(s_Train.values, lag=24, n_ahead=12, target_index=len(s_Train.columns) - 2)\n",
    "X_val, Y_val = to_Supervised_ms(s_Val.values, lag=24, n_ahead=12, target_index=len(s_Val.columns) - 2)\n",
    "X_test, Y_test = to_Supervised_ms(C1_Merge_test.values, lag=24, n_ahead=12, target_index=len(C1_Merge_test.columns) - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358776dd-664b-4462-9927-f6a2a986dc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(\n",
    "    [\n",
    "        Conv1D(filters=64, kernel_size=3, activation=LeakyReLU(), padding=\"same\", input_shape=(time_steps, n_features)),\n",
    "        MaxPooling1D(2),\n",
    "        Conv1D(filters=32, kernel_size=2, activation=LeakyReLU(), padding=\"same\"),\n",
    "        MaxPooling1D(2),\n",
    "        Conv1D(filters=16, kernel_size=2, activation=LeakyReLU(), padding=\"same\"),\n",
    "        MaxPooling1D(2),\n",
    "        LSTM(256, activation=LeakyReLU(), return_sequences=True),\n",
    "        LSTM(128, activation=LeakyReLU()),\n",
    "        Dense(64),\n",
    "        Dense(12),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "model.compile(optimizer=\"NAdam\", loss=\"logcosh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e224fd49-19ab-42fd-a732-3b5dd0ddb77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_learning = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.2, patience=3, verbose=1, mode=\"auto\", min_delta=0.000025, cooldown=4, min_lr=0)\n",
    "eary_stopping = EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=7, verbose=1, mode=\"auto\")\n",
    "\n",
    "callbacks = [reduce_learning, eary_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ac7a9d-4dd9-46c3-b3cf-c18a003b97a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val, Y_val),\n",
    "    verbose=1,\n",
    "    shuffle=False,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e1cf48-ad0d-4e10-b6f7-5c97f1c2516e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "epochs_graph = np.arange(1, len(loss) + 1)\n",
    "\n",
    "x_ticks = np.arange(0, len(loss) + 1, 5)\n",
    "x_ticks = np.insert(x_ticks, 1, 1)\n",
    "x_ticks = np.insert(x_ticks, len(x_ticks), len(loss))\n",
    "\n",
    "plt.xticks(x_ticks)\n",
    "plt.xlim(1, len(loss))\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(epochs_graph, loss, \"red\", label=\"Training loss\")\n",
    "plt.plot(epochs_graph, val_loss, \"blue\", label=\"Validation loss\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5507ae2-3e3e-402d-bdf5-4606eb1afbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"Models/UCID-Raw-12-Steps-Trend.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8d430a-8738-4ad4-91be-54b398fb3d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = load_model(\"Models/UCID-Raw-12-Steps.h5\", custom_objects={\"LeakyReLU\": tensorflow.keras.layers.LeakyReLU})\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4f3578-e884-4b07-8c5b-f098ed9ce1fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Predictions = model.predict(X_test)\n",
    "Predictions = scaler_te.inverse_transform(Predictions.reshape(-1, 1))\n",
    "Y_test = scaler_te.inverse_transform(Y_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab04a6d-872a-4f55-abab-6e9e31324c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_accuracy(Y_test, Predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
